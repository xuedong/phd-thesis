%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%									Chapitre 1											%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{An Overview of the Thesis}\label{chap:intro}
	\citationChap{
	The thing about quotes on the internet is that you can not confirm their validity
	}{Abraham Lincoln}
	\minitoc
	\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Début du chapitre

The purpose of this thesis is to provide a summary of the main research line of my PhD work carried out in between October 2017 and March 2021. During my PhD, I was hosted at the Inria Lille-Nord Europe (France) research center, in the SequeL team (now becomes Scool team). I was fortunate to be advised by Dr. Michal Valko, and also co-supervised by Dr. Emilie Kaufmann. The research thematic of SequeL lies in sequential decision making problems, to which all my contributions are devoted. In particular, this document mainly investigates sequential decision making in optimization problems.

\section{Context of the Thesis}\label{sec:intro.context}
	
\subsection{What do we study and why?}\label{sec:intro.context.what}

Imagine that we dispose a simulator for some complex numerical task. Being considered as a black box, we can only get useful information by calling the simulator with different inputs. The goal is to find an input that optimizes the performance of the simulator. In the context of this thesis, we model such a scene as the so-called \gls{sequential optimization}\footnote{We thus do not consider parallelization in this thesis.} problem where a learner sequentially feeds inputs to an environment (the simulator in the previous example) and from which they receive (deterministic or stochastic) feedback/payoffs/rewards/observations\footnote{Those terms can be interchangeably employed.}. After a certain period of trial, the learner shall be able to output a guess for the optimal input. Under some circumstances, a single interaction with the environment could be extremely costly. It is therefore of great interest to carefully choose the input at each time step based on past observations.

Sequential optimization in a stochastic environment is an active research topic in both applied mathematics and computer science communities. For example, the planning problem in a \gls{mdp}, upon which the recent breakthrough of game intelligence of Go~\citep{silver2016alphago} is constructed, is closely related to sequential optimization. Precisely, given the current state of the game, the game intelligence is designed to maximize a certain value function, whose (noisy) observations can be obtained by exploring well-chosen trajectories.

Another example, which is also one important driving force that motivates this thesis originally, is \gls{hpo} of machine learning classifiers. Modern machine learning algorithms often contain many nuisance parameters that cannot be learned through the learning process, but instead, need to be manually specified. Tuning those so-called \gls{hyper-parameters} is often considered as the most tedious part in a data science task. It is hence appealing to design HPO algorithms that automates the process of choosing those hyper-parameters. HPO can be viewed as a BBO problem where function evaluations are supposed to be very expensive. Typically, a function evaluation in HPO involves running the primary machine learning algorithm to completion on a large and high-dimensional dataset, which often takes a considerable amount of time or resources.

Besides, sequential optimization can also serve as an abstraction of numerous real-world problems. To name a few of them, we can think of (risk-averse) portfolio selection problems in finance~\citep{ziemba2010}, designing effective treatment allocation strategies in medicine~\citep{durand2018contextual}, free-energy minimization in chemical engineering or protein structure prediction~\citep{floudas2000}, metamodelling for engineering design optimization~\citep{wang2007}, parameter estimation (inverse problem) of nonlinear dynamic biochemical pathways~\citep{moles2003}, mesh distortion in material science~\citep{charpagne2019ebsd}, and way more. 

Mathematically speaking, an environment is simply a target function to be optimized. This function can be \textbf{discrete or continuous}. In this thesis, we are in particular interested in optimization of functions for which \textbf{none (or few)} regularity assumptions are made, and only \textbf{noisy (or stochastic)} function evaluations (interactions with the environment) can be observed.

%Le problème de planification dans un Processus de Décision Markovien (MDP pour Markov Decision Process, [1]), pouvant modéliser la gestion de ressources dans un système de type smart grids ou encore le contrôle d'un robot, et celui de la construction d'intelligences artificielles pour des jeux, sont très liés à celui de l'optimisation séquentielle puisqu'ils reviennent à déterminer l'action qui dans un état donné maximise une fonction valeur, dont on peut obtenir des réalisations bruitées en explorant des trajectoires bien choisies.

%In this context, since one does not make extra regularity assumptions on the target function, one can imagine that it can be very costly to evaluate the function. Thus a good strategy for choosing adaptively the next observation is needed in order to find an optimal (or quasi-optimal) point with as few number of evaluations as possible. This is the sequential optimization problem. That being said, the main problematic of my thesis is the \emph{global sequential optimization} problem. Several applications could be investigated in this thesis, in particular the \emph{automatic hyper-parameter tuning} of machine learning algorithms~\citep{samothrakis2013,hoffman2014bayesgap,jamieson2016hyperband,li2017hyperband}.

\subsection{How do we approach the problem?}\label{sec:intro.context.how}

Sequential optimization problem has been widely inspired by the literature of~\gls{mab}. The original MAB problem is first studied by~\cite{thompson1933}, and can be described in the following way: A learner is given a \emph{finite} set of arms $\{1,\ldots,K\}$ that follows $K$ unknown distributions respectively. For a given time horizon $T$, a vector of rewards is generated according to the distributions at each time step from $1$ to $T$. The learner can choose to pull one of the arms and observe its corresponding reward.

\begin{remark}\label{remark:partial}
\begin{leftbar}[remarkbar]
Note that rewards of unchosen arms at each time step are not revealed: this partially observable feedback setting is thus a special case of the online learning with experts setting.
\end{leftbar}
\end{remark}

In its original form, the objective of a MAB learner is to maximize the total rewards in the long run. A natural observation is that, the learner is required to simultaneously acquire new information for potential future well-being (exploration), and optimize the current decision based on past observations (exploitation). This trade-off is stated as the \gls{exploration-exploitation dilemma}. 
%The usual performance criterion of the learner is measured by the total loss of the chosen arm at each time step w.r.t the best arm, namely the \emph{cumulative regret}. Typical good learners like \UCB~\citep{auer2002ucb} trade off between exploration and exploitation. 

However, exploitation does not necessarily provide meaningful incentives in some real applications. Typically, in the previous working examples presented in Section~\ref{sec:intro.context.what}, we do not really care about the potential losses incurred during the whole learning phase. Indeed, we only aim at finding the (near-)optimum of the target function quickly. In this context, it is more natural to assess the learner in an optimization fashion. This setting, often named as \gls{bai}, is thus more strongly related to what we are to investigate in this thesis.

\begin{remark}
\begin{leftbar}[remarkbar]
More generally, we talk about \gls{pure exploration} problems~\citep{bubeck2011pure} instead of BAI, where the learner is supposed to gain as much information about the bandit model regardless of rewards. BAI is merely a particular instance of pure exploration, for which the learning objective is to find the optimal arm. Other learning objectives also exist, like finding arms that surpass some pre-defined threshold (see e.g.~\citealt{locatelli2016thresholding}). However, we mostly focus on BAI in this thesis since it is sufficiently representative.
\end{leftbar}
\end{remark}

\subsection{From multi-armed bandits to reinforcement learning}\label{sec:intro.context.rl}

\section{Multi-Armed Bandits and Optimization}\label{sec:intro.mab}
    
This thesis tries to address sequential optimization problems under stochastic environments from a bandit point of view. A stochastic environment refers to an environment from which stochastic feedback are acquired when an input is queried from the search/action space\footnote{Those terms can be interchangeably employed.} $\mathcal{X}$. Formally, and without loss of generality, we aim to maximize\footnote{Minimization is obviously the same problem.} a target function $f:\mathcal{X}\rightarrow\mathbb{R}$, i.e. find 
\begin{equation}\label{eq:optim}
    \argmax_{x\in\mathcal{X}} f(x)\,.
\end{equation}
Without any prior information on the target function and/or the search space $\cX$, it is just a find-a-needle-in-a-haystack mission. This thesis studies several particular instances of \eqref{eq:optim} with various search spaces and/or different (regularity) assumptions on the target function though, and brings new theoretical and practical insights. 

Before entering into details, we provide a brief overview of different settings investigated in this thesis in this section. More thorough discussion on the problem formulation, in particular how do we assess the performance of algorithms under different settings is given in Chapter~\ref{chap:mab}.

%In its simplest form where the action space $\mathcal{X}$ is finite, the problem can be modeled as a \emph{stochastic multi-armed bandit}. The term \emph{bandit} is named, by analogy, after slot machines (or one-armed bandits) in a casino. A \emph{sequential decision making} problem comes up then when facing with several slot machines (multi-armed bandits). Concretely, a stochastic bandit is a collection of $K$ actions (also called arms) $\mathcal{X} = \{x_1,\ldots,x_K\}$. Each time the learner chooses one action $x_{a_t}\in\mathcal{X}$ which is then fed to the environment. The environment generates a reward $r_{a_t,t}=r_t$ which is assumed to be drawn from an unknown $[0,1]$-valued distribution $\nu_{a_t}$ and is revealed to the learner.

%In its original formulation, the learning objective of a stochastic bandit is to maximise the total reward $\sum_{t=1}^T r_t$ obtained within a given time horizon $T$. In the literature of bandit, we usually denote by $\mu_k$ (resp. $\mu^\star$) the expectation of the unknown distribution $\nu_k$ (resp. the optimal arm), and the previous reward maximisation objective is equivalent to minimising the \emph{cumulative regret}: $T\mu^\star-\sum_{t=1}^T \mu_{a_t}$. %A such learning objective requires the learner to simultaneously acquire new information for potential future well-being (called \emph{exploration}), and optimize the current decision based on past observations (called \emph{exploitation}). 
%Multi-armed bandits naturally addresses the trade-off between exploration and exploitation.

\subsection{Best-arm identification for stochastic multi-armed bandits}

The first setting of interest, for which the search space $\mathcal{X}$ is finite and one-dimensional and the target function $f$ can be considered as a function that maps each arm $x_k$ to its mean $\mu_k$. In one of our papers~\citep{shang2020t3c}, we employ some Bayesian machinery to address the problem based on the famous Thompson sampling (\TS). \TS is a Bayesian algorithm well known for regret minimization, for which it is now seen as a major competitor to the popular \UCB-typed approaches~\citep{auer2002ucb}. A natural question to ask is whether Bayesian methods can be also a good competitor to classical BAI approaches constructed upon complicated confidence intervals. However, it is well known that direct use of \TS cannot yield optimal performance for BAI and an adaptation such as \TTTS (top-two Thompson sampling) proposed by~\cite{russo2016ttts} is needed: by choosing between two different candidate arms in each round, it enforces the exploration of sub-optimal arms, which would be under-sampled by vanilla \TS due to its objective of maximizing rewards. \cite{shang2020t3c} provides further theoretical understandings of \TTTS and proposes a computational improvement \TCC with the same guarantees. \cite{shang2019dttts} also show that with some minor modifications, such Bayesian-flavored algorithms can be good candidates for applications like hyper-parameter optimization.

\subsection{Extension to best-arm identification for linear bandits}

A very natural and popular extension of the vanilla BAI is to take a finite space of features $\cX$ in $\R^d$ as the search space and consider a target function $f$ that maps each arm $\mathbf{x}$ (also called context in this case) to its linear combination with a \emph{regression parameter} $\btheta$, and is thus called linear bandits. $\btheta$ is of course unknown to the learner. Previous work on this topic were not (asymptotically) optimal. A lower bound, which can be written as a complicated minimax optimization problem, is given by~\cite{garivier2016tracknstop}. In a subsequent work, we develop a saddle-point approach to that lower bound optimization problem, which then led to optimal algorithms for linear BAI in the fixed-confidence regime~\citep{degenne2020game}. In addition, their empirical performance is competitive with the best existing algorithms. We also investigate a natural adaptation of our Bayesian approaches developed in the previous paper~\citep{shang2020t3c}. Unfortunately, they do not seem to be (asymptotically) optimal (work not published yet). Therefore, knowing whether one can find an *optimal* Bayesian approach remains an open problem.

\subsection{Infinitely-armed bandits and black-box optimization}

Finally, a more general problem is to consider an infinite (probably uncountable) measurable space $\mathcal{X}$, and each arm $x\in\mathcal{X}$ gets its mean reward $f(x)$ through the reward function $f$. This is the \gls{go} or \gls{bbo} problem. Sometimes we can also refer to \gls{zo}, in contrast to \emph{first-order optimization} for which gradient-based information is available.

We study the noisy setting in which the obtained reward is a noisy evaluation of $f: r_t = f(x_t) + \epsilon_t$. This problem is also better known under the name \emph{black-box optimization}, to which the main approaches include Bayesian optimization~\citep{brochu2010bayesian}, evolutionary algorithms and hierarchical bandits~\citep{bubeck2010x}. We opt for another performance measure in this case, namely the simple regret: $\max_{x\in\mathcal{X}} f(x) - f(x_{j_t})$ with $x_{j_t}$ our guess at time $t$. In a recent work of ours, we provide a general wrapper for hierarchical bandit algorithms that only have guarantees for their simple regret~\cite{shang2019adaptive}. We show that with a cross-validation scheme, any hierarchical bandit algorithm with simple regret guarantees can be plugged into our meta-algorithm with only a tiny increase in the resulting simple regret.

\subsection{Hyper-parameter optimization}


\section{A Summary of the PhD}\label{sec:intro.contributions}

The full journey of the PhD is extremely rich and the present manuscript is by no means intended to introduce all the results I have obtained during that period, but rather tell a story about the bandit optimization research line. The purpose of this section is thus to provide a summary of the contributions included in this thesis, but also list all the other work I have done just for the record.

\subsection{Summary of the contributions in this thesis}\label{sec:intro.contributions.summary}

We can summarize the contributions presented in this thesis as listed below.

\subsection{List of publications and other work}\label{sec:intro.contributions.list}

\paragraph{List of publications included in this thesis}

\cite{shang2018adaptive,shang2019dttts,shang2019adaptive,shang2020dttts,degenne2020game,shang2020t3c}.

\paragraph{List of publications and work not included in this thesis.}

\cite{shang2020vector,shang2021safe,menard2021ucbmq}.

\paragraph{Software.}

\cite{rlberry2021}.

\section{Organization of the Thesis}\label{sec:intro.organization}

We conclude this chapter by providing an outline of the main text of the thesis.

% \newpage
% \bibliographystyle{plain}
% \bibliography{library}
