%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%									ANNEXES 												%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Mathematical Tools}\label{CHAP:MATHS}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reminders on Probability}\label{app:maths.proba}

$\cU([a,b])$ denotes a uniform distribution on the support $[a,b]$.

$\Beta(\cdot,\cdot)$ denotes a Beta distribution.

\subsection{One-dimensional exponential family}\label{app:maths.proba.exponential}

\begin{align}
    p_{X}(x \mid \theta ) = b(x) \exp \left[\eta (\theta ) \cdot T(x) + A(\theta )\right]\,.
\end{align}

In the whole thesis, we mainly used the following distributions from the one-dimensional exponential family.

\begin{itemize}
    \item $\Ber(\cdot)$ denotes a Bernoulli distribution.
    \item $\cB(\cdot)$ denotes a Binomial distribution.
    \item $\cN(\cdot,\cdot)$ denotes a normal distribution. Note that only normal distributions with known variance are in the one-dimensional exponential family.
\end{itemize}

%\subsection{Gaussian process}\label{app:maths.proba.gp}

\subsection{Sub-Gaussian distributions}\label{app:maths.proba.subgaussian}

\begin{definition}[sub-Gaussian]
\begin{leftbar}[defnbar]
Let $X$ be a random variable defined over a probability space $(\Omega,\mathcal{F},\Ps)$. $X$ is sub-Gaussian if there exists a constant $a\geq 0$ such that for any $\lambda\in\R$, we have
\[
    \EE{\exp(\lambda X)} \leq \exp\left\{\frac{a^2\lambda^2}{2}\right\}\,.
\]
And $X$ is called $a$-sub-Gaussian.
\end{leftbar}
\end{definition}

% \begin{example}[centered Gaussian is sub-Gaussian]
% \begin{leftbar}[examplebar]
% 	We are given a random variable $X \sim \cN(0,\sigma^2)$ with $\sigma^2$ denoting its variance. We show that $X$ is $\sigma$-sub-Gaussian. Indeed, we have $\forall \lambda\in\R$,
% 	\begin{align*}
% 		\EE{\exp(\lambda X)} &\leq \\
% 	\end{align*}
% \end{leftbar}
% \end{example}

\section{Concentration Inequalities}\label{app:maths.concentration}

\subsection{Hoeffding's inequality}

\subsection{Azuma's inequality}

%\subsection{Bernstein's inequality}

%\section{Reproducing Kernel Hilbert Space}\label{app:maths.rkhs}

\section{Information Theory}\label{app:maths.information}

In this section, we briefly recall some fundamental notions and results of information theory that are unceasingly used in the technical proofs of this thesis. Readers can refer to~\cite{cover2006} for more details.

\subsection{Entropy}\label{app:maths.information.entropy}

Given a random variable $X: \Omega \rightarrow \cX$, the \gls{entropy} $X$ measures its uncertainty, and also defines the ultimate data compression. When the random variable is discrete, its entropy $H(X)$ is defined as follow.

\begin{definition}[entropy]\label{def:entropy}
\begin{leftbar}[defnbar]
    Let $X$ be a discrete random variable defined over a probability space $(\Omega,\mathcal{F},\Ps)$ to an arbitrary space $\cX$, with probability mass function $p_X$, then its entropy $H(X)$ is defined by
    \[
        H(X) \eqdef - \sum_{x\in\cX} p_X(x)\log p_X(x)\,.
    \]
\end{leftbar}
\end{definition}

The previous definition can be extended to continuous random variables, namely \gls{differential entropy}.

\begin{definition}[differential entropy]
\begin{leftbar}[defnbar]
    Let $X$ be a continuous random variable defined over a probability space $(\Omega,\mathcal{F},\Ps)$, with probability density function $f$, then its differential entropy $h(X)$ is defined by
    \[
        h(X) \eqdef - \int f(x)\log f(x) dx\,.
    \]
\end{leftbar}
\end{definition}

\subsection{Kullback-Leibler divergence}\label{app:maths.information.kl}

 important concept is the \textit{relative entropy} or \textit{Kullback-Leibler divergence} (KL divergence), which measures the difference between two probability distributions.

\begin{remark}
\begin{leftbar}[remarkbar]
	KL divergence is not a distance since it does not satisfy the symmetry property in an usual distance definition.
\end{leftbar}
\end{remark}

Before properly defining the KL divergence, let us first give the definition of an important prerequisite notion of \textit{absolutely continuous} probability measures.

\begin{definition}[absolutely continuous probability measures]\label{def:maths.absolute_continuous}
\begin{leftbar}[defnbar]
	Let $\mathbb{P}$ and $\mathbb{Q}$ be two probability measures defined on a measurable space $(\Omega,\mathcal{F})$. If for any event $F \in \mathcal{F}$ such that $\mathbb{Q}(F) = 0$, we have also $\mathbb{P}(F) = 0$, then one says that $\mathbb{P}$ is absolutely continuous w.r.t $\mathbb{Q}$, and is denoted as $\mathbb{P} \ll \mathbb{Q}$.
\end{leftbar}
\end{definition}

The KL divergence is then defined as follow.

\begin{definition}[KL divergence]\label{def:maths.kl}
\begin{leftbar}[defnbar]
	For two probability measures $\mathbb{P}$ and $\mathbb{Q}$, if $\mathbb{P} \ll \mathbb{Q}$, then the KL divergence is defined as
	\[
		KL(\mathbb{P} \lVert \mathbb{Q}) \eqdef \int \log(\frac{d\mathbb{P}}{d\mathbb{Q}})d\mathbb{P}.
	\]
\end{leftbar}
\end{definition}

A very important property of KL divergence is its non-negativity, which is established by Gibbs' theory.

\begin{theorem}[Gibbs' inequality]\label{thm:maths.gibbs}
\begin{leftbar}[theorembar]
	For two probability measures $\mathbb{P}$ and $\mathbb{Q}$, if $\mathbb{P} \ll \mathbb{Q}$, then $KL(\mathbb{P} \lVert \mathbb{Q}) \geq 0$. The equality holds if and only if $\mathbb{P} = \mathbb{Q}$ $\mathbb{P}$-almost everywhere.
\end{leftbar}
\end{theorem}

% \begin{proof}
% 	We prove the result in the discrete case. Suppose $P = \{p_1, p_2, \ldots, p_n\}$ and $Q = \{q_1, q_2, \ldots, q_n\}$ two probability distributions.

% 	We know that $\log(\cdot)$ is a concave function, thus $-\log(\cdot)$ is a convex function. Using Jensen's inequality, we have
% 	\begin{align*}
% 		D(\mathbb{P} \lVert \mathbb{Q}) & = \sum_{i=1}^n p_i \log(p_i) \frac{\log(p_i)}{\log(q_i)} \\
% 										& = - \sum_{i=1}^n p_i \log(p_i) \frac{\log(q_i)}{\log(p_i)} \\
% 										& \geq - \log (\sum_{i=1}^n p_i \frac{q_i}{p_i}) \\
% 										& = 0.
% 	\end{align*}
% \end{proof}

\subsection{Two special cases: Gaussian and Bernoulli}\label{app:maths.information.examples}

For probability distributions in the one-dimensional exponential family, we can simply represent the KL-divergence by their means. For example, if $\mu_1$ and $\mu_2$ are respectively the means of $\mathbb{P}$ and $\mathbb{Q}$, then we can write
\[
    KL(\mathbb{P};\mathbb{Q}) = d(\mu_1;\mu_2)\,.
\]

For some particular probability distributions, simple closed-form expressions can be deduced. In Example~\ref{ex:maths.kl_gaussian} and Example~\ref{ex:maths.kl_bernoulli}, we show the expressions for Gaussian and Bernoulli distributions.

\begin{example}[KL-divergence between two Gaussian distributions]\label{ex:maths.kl_gaussian}
\begin{leftbar}[examplebar]
	We compute the KL divergence between two normal distributions $\mathbb{P}\sim\cN(\mu_1,\sigma_1)$ and $\mathbb{Q}\sim\cN(\mu_2,\sigma_2)$.
	\[
		KL(\mathbb{P} \lVert \mathbb{Q}) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}.
	\]
In particular, if $\sigma\eqdef\sigma_1=\sigma_2$, then the two distributions are parameterized by their means, we can thus denote by
	\[
		d(\mu_1;\mu_2) \eqdef KL(\mathbb{P} \lVert \mathbb{Q}) = \frac{(\mu_1-\mu_2)^2}{2\sigma^2}.
	\]
\end{leftbar}
\end{example}

\begin{example}[KL divergence between two Bernoulli distributions]\label{ex:maths.kl_bernoulli}
\begin{leftbar}[examplebar]
    We compute the KL divergence between two Bernoulli distributions $\mathbb{P}\sim\Ber(\mu_1)$ and $\mathbb{Q}\sim\Ber(\mu_2)$, we denote by
     \[
        kl(\mu_1;\mu_2) \eqdef KL(\mathbb{P} \lVert \mathbb{Q}) =  \mu_1 \ln \left( \frac{\mu_1}{\mu_2} \right) + (1-\mu_1) \ln  \left( \frac{1-\mu_1}{1-\mu_2} \right).
    \]
\end{leftbar}
\end{example}
