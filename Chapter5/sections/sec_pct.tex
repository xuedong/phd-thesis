% !TEX root = ../Chapter5.tex
\section{\texorpdfstring{\PCT}{} That Does Not Need to Know the Smoothness}\label{sec:gpo.pct}

In this section, we formally introduce a generic $\POO(\cA)$ algorithm, taking as input \emph{any} algorithm $\cA=\cA(\nu,\rho)$ requiring the smoothness parameters. We provide a simple regret upper bound for a particular instantiation called \PCT, for \POO\!(\HCT).

\subsection{Generic parallel optimistic optimization}

$\POO(\cA)$ (\textbf{p}arallel \textbf{o}ptimistic \textbf{o}ptimization) is a meta-algorithm that uses any hierarchical optimization algorithm $\cA$ that knows the smoothness as a subroutine, originally proposed by \cite{grill2015poo} for $\cA = \HOO$. In this algorithm, several instances of $\cA$ are run in parallel, each one using a different pair of parameters $(\nu,\rho)$ in a well-chosen grid~$\cG$ (defined in Line~4 of Algorithm~\ref{alg:pct}). In the end, $\POO(\cA)$ chooses the instance that has the largest empirical mean reward and returns one of the points evaluated by this instance, chosen uniformly at random.

The pseudo-code of $\POO(\cA)$ is shown in Algorithm~\ref{alg:pct}. Additionally to the base algorithm itself, it requires two parameters $\rho_{\max}$ and $\nu_{\max}$ that determine the range of $\cA(\nu,\rho)$ instances that we can compete with. However, these parameters can be set as a function of the number of evaluations as explained in details in Appendix C of~\cite{grill2015poo}, hence not mandatory in practice. An important remark is that given a budget $n$ of function evaluations, the number of $\cA$ instances $N$ run by $\POO(\cA)$ depends on $n$, and each instance is run for $\floor{n/N(n)}$ times. Due to the doubling scheme used in Lines~2-10, note however that $\POO(\cA)$ does not need to know this total number of function evaluations. Hence, if the base algorithm $\cA$ is anytime, so is $\POO(\cA)$.

\begin{algorithm}[ht]
\centering
\caption{$\POO(\cA)$ - parallel optimistic optimization with base algorithm $\cA$}
\label{alg:pct}
\begin{algorithmic}[1]
    \State {\bfseries Input:} base algorithm $\cA$, $\nu_{\max}$,$\rho_{\max}$, branching factor of the partitioning $K$
    \State {\bfseries Initialization:} $D_{\max} \gets \ln K/\ln\left( 1/\rho_{\max}\right)$, number of function evaluations $n \gets 0$, current number of instances of $\cA$: $N \gets 1$, $\mathcal{S} \gets \{(\nu_{\max},\rho_{\max})\}$
    
    \While{budget still available}
        \While{$N \le \tfrac{1}{2}D_{\max}\log\left(n/(\log n)\right)$}
            \For{$i\gets1, \dots, N$}
        		\State $s \gets \left(\nu_{\max},{\rho_{\max}}^{2N/(2i+1)}\right)$
        		\State \texttt{initialize} $\cA(s)$ (if not already done before) 
        		\State \texttt{continue running} $\cA(s)$ \texttt{until it has given} $\frac{n}{N}$ \texttt{rewards} $r_{s,1},\ldots,r_{s,n/N}$.
        		\State \texttt{compute} $\widehat{\mu}[s] = \frac{N}{n}\sum_{i=1}^{n/N}r_{s,i}.$
        	\EndFor
            \State $n \gets 2n$
            \State $N \gets 2N$
        \EndWhile
        \State \texttt{perform each $\cA(s)$ once} 
        \State \texttt{update} $\widehat{\mu}[s]$
        \State $n \gets n+N$
    \EndWhile
    \State $s^\star \gets \argmax_{s\in\mathcal{S}}~\widehat{\mu}[s]$
    \State {\bfseries Return} A point sampled u.a.r.\,from the points evaluated by $\cA(s^\star)$
\end{algorithmic}
\end{algorithm}

\subsection{Upper bound on the simple regret of \PCT{}}

Building on our new analysis of the \HCT algorithm, we are able to provide theoretical guarantees for the resulting $\POO(\HCT)$ algorithm, that we refer to as \PCT (\textbf{p}arallel \textbf{c}onfidence \textbf{t}ree). More precisely we define $\PCT(\delta)$ as \POO run on top of \HCT using confidence parameter~$\delta$.

Letting $(\nu^\star,C^\star,\rho^\star)$ be a triple of parameters for which Assumption~\ref{ass1} is true, we prove that \PCT achieves a regret that is comparable to the one obtained by $c$.

%\todo[inline]{below a formula for $N(n)$ is needed}

\begin{theorem}\label{thm:pct}
Assume that the target function $f$ satisfies Assumption~\ref{ass1} and $\nu^\star \leq \nu_{\max}$ and $\rho^\star \leq \rho_{\max}$. For $\delta = N(n)/n$ with $N(n) = \lceil (1/2)D_{\max}\log(n/\log n)\rceil$, the simple regret of $\PCT(\delta)$ after $n$ function evaluations is bounded as
\[
	\mathbb{E}[S_n^{\PCT(\delta)}]  \leq \beta D_{\max}(\nu_{\max}/\nu^\star)^{D_{\max}} \left( ((\log^2 n)/n)^{1/(d(\nu^{\star},C^\star,\rho^{\star})+2)} \right),
\]
where $\beta$ is a constant independent of $\nu_{\max}$ and $\rho_{\max}$.\footnote{More generally, Theorem~\ref{thm:pct} holds for any $\nu \leq \nu_{\max}$ and $\rho \leq \rho_{\max}$.}
\end{theorem}
By Corollary~\ref{col}, we know that the simple regret of \HCT after $n$ function evaluations run with $(\nu^\star,C^\star,\rho^\star)$ is of order $\cO\left( (\log n/n)^{1/(d(\nu^{\star},C^\star,\rho^{\star})+2)} \right)$. As a consequence, the performance of \PCT is at most a $\sqrt{\log n}$ factor away from that of the best \HCT instance.

Theorem~\ref{thm:pct} follows from Corollary~\ref{col} and Proposition~\ref{prop:WrapperPOO} below. This wrapper result highlights how cumulative regret guarantees for \emph{any} base algorithm translate into simple regret guarantees for the corresponding $\POO(\cA)$ algorithm. Its proof almost replicates the analysis of \POO{}(\HOO) by \cite{grill2015poo} and we provide it in Appendix~\ref{app:gpo.poo} for the sake of completeness.  

\begin{proposition}\label{prop:WrapperPOO} If for all $(\nu,\rho)$ the $\cA(\nu,\rho)$ algorithm has its \emph{cumulative regret} bounded as
\begin{equation}
    \mathbb{E}\left[R_n^{\cA(\nu,\rho)}\right] \leq \alpha C(\log n)^{1/(d(\nu,C,\rho)+2)}n^{(d(\nu,C,\rho)+1)/(c+2)},\label{centralConditionPOO}
\end{equation}
for any function $f$ satisfying Assumption~\ref{ass1} with parameters $(\nu,C,\rho)$, then there exists a constant $\beta$ that is independent of $\nu_{\max}$ and $\rho_{\max}$ such that 
\[
    \mathbb{E}\left[S_n^{\POO(\cA)}\right]  \leq \beta D_{\max}(\nu_{\max}/\nu^\star)^{D_{\max}} \left( (\log^2 n)/n)^{1/(d(\nu^\star,C^\star,\rho^\star)+2)} \right)\!,
\]
for any function $f$ satisfying Assumption~\ref{ass1} with parameters $\nu^\star \leq \nu_{\max}$ and $\rho^\star\leq \rho_{\max}$.
\end{proposition}
