\section{Best-Arm Identification for HPO}\label{sec:dttts.bai}

Hyper-parameter optimization can  be modeled as a BAI task in a bandit model. Given a finite set of arms $\cA \eqdef \{1,\ldots,K\}$,  when we select arm $i$, we get an independent observation from some unknown distribution $\nu_i$ with mean $\mu_i$. A BAI algorithm sequentially selects arms in order to identify the arm with the largest mean, $I^\star \eqdef \argmax_{i\in\cA}\mu_i$\footnote{Here we present BAI problems in a standard way for which we search for an arm with the largest mean. For HPO, however, it is important to mention that we are searching for a hyper-parameter configuration that minimizes the validation error. One can easily see that it does not change the problem in principle.}. In the context of HPO, each arm models the quality of a given hyper-parameter configuration $\bm\lambda$. When the arm is sampled, a noisy evaluation of $f(\bm\lambda)$ is received, which is the mean reward of that arm.
A BAI (or pure-exploration) algorithm consists of a sequential arm selection strategy, indicating which arm $I_t$ is selected at round $t$, coupled with \emph{recommendation rule} that selects a candidate best arm ${I}^\star_t$ at round $t$. The goal is either to minimize the error probability $\bP(\mu_{{I}^\star_t}\neq\mu_{I^\star})$ \citep{audibert2010budget,karnin2013sha} or the \emph{simple regret}~\citep{bubeck2009pure,gabillon2012ugape}, defined as 
$r_t = \mu_{I^\star} - \mu_{{I}^\star_t}$, possibly after a total budget $B$, whose knowledge may be used by the algorithm. Note that the BAI problem can also be studied from a fixed-confidence point of view \citep{even-dar2003confidence}. 

Standard BAI algorithms are however not straightforwardly applicable to HPO when the search space can be infinite and is often continuous. To handle those interesting cases, we rather turn our attention to BAI in a infinitely many armed bandit \citep{carpentier2015siri}. In this context, there is an infinite pool of arms, whose means are assumed to be drawn from some \emph{reservoir distribution} $\nu_0$. In such a model, a BAI algorithm maintains a list of arms (hyper-parameter configurations) that have been tried before. At each round it can either query a new arm from the reservoir (add a new hyper-parameter, selected at random, to the current pool), add it to the list and sample it (evaluate it), or sample an arm already in the list (re-evaluate a configuration tried before).     

A natural way to perform  BAI in an infinite-many armed bandit model consists in first querying a \emph{well-chosen} number of arms from the reservoir and then running a standard BAI algorithm on those arms \citep{carpentier2015siri}. However this ideal number may rely on the difficult of the learning task, which is hardly known in practice. The \Hyperband{} algorithm \citep{li2017hyperband} takes a step further and successively queries several batches of arms from the reservoir, including a decreasing number of arms in each batch, while increasing the budget dedicated to each of them. \texttt{SHA} \citep{karnin2013sha}, a state-of-the-art BAI algorithm, is then run on each of these batches of arms. This approach seems more robust in that it trades off between \emph{the number of arms that is needed to capture a good arm} and \emph{how much measurement effort we should allocate to each of them}. However, a numerical study performed by \cite{aziz2018infinite} seems to reveal that an infinite bandit algorithm based on \texttt{SHA} should always query the maximal number of arms from the reservoir\footnote{This reference is a preliminary draft that has been withdrawn due to technical issues in the proofs. Yet we believe the experimental section to be sound.}. 

All the existing algorithms are still subject to a pre-defined scheduling of how many arms should be queried from the reservoir. The algorithm (\DTTTS) we proposed in this paper does not need to decide in advance how many arms will be queried, and is therefore fully \emph{dynamic}. %The algorithm is inspired by a variant of Thompson sampling~\citep{thompson1933} called Top-Two Thompson sampling (\TTTS,~\citealt{russo2016ttts}). 

\begin{remark}
\Hyperband is proposed specifically for hyper-parameter tuning. Its original philosophy is to adaptively allocate \emph{resources} to more promising configurations. Resources here can be time, dataset sub-sampling, feature sub-sampling, etc. In such a setting, the classifier is not always trained into completion given a parameter configuration, but is rather stopped early if it is shown to be bad so that we can allocate more resources to other configurations. In this case, different evaluations of a single configuration cannot be considered as i.i.d. anymore. Thus, HPO is stated as a non-stochastic infinitely-armed bandit problem. This idea of early stopping is also further investigated by combining Bayesian optimization with it~\citep{falkner2018bohb}. However, this is out of the scope of this paper.
\end{remark}
