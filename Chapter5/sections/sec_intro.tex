\section{Introduction}\label{sec:dttts.intro}

Training a machine learning algorithm often requires to specify several parameters. For instance, for neural networks, it is the architecture of the network and also the parameters of the gradient algorithm used or the choice of regularization. These \emph{hyper-parameters} are difficult to learn through the standard training process and are often manually specified.
%\todo{some ref so this is not just blabla}

When it is not feasible to design algorithms with a few hyper-parameters, we opt for \emph{hyper-parameter optimization} (HPO).  HPO can be viewed as a \emph{black-box optimization} problem where the evaluation of the objective function is expensive as it is the accuracy of a learning algorithm for a given configuration of hyper-parameters. %Indeed, a typical function evaluation involves training the primary machine learning algorithm to completion on a large dataset, which often takes a considerable amount of time or resources. 
This vastly limits the number of evaluations that can be carried out, which calls for a design of efficient high-level algorithms that automate the tuning procedure.

Several naive but daily used HPO methods are  grid search and random search. More sophisticated methods address HPO as a \emph{sequential resource allocation problem}, by adaptively choosing the next hyper-parameter(s) to explore, based on the result obtained previously. For example, \emph{evolutionary optimization} follows a process inspired by the biological concept of \emph{evolution}, which repeatedly replaces the worst-performing hyper-parameter configurations from a randomly initialized population of solutions; see~\citet{loshchilov2016cmaes} for an example of using \texttt{CMA-ES} for hyper-parameter tuning. A major drawback of evolutionary optimization is its lack of theoretical understanding.

Bayesian optimization (BO) is another approach that leverages the sequential nature of the setting. BO depends on a prior belief for the target function, typically a Gaussian process. This prior distribution can be updated to a posterior given a sequence of observations. Several algorithms exploiting this posterior distribution to decide where to sample next have been given (see  \citealp{shahriari2016loop}, for a survey). \citet{snoek2012spearmint} and \citet{klein2017robo} provide Python packages called \Spearmint\ and \RoBO\ to perform hyper-parameter tuning with BO methods. Similar packages are available for \PyTorch\ (\BoTorch\footnote{\url{https://botorch.org/}}) and \TensorFlow\ (\flow\ by~\citealt{knudde2017gpflowopt}). Among BO algorithms, \TPE~\citep{bergstra2011tpe} and \SMAC~\citep{hutter2011smac} were specifically proposed for HPO. A shortcoming of BO is that most algorithms select where to sample next based on optimizing some \emph{acquisition function} computed from the posterior, e.g., the expected improvement~\citep{jones1998ei}. This auxiliary  task cannot be solved analytically but needs to be performed itself by optimization procedures as \LBFGS that make the process slow. %although this function is usually more regular than the target function.  

Bandits (see \citealp{lattimore2018} for a recent book) are a simple model for sequential resource allocation, and some bandit tools have already been explored for global optimization and HPO: First, in the field of Bayesian optimization, the \GPUCB algorithm \citep{srinivas2010gpucb} is a Gaussian process extension of the classical \UCB bandit algorithm \citep{auer2002ucb}. Later, \citet{hoffman2014bayesgap} proposed to use \emph{best-arm identification} (BAI) tools---still with a Bayesian flavor---for automated machine learning, where the goal is to smartly try hyper-parameters from a pre-specified \emph{finite} grid. 

However, in most cases, the number of hyper-parameter configurations to explore is infinite. In this paper, we investigate the use of bandit tools suited for an \emph{infinite} number of arms. There are two lines of work for tackling a very large or infinite number of configurations (arms). The first combines standard bandit tools with a hierarchical partitioning of the arm space and aims at exploiting the (possibly unknown) smoothness of the black-box function to optimize~\citep{bubeck2010x,grill2015poo,shang2019adaptive,bartlett2019simple}. To the best of our knowledge, these methods have never been investigated for HPO. The second line of work does not assume any smoothness: At each round, the learner may ask for a new arm from a \emph{reservoir distribution} $\nu_0$ (pick randomly a new hyper-parameter configuration) and add it to the current arm pool $\cA$, or re-sample one of the previous arms (evaluate configuration already included in $\cA$), in order to find an arm with a good mean reward (i.e., a hyper-parameter configuration with a good validation accuracy). The \emph{stochastic infinitely many-armed bandits} (SIAB) is studied by \citet{berry1997infinite,wang2008ucbv} for the rewards maximization problem while \citet{carpentier2015siri,aziz2018confidence} %\todo{add 2 other recent papers on this topic} 
study the simple regret problem, which is related to BAI. While most proposed algorithms consist of querying an \emph{adequate} number of arms from the reservoir before running a standard BAI algorithm, \cite{li2017hyperband} propose a more robust approach called \Hyperband that uses several such phases.

In this paper, we go even further and propose the first \emph{dynamic} algorithm for BAI in SIAB, that at each round, may either query a new arm from the reservoir or re-sample arms previously queried. Our algorithm leverages a Bayesian model and builds on the top-two Thompson sampling (\TTTS) algorithm by~\citet{russo2016ttts}. %We also introduce a variant of \Hyperband where the \texttt{SHA} sub-routine \citep{karnin2013sha} is replaced by \TTTS. 
An extensive numerical study is presented to show the competitiveness of our algorithm with respect to state-of-the-art HPO methods.
%\todo{cut this in space needed}
%\paragraph{Outline} We first introduce a general framework for hyper-parameter optimization in Section~\ref{sec:framework}. In Section~\ref{sec:bai}, we explain how infinitely-many armed bandit algorithms can be used for HPO, before presenting our two new algorithms in Section~\ref{sec:algo}. Finally, we provide experimental results in Section~\ref{sec:result}. % before concluding.
