% !TEX root = ../Chapter5.tex
\section{Introduction}\label{sec:gpo.intro}

In this chapter, we adopt another perspective on sequential optimization. We are still interested in identifying the best arm, but this time among an infinite number of candidates. The search space can be countably infinite or even continuous. That is the global optimization. 

\gls{go} has applications in several domains including hyper-parameter tuning \citep{jamieson2016hyperband, li2017hyperband,samothrakis2013} which is the main topic of Chapter~\ref{CHAP:DTTTS}. \gls{go} usually consists of a data-driven optimization process over an expensive-to-evaluate function. It is also known as \gls{bbo} since the inner behavior of a function is often unknown.

Contrary to Chapter~\ref{CHAP:T3C} and Chapter~\ref{CHAP:LGC}, we are interested in a budgeted setting in this Chapter as we are subject to a high resource-consuming target, hence a pre-defined budget limit can be favored. In budgeted function optimization, a learner optimizes a function $f: \cX \rightarrow \R$ depending on a number of function evaluations limited by $N$ which are sequentially selected. For each of the $N$ evaluations, at round $n$, the learner
picks an element $x_n\in\cX$ and observes a real number $r_n$, where 
\[
    r_n = f(x_n) + \epsilon_n\,,
\]
with $\epsilon_n$ the noise. At the end, the learner is supposed to output a guess of the optimum and their performance is assessed by the simple regret (see Definition~\ref{def:mab.simple_regret}). The true function value $f(x)$ can thus be interpreted as the arm mean of $x$ using MAB terminology. 

Based on $\epsilon_n$, we can distinguish \emph{deterministic} feedback setting and \emph{stochastic} feed setting. For deterministic feedback, the function evaluations are noiseless (see e.g.~\citealt{defreitas2012gp} for motivation and applications). We pay our attention to the stochastic setting where the noise is assumed to be independent from past observations: $\EE{r_n|x_n} = f(x_n)$.

Treating the problem without any further assumption would be a \emph{mission impossible}. However, the setting gets easier if we assume a global smoothness of the reward function~\citep{agrawal1995continuum,kleinberg2004nearly,kleinberg2008multi,cope2009,auer2007improved,slivkins2011taxonomy,kleinberg2013}. A weaker condition is some \emph{local} smoothness where only neighborhoods around the maximum are required to be smooth.  In fact, local smoothness is sufficient for achieving near-optimality~\citep{valko2013stosoo,azar2014online,grill2015poo,bull2015adaptive}. This is the \gls{continuum-armed bandits} setting.

We base our work on optimistic tree-based optimization algorithms~\citep{munos2011soo,valko2013stosoo,preux2014bandits,azar2014online} that approach the problem with a hierarchical partitioning of the arm space and take the \textit{optimistic principle}. This idea comes from \emph{planning} in MDP~\citep{kocsis2006bandit,munos2014,grill2016trail}.

Our work is motivated by the \gls{poo} approach proposed by~\citet{grill2015poo}, that \emph{adapts to the smoothness} without the knowledge of it. \POO is a \emph{meta-algorithm} which can be used on top of any hierarchical optimization algorithm that \emph{knows the smoothness}, that we call a subroutine. Not only does \POO require only the mildest local regularity conditions, but it also gets rid of the unnecessary metric assumption that is often required. Local smoothness naturally covers a larger class of functions than global smoothness, yet still assures that the function does not decrease too fast around the maximum. We highlight that the analysis of \POO{} is modular: Assuming the subroutine has a \textbf{cumulative} regret of order $R_N$ \emph{under a local smoothness assumption with respect to a fixed partitioning} (\citealt{grill2015poo}, Assumption~\ref{ass:gpo.local}, formally introduced in Section~\ref{sec:gpo.pre}), \POO{} run with such subroutine has a \textbf{simple} regret bounded by $R_N \sqrt{\log N}$. 

The analysis of \POO{} heavily relies on the subroutine having guarantees on the cumulative regret. In the context of optimization where sometimes only simple regret guarantees are available, this is not always desirable as requirements. In this chapter, we provide a more general wrapper for algorithms that only have guarantees on their simple regret, called \gls{gpo}. We show that with a cross-validation scheme instead of the original recommendation strategy, any hierarchical bandit algorithm with simple regret guarantee can be plugged into \GPO{} with only a tiny increase in the resulting simple regret. Note that any subroutine that is able to underpin \POO{} can do the same for \GPO{} while the converse is not necessarily true. This can be explained by~\ref{remark:mab.simple} as good cumulative regret guarantees imply good simple regret guarantees but not the converse.

To validate \GPO{}, it is necessary to find a subroutine that achieves meaningful regret bound (can be either cumulative regret or simple regret) under Assumption~\ref{ass:gpo.local}. A natural candidate can refer to the subroutine of \POO{}. \POO{} was originally analyzed using \gls{hoo} as its base algorithm. However, unlike what~\cite{grill2015poo} hypothesize, it is non-trivial to provide a regret bound for \HOO{} under Assumption~\ref{ass:gpo.local}. We elaborate on that in Section~\ref{sec:gpo.hct}. In order to validate \POO{} as well as \GPO{}, there needs to exist a subroutine with a regret guarantee that is provable under Assumption~\ref{ass:gpo.local}. This is another message that we deliver.

In particular, we prove that \HCT-$\operatorname{iid}$, denoted by \gls{hct} in the rest of the paper since we do not consider the correlated feedback setting, of~\cite{azar2014online} satisfies the required regret guarantee, and is, therefore, a desirable subroutine to be plugged in \POO{} and \GPO{}. Similar to \HOO{}, \HCT{} is a hierarchical optimization algorithm based on confidence intervals. However, unlike \HOO{}, these confidence intervals are obtained by repeatedly sampling a representative point of each cell in the partitioning before splitting the cell. This yields partition trees that have a \emph{controlled depth}, which are easier to analyze under a local smoothness assumption with respect to the partitioning. Whether \HOO{} has similar regret guarantees under the desired local metricless assumption remains an open question.

\paragraph{Contributions.}
\textbf{1)} We propose to use a cross-validation scheme to wrap up algorithms that only possess simple regret guarantees at the cost of a slight loss in the final regret bound.
\textbf{2)} We show that \HCT{} can serve as a valid subroutine under desired assumptions.
\textbf{3)} We further provide numerical illustrations to show that \gls{hct} is empirically comparable to \gls{hoo} as a subroutine.

%\paragraph{Outline.}
%We first formulate the sequential optimization problem and introduce some preliminary notions and assumptions in Section~\ref{sec:gpo.pre}. Our main result is presented in Section~\ref{sec:gpo.hct}, where we provide a regret upper bound for \HCT under local smoothness with respect to the partitioning. In Section~\ref{sec:gpo.pct}, we present the instantiation of \POO studied in the paper that we call \PCT, in which the underlying subroutine \HOO is replaced by \HCT. We show that \PCT enjoys the same regret bound as \HCT up to a $\sqrt{\log n}$ factor. The general wrapper and its simple regret analysis are presented in Section~\ref{sec:gpo.gpo}. We conclude by some numerical simulations in Section~\ref{sec:gpo.experiments}.

%\todomi{I  suggest carefully and consistently applying editorial comments of Michael Littman: \url{http://cs.brown.edu/~mlittman/etc/style.html}.For example, you are using  citations as nouns.}
