%!TEX root = ../Chapter3.tex
\section{Bayesian BAI Strategies}\label{sec:t3c.algorithm}

In this section, we give an overview of the sampling rule \TTTS and introduce \TCC. We provide details for Bayesian updating for Gaussian and Bernoulli models respectively, and introduce associated Bayesian stopping and recommendation rules. 

\subsection{Sampling rules}

Both \TTTS and \TCC employ a Bayesian machinery and make use of a prior distribution $\Pi_1$ over a set of parameters $\Theta$, that contains the unknown true parameter vector $\bmu$. Upon acquiring observations $(r_{I_1,1},\cdots,r_{I_{n-1},n-1})$, we update our beliefs according to Bayes' rule and obtain a posterior distribution $\Pi_{n}$ which we assume to have density $\pi_n$ w.r.t.\,the Lebesgue measure. %The posterior distribution can be computed by conjugacy when using common probability distributions for the prior. For example, \citet{russo2016ttts} uses \emph{correlated} and \emph{bounded} one-dimensional exponential family priors.
\citeauthor{russo2016ttts}'s analysis is restricted to strong regularity properties on the models and priors that exclude two important useful cases we consider in this paper: (1) the observations of each arm~$i$ follow a Gaussian distribution $\cN(\mu_i,\sigma^2)$ with common known variance $\sigma^2$, with imposed Gaussian prior $\cN(\mu_{1,i},\sigma_{1,i}^2)$, (2) all arms receive Bernoulli rewards with unknown means, with a uniform prior on each arm.

\paragraph{Gaussian model.} For Gaussian bandits with a $\mathcal{N}(0,\kappa^2)$ prior on each mean, the posterior distribution of $\mu_i$ at round $n$ is Gaussian with mean and variance that are respectively given by
\[\frac{\sum_{\ell=1}^{n-1} \1\{I_\ell = i\} r_{I_\ell,\ell}}{T_{n,i} + \sigma^2/\kappa^2}\quad\text{ and }\quad \frac{\sigma^2}{T_{n,i} + \sigma^2/\kappa^2},\]
where $T_{n,i}\eqdef\sum_{\ell=1}^{n-1} \1{\{ I_{\ell} = i \}}$ is the number of selections of arm $i$ before round $n$.
% 
% \begin{align*}
% %\begin{equation*}
% 	\mu_{n,i} &=
% 	\left\{ \begin{array}{ll}
% 				\ddfrac{\sigma_{n-1,i}^{-2}\mu_{n-1,i}+\sigma^{-2}Y_{n-1,i}}{\sigma_{n-1,i}^{-2}+\sigma^{-2}} & \operatorname{if} I_{n-1} = i,\\
% 				\mu_{n-1,i} & \operatorname{otherwise, and}
% 			\end{array}\right.\\
% %\end{equation*}
% %\begin{equation*}
% 	\sigma_{n,i}^2 &=
% 	\left\{ \begin{array}{ll}
% 				\ddfrac{1}{\sigma_{n-1,i}^{-2}+\sigma^{-2}} & \operatorname{if} I_{n-1} = i,\\
% 				\sigma_{n-1,i}^2 & \operatorname{otherwise}.
% 			\end{array}\right.
% %\end{equation*}
% \end{align*}
For the sake of simplicity, we consider improper Gaussian priors with $\mu_{1,i}=0$ and $\sigma_{1,i}=+\infty$ for all $i\in\cX$, for which
\[
    \mu_{n,i}  = \frac{1}{T_{n,i}} \sum_{\ell=1}^{n-1} \1\{I_\ell = i\}r_{I_\ell,\ell} \quad \text{and} \quad \sigma_{n,i}^2 = \frac{\sigma^2}{T_{n,i}}.
\]
Observe that in that case the posterior mean $\mu_{n,i}$ coincides with the empirical mean.

\paragraph{Beta-Bernoulli model.} For Bernoulli bandits with a uniform ($\cB eta(1,1)$) prior on each mean, the posterior distribution of $\mu_i$ at round $n$ is a Beta distribution with shape parameters $\alpha_{n,i} = \sum_{\ell=1}^{n-1} \1{\{ I_{\ell} = i \}} r_{I_\ell,\ell} +1$ and $\beta_{n,i} = T_{n,i} - \sum_{\ell=1}^{n-1} \1{\{ I_{\ell} = i \}} r_{I_\ell,\ell} + 1$. 
%We can calculate the posterior mean of arm $i$ at time $n$ by
%\begin{align*}
%\mu_{n, i} = \frac{\alpha_{n,i}}{\alpha_{n,i} + \beta_{n, i}} = \frac{1 + \sum_{\ell=1}^{n-1} \1{\{ I_{\ell} = i \}} Y_{\ell,I_{\ell}}}{2 + T_{n,i}}.
%\end{align*}

Now we briefly recall \TTTS and introduce \TCC.

\paragraph{Description of \TTTS.} 
At each time step $n$, \TTTS has two potential actions: (1) with probability $\beta$, a parameter vector $\btheta$ is sampled from $\Pi_{n}$, and \TTTS chooses to play 
\[
    I_n^{(1)} \eqdef \argmax_{i\in\cX} \theta_i\,,
\] 
(2) and with probability $1-\beta$, the algorithm continues sampling new $\btheta'$ until we obtain a \emph{challenger} 
\[
I_n^{(2)} \eqdef \argmax_{i\in\cX} \theta_i'
\]
that is different from $I_n^{(1)}$, and \TTTS then selects the challenger.

\paragraph{Description of \TCC.} 
One drawback of \TTTS is that, in practice, when the posteriors become concentrated, it takes many Thompson samples before the challenger $I_n^{(2)}$ is obtained. We thus propose a variant of \TTTS, called \TCC, which alleviates this computational burden. Instead of re-sampling from the posterior until a different candidate appears, we define the challenger as the arm that has the lowest \emph{transportation cost} $W_n(I_n^{(1)},i)$ with respect to the first candidate (with ties broken uniformly at random). 

Let $\mu_{n,i}$ be the empirical mean of arm $i$ and 
\[
    \mu_{n,i,j} \eqdef \dfrac{(T_{n,i}\mu_{n,i} +T_{n,j}\mu_{n,j})}{(T_{n,i}+T_{n,j})}\,,
\]
then we define
\begin{equation}\label{def:Transportation}
	W_n(i,j) \eqdef
	\left\{ \begin{array}{ll}
				0 & \operatorname{if} \mu_{n,j} \geq \mu_{n,i},\\
				W_{n,i,j}+W_{n,j,i} & \operatorname{otherwise},
			\end{array}\right.
\end{equation}
where 
\[
    W_{n,i,j}\eqdef T_{n,i} d\left(\mu_{n,i},\mu_{n,i,j}\right)
\]
for any $i,j$. One may notice that we actually recover the generalized likelihood ratio statistic stated in Section~\ref{sec:mab.bai.stopping}.

Recall that we have closed-form expressions for the \texttt{KL}-divergence in some cases: in the Gaussian case, $d(\mu;\mu') = (\mu-\mu')^2/(2\sigma^2)$ while in the Bernoulli case $d(\mu;\mu') = \mu \ln (\mu/\mu') + (1-\mu)\ln (1-\mu)/(1-\mu')$ (see Appendix~\ref{app:maths.information.kl}).
%\begin{equation}\left\{\begin{array}{ccl}
%W_n(i,j)\!\!\! &=& \!\!0 \ \text{ if } \ \mu_{n,j} \geq \mu_{n,i},\\
%W_n(i,j)\!\!\! &=& \!\!T_{n,i} d\left(\mu_{n,i},\mu_{n,i,j}\right) + T_{n,j} d\left(\mu_{n,j},\mu_{n,i,j}\right) \text{ else.} 
%\end{array}\right.\label{def:Transportation}\end{equation}
For Gaussian bandits, we can further obtain a nice closed-form expression for the transportation cost, 
\[
    W_n(i,j) = \dfrac{(\mu_{n,i}-\mu_{n,j})^2}{2\sigma^2(1/T_{n,i}+1/T_{n,j})}\1\{\mu_{n,j}<\mu_{n,i}\}\,.
\]


The pseudo-code of \TTTS and \TCC are shown in Algorithm~\ref{alg:ttts_sampling_rule} and Algorithm~\ref{alg:t3c_sampling_rule}. Note that under the Gaussian model with improper priors, one should pull each arm once at the beginning for the sake of obtaining proper posteriors.

\begin{algorithm}[ht]
\centering
\caption{Sampling rule of \TTTS}
\label{alg:ttts_sampling_rule}
\footnotesize
\begin{algorithmic}[1]
   \State {\bfseries Input:} $\beta$ %(and the $W_n$ function for \textcolor{red}{\TCC})
   %\STATE {\bfseries Initialization:} $\forall \blambda\in\Omega, S_{\blambda}=0, F_{\blambda}=0$
   \For{$n \leftarrow 1,2,\cdots$}
        \State \text{Sample} $\btheta \sim \Pi_n$
        \State $I^{(1)} \leftarrow \argmax_{i\in\cX}\theta_i$
	    \State \text{Sample} $b \sim \cB ern(\beta)$
	    \If{$b = 1$}
	        \State \text{Evaluate arm} $I^{(1)}$
	    \Else
	        \State \text{Repeat sample} $\btheta' \sim \Pi_n$
            \State $I^{(2)} \leftarrow \argmax_{i\in\cX}\theta_i'$
	        \State \text{until} $I^{(2)} \neq I^{(1)}$
		    \State \text{Evaluate arm} $I^{(2)}$
	    \EndIf
	    \State \text{Update mean and variance}
	    \State $t = t+1$
   \EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\centering
\caption{Sampling rule of \textcolor{red}{\TCC}}
\label{alg:t3c_sampling_rule}
\footnotesize
\begin{algorithmic}[1]
   \State {\bfseries Input:} $\beta$ %(and the $W_n$ function for \textcolor{red}{\TCC})
   %\STATE {\bfseries Initialization:} $\forall \blambda\in\Omega, S_{\blambda}=0, F_{\blambda}=0$
   \For{$n \leftarrow 1,2,\cdots$}
        \State \text{Sample} $\btheta \sim \Pi_n$
        \State $I^{(1)} \leftarrow \argmax_{i\in\cX}\theta_i$
	    \State \text{Sample} $b \sim \cB ern(\beta)$
	    \If{$b = 1$}
	        \State \text{Evaluate arm} $I^{(1)}$
	    \Else
	        \State \textcolor{red}{$I^{(2)} \leftarrow \argmin_{i\neq I^{(1)}}W_n(I^{(1)},i), $ cf.\,\eqref{def:Transportation}}
		    \State \text{Evaluate arm} $I^{(2)}$
	    \EndIf
	    \State \text{Update mean and variance}
	    \State $t = t+1$
   \EndFor
\end{algorithmic}
\end{algorithm}

$W_n$ in Line 9 of Algorithm~\ref{alg:t3c_sampling_rule} is the transportation cost defined in (\ref{def:Transportation}).

%\begin{algorithm}[ht]
%\centering
%\caption{Sampling rule of \TTTS (Beta priors)}
%\label{alg:ttts_beta}
%\footnotesize
%\begin{algorithmic}[1]
%   \State {\bfseries Input:} $n$, $\beta$
%   %\STATE {\bfseries Initialization:} $\forall %\blambda\in\Omega, S_{\blambda}=0, F_{\blambda}=0$
%   \For{$t \leftarrow 1$ {\bfseries to} $n$}
%        \State $\forall i\in\cX$, \texttt{sample} $\theta_i \sim \cB eta(\alpha_{t-1,i}, \beta_{t-1,i})$
%        \State $I^{(1)} \leftarrow \argmax_{i\in\cX}\theta_i$
%	    \State \texttt{sample} $b \sim \cB ern(\beta)$
%	    \If{$b = 1$}
%	        \State \texttt{evaluate arm} $I^{(1)}$
%	    \Else
%	        \State \texttt{repeat} 
%	            \State \texttt{sample} $\theta_i' \sim \cB eta(\alpha_{t-1,i}, \beta_{t-1,i})$
%                \State $I^{(2)} \leftarrow \argmax_{i\in\cX}\theta_i'$
%	        \State \texttt{until} $I^{(2)} \neq I^{(1)}$
%		    \State \texttt{evaluate arm} $I^{(2)}$
%	    \EndIf
%	    \State \texttt{update mean and variance}
%	    \State $t = t+1$
%   \EndFor
%\end{algorithmic}
%\end{algorithm}

\subsection{Rationale for \TCC}

In order to explain how \TCC can be seen as an approximation of the re-sampling performed by \TTTS, we first need to define the \emph{optimal action probabilities}. 

\paragraph{Optimal action probability.} The optimal action probability $a_{n,i}$ is defined as the posterior probability that arm $i$ is optimal. Formally, letting $\Theta_i$ be the subset of $\Theta$ such that arm $i$ is the optimal arm,
\[
    \Theta_i \eqdef \left\{ \btheta\in\Theta \biggm| \theta_i > \max_{j\neq i}\theta_j \right\},
\]
then we define
\[
   \quad a_{n,i} \eqdef \Pi_{n}(\Theta_i) = \int_{\Theta_i} \pi_n(\btheta) \text{d} \btheta.
\]
With this notation, one can show that under \TTTS, 
\begin{equation}\label{CondDist}
    \Pi_n\left(I_n^{(2)} =j | I_n^{(1)} = i\right) = \frac{a_{n,j}}{\sum_{k\neq i} a_{n,k}}.
\end{equation}
Furthermore, when $i$ coincides with the empirical best mean (and this will often be the case for $I_n^{(1)}$ when $n$ is large due to posterior convergence) one can write 
\[a_{n,j} \simeq \Pi_n\left(\theta_j \geq \theta_{i}\right) \simeq \exp\left(-W_n(i,j)\right),\]
where the last step is justified in Lemma~\ref{lemma:gaussiantails} in the Gaussian case (and Lemma~\ref{lemma:binomial_tail} in Appendix~\ref{app:posterior_beta.aux} in the Bernoulli case). Hence, \TCC replaces sampling from the distribution \eqref{CondDist} by an approximation of its mode which is \emph{easy to compute}. Note that directly computing the mode would require to compute $a_{n,j}$, which is much more costly than the computation of $W_{n}(i,j)$\footnote{the \TTPS sampling rule \citep{russo2016ttts} also requires the computation of $a_{n,i}$, thus we do not report simulations for this Bayesian sampling rule in Section~\ref{sec:t3c.experiments}}. 

\subsection{Stopping and decision rules}

In order to use \TTTS or \TCC as sampling rule for fixed-confidence BAI, we need to additionally define stopping and decision rules. While \cite{qin2017ttei} suggest to couple \TTEI with the ``frequentist'' Chernoff stopping rule \citep{garivier2016tracknstop}, we propose in this section natural Bayesian stopping and recommendation rule. They both rely on the optimal action probabilities defined above.

\paragraph{Bayesian recommendation rule.} 
At time step $n$, a natural candidate for the best arm is the arm with largest optimal action probability, hence we define 
\[
    J_n \eqdef \argmax_{i\in\cX} a_{n,i}.
\]

\paragraph{Bayesian stopping rule.}
In view of the recommendation rule, it is natural to stop when the posterior probability that the recommended action is optimal is large, and exceeds some threshold $c_{n,\delta}$ which gets close to 1. Hence our Bayesian stopping rule is \begin{equation}\label{eq:stopping}
    \tau_{\delta} \eqdef \inf \left\{ n\in\NN:\max_{i\in\cX} a_{n,i} \geq c_{n,\delta} \right\}.
\end{equation}

\paragraph{Links with frequentist counterparts.} 
Using the transportation cost $W_n(i,j)$ defined in \eqref{def:Transportation}, the Chernoff stopping rule of~\cite{garivier2016tracknstop} can actually be rewritten as
\begin{equation}\label{eq:chernoffstoppingtime}
    \tau_\delta^{\text{Ch.}} \eqdef \inf \left\lbrace n \in \NN : \max_{i \in \cX} \min_{j \in \cX \setminus \{i\} } W_{n}(i,j) > d_{n,\delta} \right\rbrace\,.
\end{equation}
This stopping rule coupled with the recommendation rule $J_n = \argmax_{i} \mu_{n,i}$. 

As explained in that paper, $W_{n}(i,j)$ can be interpreted as a (log) Generalized Likelihood Ratio statistic for rejecting the hypothesis $\cH_0 : (\mu_i < \mu_j)$. Through our Bayesian lens, we rather have in mind the approximation $\Pi_n(\theta_j > \theta_i) \simeq \expp{-W_n(i,j)}$, valid when $\mu_{n,i}> \mu_{n,j}$, which permits to analyze the two stopping rules using similar tools, as will be seen in the proof of Theorem~\ref{thm:t3c.pac}. 

As shown later in Section~\ref{sec:t3c.confidence}, $\tau_\delta$ and $\tau_\delta^{\text{Ch.}}$ prove to be fairly similar for some corresponding choices of the thresholds $c_{n,\delta}$ and $d_{n,\delta}$. This endorses the use of the Chernoff stopping rule in practice, which does not require the (heavy) computation of optimal action probabilities. Still, our sample complexity analysis applies to the two stopping rules, and we believe that a frequentist sample complexity analysis of a fully Bayesian BAI strategy is a nice theoretical contribution.

\paragraph{Useful notation.}

% Rianne: we already defined this earlier, except for the full vector. 
%In our analysis to follow, we shall quantify the proportion of samples that is allocated by the sampling rule to each arm. To measure this allocation effort, one can use $T_{n,i} \eqdef \sum_{l=1}^{n-1} \1\{I_l = i\}$, the number of selections of arm $i$ before round $n$. We shall denote by $\bT_n$ the vector of number of arm selections.

%\emilie{not clear this is the place to define $\bT_n$, it's not use anytime soon}
%\rh{We already defined $T_{n,i}$ earlier, so I commented this section now. See later where we need $\bT_n$.}
%RIANNE: moved the above the appendix

We follow the notation of \citet{russo2016ttts} and define the following measures of effort allocated to arm $i$ up to time $n$,
\begin{align*}
    \psi_{n,i} \eqdef \PP{I_n = i | \cF_{n-1}}\quad \text{and} \quad \Psi_{n,i} \eqdef \sum_{l=1}^n \psi_{l,i}.
\end{align*}

In particular, for \TTTS we have
\[
    \psi_{n,i} =  \beta a_{n,i} + (1-\beta) a_{n,i}\sum_{j\neq i} \frac{a_{n,j}}{1-a_{n,j}},
\]
while for \TCC
\[
    \psi_{n,i} = \beta a_{n,i} + (1-\beta) \sum_{j\neq i} a_{n,j}\frac{\1\{W_n(j,i)=\min_{k\neq j} W_n(j,k)\}}{\#\left|\argmin_{k\neq j } W_n(j,k)\right|}.
\]

Recall that $\Sigma_K = \{\bomega : \sum_{k=1}^K \omega_k = 1\}$ is the probability simplex of dimension $K$.
