%!TEX root = ../Chapter3.tex
\section{Introduction}\label{sec:t3c.intro}

In this chapter we study the very original setting of BAI. Recall that we consider a finite-armed bandit model $\cX\eqdef\{1,\cdots,K\}$, parameterized by their means $\mu_1, \cdots, \mu_K$. And we focus on the fixed-confidence setting, introduced by~\cite{even-dar2003confidence}, in which given a risk parameter $\delta$, the goal is to ensure that the probability to stop and recommend a wrong arm, $\PP{J_\tau \neq I^\star}$, is smaller than $\delta$, while minimizing the expected total number of samples to make this accurate recommendation, $\EE{\tau}$. 

%The most studied alternative is the fixed-budget setting for which the stopping rule $\tau$ is fixed to some (known) maximal budget $n$, and the goal is to minimize the error probability $\PP{J_n \neq I^\star}$ \citep{audibert2010budget}.

As already elaborated in Chapter~\ref{CHAP:MAB} that most of the existing sampling rules for the fixed-confidence setting depend on the risk parameter $\delta$, and they either rely on careful construction of confidence intervals and use of OFU or arm eliminations. The first known sampling rule for BAI that does not depend on $\delta$ is the \emph{tracking} rules proposed by~\cite{garivier2016tracknstop}, which is proved to achieve the minimal sample complexity when combined with the Chernoff stopping rule as $\delta$ goes to zero. Such an \emph{anytime} sampling rule (neither depending on a risk $\delta$ or a budget $N$) is very appealing for applications, as advocated by~\cite{jun2016atlucb}, who introduce the anytime best-arm identification framework. 

%\emilie{a (different type of) tracking rule was indeed proposed by Antos et al. for a different active learning problem, not sure it is work mentionning}

In this chapter, we investigate the problem from a different perspective, and we are in particular interested in another anytime sampling rule for BAI: \gls{ttts}. 

\TTTS is inspired by the famous \gls{ts}~\citep{thompson1933} and studies BAI from a Bayesian point of view. \TS is a Bayesian algorithm well known for regret minimization, for which it is now seen as a major competitor to \UCB-typed approaches \citep{burnetas1996optimal,auer2002ucb,cappe2013klucb}. However, it is also well known that regret minimizing algorithms cannot yield optimal performance for BAI \citep{bubeck2011pure,kaufmann2017survey} and as we opt Thompson Sampling for BAI, then its adaptation is necessary. Such an adaptation, \TTTS, was given by \citet{russo2016ttts} along with the other top-two sampling rules \TTPS and \TTVS. By choosing between two different candidate arms in each round, these sampling rules enforce the exploration of sub-optimal arms, that would be under-sampled by vanilla \TS due to its objective of maximizing rewards.

While \TTTS appears to be a good anytime sampling rule for the fixed-confidence BAI when coupled with an appropriate stopping rule, so far there is no theoretical support for this employment. Indeed, the (Bayesian-flavored) asymptotic analysis of \cite{russo2016ttts} shows that under \TTTS, the posterior probability that $I^\star$ is the best arm converges almost surely to 1 at the best possible rate. However, this property does not by itself translate into sample complexity guarantees. Since the result of \cite{russo2016ttts}, \citet{qin2017ttei} proposed and analyzed \TTEI, another Bayesian sampling rule, both in the fixed-confidence setting and in terms of posterior convergence rate. Nonetheless, similar guarantees for \TTTS have been left as an open question by \cite{russo2016ttts}. In the present paper, we answer this open question. In addition, we propose \gls{t3c}, a computationally more favorable variant of \TTTS and  extend the fixed-confidence guarantees to \TCC as well.

% and , has several desirable properties. First, it relies on choosing between two different candidates at each round to enforce the exploration and thus overcome the aforementioned drawback of Thompson sampling. Second, it does not depend on the confidence level $\delta$ just as \Track. While \TTEI has a fixed-confidence guarantee, \TTTS is only analyzed from a Bayesian asymptotic perspective (in a sense that we detail later). In the mean time one would also guess that \TTTS is a natural candidate for fixed-confidence BAI, when coupled with an appropriate stopping rule. We validate that guess in this work.

%% Too much details on anytime BAI (though well written!)

% The fact that the two previous settings provide a sampling rule that depends either on a confidence parameter $\delta$ or a budget parameter $n$ is not desirable in some real applications. To address this problem, \cite{jun2016atlucb} propose to use a \emph{doubling trick} upon fixed-budget algorithms like \SR~\citep{bubeck2009pure} and \SHA~\citep{karnin2013sha}, or use a time-varying confidence parameter when dealing with the fixed-confidence setting. This allows us to stop the learning process \emph{anytime} we want, meaning that we want the probability of $i^{\star}$ not being recommended to be decreasing as fast as possible. \TTTS, on the other hand, provides an interesting alternative that evaluates sampling rules in a Bayesian perspective, which further motivates our interest on it.
 
% \paragraph{Fixed-confidence setting}
% 
% In the fixed-confidence setting, the learner tries to obtain a fixed confidence $\delta$ about the quality of the returned arm with as few numbers of samples as possible. Thus a policy for this setting contains: (1) a \emph{sampling rule} that tells the learner which arm to sample at each stage, (2) a \emph{stopping rule} that tells the learner when to stop the learning process, and (3) a \emph{recommendation rule} that outputs a recommendation at the end of the exploration phase.

% More formally, we are given a set of $K$ arms $\cA\eqdef\{1,\ldots,K\}$ that follow $K$ unknown distributions $(\nu_k)_{1 \leq k \leq K}$, a confidence level $\delta$, and a time stopping time $\tau$ w.r.t. the observations. At each time step $t$, the learning process consists of the following actions:
% \begin{itemize}
%     \item a vector of rewards $(Y_{t,1} \sim \nu_1, \ldots, Y_{t,K} \sim \nu_K)$ is generated,
% 	\item the learner picks an arm $I_t \in \cA$ (according to the sampling rule), 
% 	\item the learner potentially recommends an arm $J_t \in \cA$ (according to the recommendation rule),
% 	\item the learner observes the reward $Y_{t,I_t}$, and
% 	\item the learner stops if $\PP{J_{\tau}\neq I^{\star}} \leq \delta$, where $I^{\star}$ is the true best arm.
% \end{itemize}
% \pmd{The last point is strange since the learner do not know this proba. the learner just decides to stop or not.}
% The goal is to obtain a small expected number of samples $\EEs{\bmu}{\tau}$, where $\bmu=(\mu_1,\mu_2,\cdots,\mu_K)$ is the underlying bandit model associated to the given set of $K$ arms.

\paragraph{Contributions} %Our main contributions are the following: 
(1) We propose a new Bayesian sampling rule, \TCC, which is inspired by \TTTS but easier to implement and computationally advantageous (2) We investigate two Bayesian stopping and recommendation rules and establish their $\delta$-correctness for a bandit model with Gaussian rewards.\footnote{hereafter `Gaussian bandits' or `Gaussian model'} (3) We provide the first sample complexity analysis of \TTTS and \TCC for a Gaussian model and our proposed stopping rule. (4) \citeauthor{russo2016ttts}'s posterior convergence results for \TTTS were obtained under restrictive assumptions on the models and priors, which exclude the two mostly used in practice: Gaussian bandits with Gaussian priors and bandits with Bernoulli rewards\footnote{hereafter `Bernoulli bandits'} with Beta priors. We prove that optimal posterior convergence rates can be obtained for those two as well.

\paragraph{Outline} In Section~\ref{sec:t3c.algorithm}, we give a reminder of \TTTS and introduce \TCC along with our proposed recommendation and stopping rules. Then, in Section~\ref{sec:related}, we describe in detail two important notions of optimality that are invoked in this paper. The main fixed-confidence analysis follows in Section~\ref{sec:t3c.confidence}, and further Bayesian optimality results are given in Section~\ref{sec:bayesian}. Numerical illustrations are given in Section~\ref{sec:t3c.experiments}.
