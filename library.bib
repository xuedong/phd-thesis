Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{li2020parallel,
abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
archivePrefix = {arXiv},
arxivId = {1810.05934},
author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
booktitle = {Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys)},
eprint = {1810.05934},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - A system for massively parallel hyperparameter tuning.pdf:pdf},
title = {{A system for massively parallel hyperparameter tuning}},
url = {http://arxiv.org/abs/1810.05934},
year = {2020}
}
@inproceedings{talebi2018variance,
abstract = {The problem of reinforcement learning in an unknown and discrete Markov Decision Process (MDP) under the average-reward criterion is considered, when the learner interacts with the system in a single stream of observations, starting from an initial state without any reset. We revisit the minimax lower bound for that problem by making appear the local variance of the bias function in place of the diameter of the MDP. Furthermore, we provide a novel analysis of the KL-Ucrl algorithm establishing a high-probability regret bound scaling as O(√S $\Sigma$s,a V*s,aT ) for this algorithm for ergodic MDPs, where S denotes the number of states and where V*s,a is the variance of the bias function with respect to the next-state distribution following action a in state s. The resulting bound improves upon the best previously known regret bound O(DS√AT) for that algorithm, where A and D respectively denote the maximum number of actions (per state) and the diameter of MDP. We finally compare the leading terms of the two bounds in some benchmark MDPs indicating that the derived bound can provide an order of magnitude improvement in some cases. Our analysis leverages novel variations of the transportation lemma combined with Kullback-Leibler concentration inequalities, that we believe to be of independent interest.},
archivePrefix = {arXiv},
arxivId = {1803.01626},
author = {Talebi, Mohammad Sadegh and Maillard, Odalric Ambrym},
booktitle = {Proceedings of the 29th International Conference on Algorithmic Learning Theory (ALT)},
eprint = {1803.01626},
issn = {23318422},
keywords = {Bellman Optimality,Concentration Inequalities,Markov Decision Processes,Regret Minimization,Undiscounted Reinforcement Learning},
title = {{Variance-aware regret bounds for undiscounted reinforcement learning in MDPs}},
url = {https://arxiv.org/pdf/1803.01626.pdf},
year = {2018}
}
@inproceedings{zoph2018nas,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
booktitle = {Proceedings of the 31st IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph et al. - 2018 - Learning transferable architectures for scalable image recognition.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {8697--8710},
title = {{Learning transferable architectures for scalable image recognition}},
url = {https://arxiv.org/pdf/1707.07012.pdf},
year = {2018}
}
@inproceedings{pathak2017curiosity,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward; 2) exploration with no extrinsic reward; and 3) generalization to unseen scenarios (e.g. new levels of the same game).},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2017.70},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-driven exploration by self-supervised prediction}},
url = {https://openaccess.thecvf.com/content{\_}cvpr{\_}2017{\_}workshops/w5/papers/Pathak{\_}Curiosity-Driven{\_}Exploration{\_}by{\_}CVPR{\_}2017{\_}paper.pdf},
year = {2017}
}
@inproceedings{szita2010model,
abstract = {One might believe that model-based algorithms of reinforcement learning can propagate the obtained experience more quickly, and are able to direct exploration better. As a consequence, fewer exploratory actions should be enough to learn a good policy. Strangely enough, current theoretical results for model-based algorithms do not support this claim: In a finite Markov decision process with N states, the best bounds on the number of exploratory steps necessary are of order O(N2 log N), in contrast to the O(N log N) bound available for the model-free, delayed Q-LEARNING algorithm. In this paper we show that MORMAX, a modified version of the RMAX algorithm needs to make at most O(N log N) exploratory steps. This matches the lower bound up to logarithmic factors, as well as the upper bound of the state-of-the-art model-free algorithm, while our new bound improves the dependence on other problem parameters. Copyright 2010 by the author(s)/owner(s).},
author = {Szita, Istv{\'{a}}n and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML)},
isbn = {9781605589077},
pages = {1031--1038},
title = {{Model-based reinforcement learning with nearly tight exploration complexity bounds}},
url = {https://icml.cc/Conferences/2010/papers/546.pdf},
year = {2010}
}
@article{kurach2019football,
abstract = {Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simula-tor. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. We further propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and ApeX DQN), and we also provide a diverse set of simpler scenarios with the Football Academy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1907.11180v1},
author = {Kurach, Karol and Raichuk, Anton and Sta, Piotr and Bachem, Olivier and Espeholt, Lasse and Riquelme, Carlos and Vincent, Damien},
eprint = {arXiv:1907.11180v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurach et al. - Unknown - Google research football A novel reinforcement learning environment.pdf:pdf},
journal = {arXiv preprint arXiv:1907.11180},
title = {{Google research football : A novel reinforcement learning environment}},
url = {https://arxiv.org/pdf/1907.11180.pdf}
}
@article{russo2016ts,
abstract = {We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.},
archivePrefix = {arXiv},
arxivId = {1403.5341},
author = {Russo, Daniel and {Van Roy}, Benjamin},
eprint = {1403.5341},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russo, Van Roy - 2016 - An information-theoretic analysis of Thompson sampling(2).pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Information theory,Mutli-armed bandit,Online optimization,Regret bounds,Thompson sampling},
title = {{An information-theoretic analysis of Thompson sampling}},
url = {http://www.jmlr.org/papers/volume17/14-087/14-087.pdf},
volume = {17},
year = {2016}
}
@inproceedings{abbeel2004irl,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y.},
booktitle = {Proceedings of the 21st International Conference on Machine Learning (ICML)},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
isbn = {1581138285},
issn = {0028-0836},
pmid = {25719670},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {https://ai.stanford.edu/{~}ang/papers/icml04-apprentice.pdf},
year = {2004}
}
@book{kulesza2012,
abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
archivePrefix = {arXiv},
arxivId = {1207.6083},
author = {Kulesza, Alex and Taskar, Ben},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000044},
eprint = {1207.6083},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:pdf},
isbn = {9781601986283},
issn = {1935-8237},
pages = {123--286},
title = {{Determinantal Point Processes for Machine Learning}},
url = {http://www.alexkulesza.com/pubs/dpps{\_}fnt12.pdf},
volume = {5},
year = {2012}
}
@inproceedings{auer2016pareto,
abstract = {We consider the problem of identifying the Pareto front for multiple objectives from a finite set of operating points. Sampling an operating point gives a random vector where each coordinate corresponds to the value of one of the objectives. The Pareto front is the set of operating points that are not dominated by any other operating point in respect to all objectives (considering the mean of their objective values). We propose a confidence bound algorithm to approximate the Pareto front, and prove problem specific lower and upper bounds, showing that the sample complexity is characterized by some natural geometric properties of the operating points. Experiments confirm the reliability of our algorithm. For the problem of finding a sparse cover of the Pareto front, we propose an asymmetric covering algorithm of independent interest.},
author = {Auer, Peter and Chiang, Chao Kai and Ortner, Ronald and Drugan, Madalina M.},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auer et al. - 2016 - Pareto front identification from stochastic bandit feedback.pdf:pdf},
pages = {939--947},
title = {{Pareto front identification from stochastic bandit feedback}},
url = {http://proceedings.mlr.press/v51/auer16.pdf},
year = {2016}
}
@article{perchet2011blackwell,
abstract = {We provide a necessary and sufficient condition under which a convex set is approachable in a game with partial monitoring, i.e. where players do not observe their opponents' moves but receive random signals. This condition is an extension of Blackwell's Criterion in the full monitoring framework, where players observe at least their payoffs. When our condition is fulfilled, we construct explicitly an approachability strategy, derived from a strategy satisfying some internal consistency property in an auxiliary game. We also provide an example of a convex set, that is neither (weakly)-approachable nor (weakly)-excludable, a situation that cannot occur in the full monitoring case. We finally apply our result to describe an $\epsilon$-optimal strategy of the uninformed player in a zero-sum repeated game with incomplete information on one side. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Perchet, Vianney},
doi = {10.1007/s10957-011-9797-3},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perchet - 2011 - Approachability of convex sets in games with partial monitoring.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Blackwell approachability,Convex sets,Incomplete information,Partial monitoring,Repeated games},
number = {3},
pages = {665--677},
title = {{Approachability of convex sets in games with partial monitoring}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10957-011-9797-3.pdf},
volume = {149},
year = {2011}
}
@book{busa-fekete2013,
abstract = {In subset ranking, the goal is to learn a ranking function that approximates a gold standard partial ordering of a set of objects (in our case, a set of documents retrieved for the same query). The partial ordering is given by relevance labels representing the relevance of documents with respect to the query on an absolute scale. Our approach consists of three simple steps. First, we train standard multi-class classifiers (AdaBoost.MH and multi-class SVM) to discriminate between the relevance labels. Second, the posteriors of multi-class classifiers are calibrated using probabilistic and regression losses in order to estimate the Bayes-scoring function which optimizes the Normalized Discounted Cumulative Gain (NDCG). In the third step, instead of selecting the best multi-class hyperparameters and the best calibration, we mix all the learned models in a simple ensemble scheme. Our extensive experimental study is itself a substantial contribution. We compare most of the existing learning-to-rank techniques on all of the available large-scale benchmark data sets using a standardized implementation of the NDCG score. We show that our approach is competitive with conceptually more complex listwise and pairwise methods, and clearly outperforms them as the data size grows. As a technical contribution, we clarify some of the confusing results related to the ambiguities of the evaluation tools, and propose guidelines for future studies. {\textcopyright} 2013 The Author(s).},
author = {Busa-Fekete, R{\'{o}}bert and K{\'{e}}gl, Bal{\'{a}}zs and {\'{E}}ltető, Tam{\'{a}}s and Szarvas, Gy{\"{o}}rgy},
booktitle = {Machine Learning},
doi = {10.1007/s10994-013-5360-9},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Busa-Fekete et al. - 2013 - Tune and mix Learning to rank using ensembles of calibrated multi-class classifiers.pdf:pdf},
isbn = {1099401353609},
issn = {15730565},
keywords = {Class Probability Calibration,Ensemble methods,Learning-to-rank,Multi-class classification,Regression Based Calibration},
number = {2-3},
pages = {261--292},
title = {{Tune and mix: Learning to rank using ensembles of calibrated multi-class classifiers}},
volume = {93},
year = {2013}
}
@article{mason2020epsilon,
abstract = {The pure-exploration problem in stochastic multi-armed bandits aims to find one or more arms with the largest (or near largest) means. Examples include finding an {\{}$\backslash$epsilon{\}}-good arm, best-arm identification, top-k arm identification, and finding all arms with means above a specified threshold. However, the problem of finding all {\{}$\backslash$epsilon{\}}-good arms has been overlooked in past work, although arguably this may be the most natural objective in many applications. For example, a virologist may conduct preliminary laboratory experiments on a large candidate set of treatments and move all {\{}$\backslash$epsilon{\}}-good treatments into more expensive clinical trials. Since the ultimate clinical efficacy is uncertain, it is important to identify all {\{}$\backslash$epsilon{\}}-good candidates. Mathematically, the all-{\{}$\backslash$epsilon{\}}-good arm identification problem presents significant new challenges and surprises that do not arise in the pure-exploration objectives studied in the past. We introduce two algorithms to overcome these and demonstrate their great empirical performance on a large-scale crowd-sourced dataset of 2.2M ratings collected by the New Yorker Caption Contest as well as a dataset testing hundreds of possible cancer drugs.},
archivePrefix = {arXiv},
arxivId = {2006.08850},
author = {Mason, Blake and Jain, Lalit and Tripathy, Ardhendu and Nowak, Robert},
eprint = {2006.08850},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mason et al. - 2020 - Finding all {\{}epsilon{\}}-Good arms in stochastic bandits.pdf:pdf},
journal = {arXiv preprint arXiv:2006.08850},
title = {{Finding all {\{}$\backslash$epsilon{\}}-Good arms in stochastic bandits}},
url = {http://arxiv.org/pdf/2006.08850.pdf},
year = {2020}
}
@inproceedings{liu2017calibrated,
abstract = {Information retrieval (IR) models often employ complex variations in term weights to compute an aggregated similarity score of a query-document pair. Treating IR models as black-boxes makes it difficult to understand or explain why certain documents are retrieved at top-ranks for a given query. Local explanation models have emerged as a popular means to understand individual predictions of classification models. However, there is no systematic investigation that learns to interpret IR models, which is in fact the core contribution of our work in this paper. We explore three sampling methods to train an explanation model and propose two metrics to evaluate explanations generated for an IR model. Our experiments reveal some interesting observations, namely that a) diversity in samples is important for training local explanation models, and b) the stability of a model is inversely proportional to the number of parameters used to explain the model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.01875v1},
author = {Liu, Yang and Radanovic, Goran and Dimitrakakis, Christos and Mandal, Debmalya and Parkes, David C.},
booktitle = {4th Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML)},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {arXiv:1707.01875v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Calibrated fairness in bandits.pdf:pdf},
isbn = {9781450361729},
keywords = {Interpretability,Point-wise explanations,Ranking},
title = {{Calibrated fairness in bandits}},
year = {2017}
}
@inproceedings{klein2017robo,
abstract = {Bayesian optimization is a powerful approach for the global derivative-free opti-mization of non-convex expensive functions. Even though there is a rich literature on Bayesian optimization, the source code of advanced methods is rarely available, making it difficult for practitioners to use them and for researchers to compare to and extend them. The BSD-licensed python package ROBO, released with this paper, tackles these problems by facilitating both ease of use and extensibil-ity. Beyond the standard methods in Bayesian optimization, RoBO offers (to the best of our knowledge) the only available implementations of Bayesian optimiza-tion with Bayesian neural networks, multi-task optimization, and fast Bayesian hyperparameter optimization on large datasets (Fabolas).},
author = {Klein, Aaron and Falkner, Stefan and Mansur, Numair and Hutter, Frank},
booktitle = {7th Workshop on Bayesian Optimization at Neural Information Processing Systems (NIPS-BayesOpt)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein et al. - 2017 - RoBO A flexible and robust Bayesian optimization framework in Python.pdf:pdf},
title = {{RoBO: A flexible and robust Bayesian optimization framework in Python}},
url = {https://pdfs.semanticscholar.org/8d3d/080c425f1c8d2cb8e81962ac274df10b5565.pdf},
year = {2017}
}
@inproceedings{locatelli2018adaptivity,
author = {Locatelli, Andrea and Carpentier, Alexandra},
booktitle = {Proceedings of the 31st Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locatelli, Carpentier - 2018 - Adaptivity to smoothness in X-armed bandits(2).pdf:pdf},
pages = {1463--1492},
title = {{Adaptivity to smoothness in X-armed bandits}},
url = {http://proceedings.mlr.press/v75/locatelli18a.html},
year = {2018}
}
@inproceedings{gabillon2012ugape,
abstract = {We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.},
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabillon, Ghavamzadeh, Lazaric - 2012 - Best arm identification A unified approach to fixed budget and fixed confidence.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {3212--3220},
title = {{Best arm identification: A unified approach to fixed budget and fixed confidence}},
url = {http://papers.nips.cc/paper/4640-best-arm-identification-a-unified-approach-to-fixed-budget-and-fixed-confidence.pdf},
year = {2012}
}
@article{friedler2016impossible,
abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the "observed" space) and outputs (the "decision" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
archivePrefix = {arXiv},
arxivId = {1609.07236},
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
eprint = {1609.07236},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedler, Scheidegger, Venkatasubramanian - 2016 - On the (im)possibility of fairness.pdf:pdf},
journal = {arXiv preprint arXiv:1609.07236},
title = {{On the (im)possibility of fairness}},
url = {http://arxiv.org/pdf/1609.07236.pdf},
year = {2016}
}
@book{mankiw2014micro,
author = {Mankiw, Gregory N.},
edition = {5th},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mankiw - 2014 - Principles of Economics - Microeconomics.pdf:pdf},
isbn = {9780324589986},
publisher = {Cengage Learning},
title = {{Principles of Economics - Microeconomics}},
year = {2014}
}
@book{goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Bengio, Courville - 2016 - Deep Learning.pdf:pdf},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@inproceedings{yogatama2014transfer,
abstract = {We propose a fast and effective algorithm for automatic hyperparameter tuning that can generalize across datasets. Our method is an instance of sequential model-based optimization (SMBO) that transfers information by constructing a common response surface for all datasets, similar to Bardenet et al. (2013). The time complexity of reconstructing the response surface at every SMBO iteration in our method is linear in the number of trials (significantly less than previous work with comparable performance), allowing the method to realistically scale to many more datasets. Specifically, we use deviations from the per-dataset mean as the response values. We empirically show the superiority of our method on a large number of synthetic and real-world datasets for tuning hyperparameters of logistic regression and ensembles of classifiers.},
author = {Yogatama, Dani and Mann, Gideon},
booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yogatama, Mann - 2014 - Efficient transfer learning method for automatic hyperparameter tuning.pdf:pdf},
issn = {15337928},
pages = {1077--1085},
title = {{Efficient transfer learning method for automatic hyperparameter tuning}},
url = {https://pdfs.semanticscholar.org/75f2/6734972ebaffc6b43d45abd3048ef75f15a5.pdf},
year = {2014}
}
@inproceedings{mishra2014,
abstract = {In this paper we study the problem of pri-vate stochastic multi-arm bandits. Our notion of privacy is the same as some of the ear-lier works in the general area of private on-line learning (Dwork et al., 2010; Jain et al., 2012; Smith and Thakurta, 2013). We design algorithms that are i) differentially private, and ii) have regret guarantees that (almost) match the regret guarantees for the best non-private algorithms (e.g., upper confidence bound sam-pling). Moreover, through our experiments on simulated and real-world data sets, we empiri-cally show the effectiveness of our algorithms.},
author = {Mishra, N. and Thakurta, A.},
booktitle = {Workshop on Learning, Security, and Privacy at Internation Conference on Machine Learning},
keywords = {from theory to practice,vate stochastic multi-arm bandits},
title = {{Private stochastic multi-arm bandits: From theory to practice}},
url = {http://www.cse.chalmers.se/{~}chrdimi/workshops/lsp-2014/Thakurta-PrivateStochasticBandits.pdf},
year = {2014}
}
@article{schulman2017ppo,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
doi = {10.1007/s00038-010-0125-8},
eprint = {1707.06347},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal policy optimization algorithms.pdf:pdf},
isbn = {0008-5286 (Print)$\backslash$r0008-5286 (Linking)},
issn = {1661-8564},
journal = {arXiv preprint arXiv:1707.06347},
pmid = {20151171},
title = {{Proximal policy optimization algorithms}},
url = {http://arxiv.org/pdf/1707.06347.pdf},
year = {2017}
}
@inproceedings{valko2013stosoo,
abstract = {We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.},
author = {Valko, Michal and Carpentier, Alexandra and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valko, Carpentier, Munos - 2013 - Stochastic simultaneous optimistic optimization.pdf:pdf},
pages = {19--27},
title = {{Stochastic simultaneous optimistic optimization}},
url = {http://proceedings.mlr.press/v28/valko13.pdf},
year = {2013}
}
@inproceedings{hazan2018spectral,
abstract = {We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm — an iterative application of compressed sensing techniques for orthogonal polynomials — requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization. We also outperform Random Search 8×. Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform. We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).},
archivePrefix = {arXiv},
arxivId = {1706.00764},
author = {Hazan, Elad and Klivans, Adam and Yuan, Yang},
booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR)},
eprint = {1706.00764},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hazan, Klivans, Yuan - 2018 - Hyperparameter optimization A spectral approach(2).pdf:pdf},
title = {{Hyperparameter optimization: A spectral approach}},
url = {https://arxiv.org/pdf/1706.00764.pdf},
year = {2018}
}
@inproceedings{even-dar2002pac,
abstract = {The bandit problem is revisited and considered under the PAC model. Our main contribution in this part is to show that given n arms, it suffices to pull the arms O(n/∈2 log 1/$\delta$) times to find an ∈-optimal arm with probability of at least 1 - $\delta$. This is in contrast to the naive bound of O(n/∈2 log n/$\delta$). We derive another algorithm whose complexity depends on the specific settingo f the rewards, rather than the worst case setting. We also provide a matching lower bound. We show how given an algorithm for the PAC model Multi-armed Bandit problem, one can derive a batch learningalg orithm for Markov Decision Processes. This is done essentially by simulating Value Iteration, and in each iteration invokingt he multi-armed bandit algorithm. Using our PAC algorithm for the multi-armed bandit problem we improve the dependence on the number of actions.},
author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
booktitle = {Proceedings of the 15th Annual Conference on Learning Theory (CoLT)},
doi = {10.1007/3-540-45435-7_18},
isbn = {354043836X},
issn = {03029743},
pages = {255--270},
title = {{PAC bounds for multi-armed bandit and Markov decision processes}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.2371{\&}rep=rep1{\&}type=pdf},
volume = {2375},
year = {2002}
}
@article{zeng2016online,
abstract = {Recommender systems are designed to effectively support individuals' decision-making process on various web sites. It can be naturally represented by a user-object bipartite network, where a link indicates that a user has collected an object. Recently, research on the information backbone has attracted researchers' interests, which is a sub-network with fewer nodes and links but carrying most of the relevant information. With the backbone, a system can generate satisfactory recommenda- tions while saving much computing resource. In this paper, we propose an enhanced topology-aware method to extract the information backbone in the bipartite network mainly based on the information of neighboring users and objects. Our backbone extraction method enables the recommender systems achieve more than 90{\%} of the accuracy of the top-L recommendation, however, consuming only 20{\%} links. The experimental results show that our method outperforms the alternative backbone extraction methods. Moreover, the structure of the information backbone is studied in detail. Finally, we highlight that the information backbone is one of the most important properties of the bipartite network, with which one can significantly improve the efficiency of the recommender system.},
author = {Zeng, Wei and Fang, Meiling and Shao, Junming and Shang, Mingsheng},
doi = {10.1038/srep34292},
issn = {20452322},
journal = {Scientific Reports},
title = {{Uncovering the essential links in online commercial networks}},
url = {https://www.nature.com/articles/srep34292},
volume = {6},
year = {2016}
}
@article{pinneri2020cem,
abstract = {Trajectory optimizers for model-based reinforcement learning, such as the Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22× less samples and yielding a performance increase of 1.2-10× in high-dimensional control problems.},
archivePrefix = {arXiv},
arxivId = {2008.06389},
author = {Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Achterhold, Jan and St{\"{u}}ckler, J{\"{o}}rg and Rol{\'{i}}nek, Michal and Martius, Georg},
eprint = {2008.06389},
journal = {arXiv preprint arXiv:2008.06389},
keywords = {Cross-entropy-method,Model-based reinforcement learning,Model-predictive-control,Planning,Trajectory-optimization},
title = {{Sample-efficient cross-entropy method for real-time planning}},
url = {https://arxiv.org/pdf/2008.06389.pdf},
year = {2020}
}
@inproceedings{pedregosa2016,
abstract = {Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of ℓ2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.},
archivePrefix = {arXiv},
arxivId = {1602.02355},
author = {Pedregosa, Fabian},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
eprint = {1602.02355},
isbn = {9781510829008},
pages = {1150--1159},
title = {{Hyperparameter optimization with approximate gradient}},
url = {https://arxiv.org/pdf/1602.02355.pdf},
volume = {2},
year = {2016}
}
@article{weng2020momentum,
abstract = {Existing studies indicate that momentum ideas in conventional optimization can be used to improve the performance of Q-learning algorithms. However, the finite-sample analysis for momentum-based Q-learning algorithms is only available for the tabular case without function approximations. This paper analyzes a class of momentum-based Q-learning algorithms with finite-sample guarantee. Specifically, we propose the MomentumQ algorithm, which integrates the Nesterov's and Polyak's momentum schemes, and generalizes the existing momentum-based Q-learning algorithms. For the infinite state-action space case, we establish the convergence guarantee for MomentumQ with linear function approximations and Markovian sampling. In particular, we characterize the finite-sample convergence rate which is provably faster than the vanilla Q-learning. This is the first finite-sample analysis for momentum-based Q-learning algorithms with function approximations. For the tabular case under synchronous sampling, we also obtain a finite-sample convergence rate that is slightly better than the SpeedyQ (Azar et al., 2011) when choosing a special family of step sizes. Finally, we demonstrate through various experiments that the proposed MomentumQ outperforms other momentum-based Q-learning algorithms.},
archivePrefix = {arXiv},
arxivId = {2007.15418},
author = {Weng, Bowen and Xiong, Huaqing and Zhao, Lin and Liang, Yingbin and Zhang, Wei},
eprint = {2007.15418},
issn = {23318422},
journal = {arXiv preprint arXiv:2007.15418},
keywords = {Convergence rate,Finite-sample analysis,Linear function approximation,Momentum scheme,Q-learning,Tabular Q-learning},
title = {{Momentum Q-learning with finite-sample convergence guarantee}},
url = {https://arxiv.org/pdf/2007.15418.pdf},
year = {2020}
}
@inproceedings{blum2018discrimination,
abstract = {We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of equalized odds that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, equalized error rates, we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination.},
archivePrefix = {arXiv},
arxivId = {1810.11829},
author = {Blum, Avrim and Gunasekar, Suriya and Lykouris, Thodoris and Srebro, Nathan},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1810.11829},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum et al. - 2018 - On preserving non-discrimination when combining expert advice.pdf:pdf},
issn = {10495258},
pages = {8376--8387},
title = {{On preserving non-discrimination when combining expert advice}},
url = {https://arxiv.org/pdf/1810.11829.pdf},
year = {2018}
}
@inproceedings{soare2014linear,
abstract = {We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter {\$}\backslashtheta{\^{}}*{\$} and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the {\$}G{\$}-optimality criterion used in optimal experimental design.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.6110v2},
author = {Soare, Marta and Lazaric, Alessandro and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
eprint = {arXiv:1409.6110v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soare, Lazaric, Munos - 2014 - Best-arm identification in linear bandits.pdf:pdf},
issn = {10495258},
pages = {828--836},
title = {{Best-arm identification in linear bandits}},
url = {https://arxiv.org/pdf/1409.6110.pdf},
year = {2014}
}
@inproceedings{snoek2012spearmint,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - 2012 - Practical Bayesian optimization of machine learning algorithms(2).pdf:pdf},
isbn = {9781627480031},
pages = {2951--2959},
title = {{Practical Bayesian optimization of machine learning algorithms}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
year = {2012}
}
@inproceedings{schaul2016prioritized,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR)},
eprint = {1511.05952},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2016 - Prioritized experience replay(2).pdf:pdf},
title = {{Prioritized experience replay}},
url = {https://arxiv.org/pdf/1511.05952.pdf},
year = {2016}
}
@article{doan2018,
abstract = {Generative Adversarial Networks (GANs) can successfully learn a probability distribution and produce realistic samples. However, open questions such as sufficient convergence conditions and mode collapse still persist. In this paper, we build on existing work in the area by proposing a novel framework for training the generator against an ensemble of discriminator networks, which can be seen as a one-student/multiple-teachers setting. We formalize this problem within the non-stationary Multi-Armed Bandit (MAB) framework, where we evaluate the capability of a bandit algorithm to select discriminators for providing the generator with feedback during learning. To this end, we propose a reward function which reflects the amount of knowledge learned by the generator and dynamically selects the optimal discriminator network. Finally, we connect our algorithm to stochastic optimization methods and show that existing methods using multiple discriminators in literature can be recovered from our parametric model. Experimental results based on the Fr$\backslash$'echet Inception Distance (FID) demonstrates faster convergence than existing baselines and show that our method learns a curriculum.},
archivePrefix = {arXiv},
arxivId = {1808.00020},
author = {Doan, Thang and Monteiro, Joao and Albuquerque, Isabela and Mazoure, Bogdan and Durand, Audrey and Pineau, Joelle and Hjelm, R Devon},
eprint = {1808.00020},
journal = {arXiv preprint arXiv:1808.00020},
title = {{Online adaptative curriculum learning for GANs}},
url = {https://arxiv.org/pdf/1808.00020.pdf},
year = {2018}
}
@inproceedings{feige2019invariant,
abstract = {Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.},
archivePrefix = {arXiv},
arxivId = {1902.03251},
author = {Feige, Ilya},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1902.03251},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feige - 2019 - Invariant-equivariant representation learning for multi-class data.pdf:pdf},
title = {{Invariant-equivariant representation learning for multi-class data}},
url = {http://arxiv.org/pdf/1902.03251.pdf},
year = {2019}
}
@article{sen2018mfhoo,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.10482v1},
author = {Sen, Rajat and Kandasamy, Kirthevasan and Shakkottai, Sanjay},
eprint = {arXiv:1810.10482v1},
journal = {arXiv preprint arXiv:1810.10482},
title = {{Noisy blackbox optimization with multi-fidelity queries : A tree search approach}},
url = {https://arxiv.org/pdf/1810.10482.pdf},
year = {2018}
}
@inproceedings{jaderberg2017aux,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also learns separate policies for maximising many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87{\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
eprint = {1611.05397},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2017 - Reinforcement learning with unsupervised auxiliary tasks(2).pdf:pdf},
title = {{Reinforcement learning with unsupervised auxiliary tasks}},
url = {https://arxiv.org/pdf/1611.05397.pdf},
year = {2017}
}
@article{dziugaite2017generalization,
abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
archivePrefix = {arXiv},
arxivId = {1703.11008},
author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
eprint = {1703.11008},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dziugaite, Roy - 2017 - Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than.pdf:pdf},
journal = {arXiv preprint arXiv:1703.11008},
title = {{Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data}},
url = {http://arxiv.org/pdf/1703.11008.pdf},
year = {2017}
}
@unpublished{shang2021safe,
author = {Shang, Xuedong and Barlier, Merwan and Colin, Igor and {Dos Santos}, Ludovic},
title = {{Safe best-arm identification in linear bandits}},
year = {2021}
}
@article{raileanu2020ride,
abstract = {Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.},
archivePrefix = {arXiv},
arxivId = {2002.12292},
author = {Raileanu, Roberta and Rockt{\"{a}}schel, Tim},
eprint = {2002.12292},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raileanu, Rockt{\"{a}}schel - 2020 - RIDE Rewarding impact-driven exploration for procedurally-generated environments(2).pdf:pdf},
journal = {arXiv},
pages = {1--21},
title = {{RIDE: Rewarding impact-driven exploration for procedurally-generated environments}},
url = {https://openreview.net/pdf?id=rkg-TJBFPB},
year = {2020}
}
@article{zhang2020rl,
abstract = {Episodic reinforcement learning and contextual bandits are two widely studied sequential decision-making problems. Episodic reinforcement learning generalizes contextual bandits and is often perceived to be more difficult due to long planning horizon and unknown state-dependent transitions. The current paper shows that the long planning horizon and the unknown state-dependent transitions (at most) pose little additional difficulty on sample complexity. We consider the episodic reinforcement learning with S states, A actions, planning horizon H, total reward bounded by 1, and the agent plays for K episodes. We propose a new algorithm, Monotonic Value Propagation (MVP), which relies on a new Bernstein-type bonus. The new bonus only requires tweaking the constants to ensure optimism and thus is significantly simpler than existing bonus constructions. We show MVP enjoys an O ((√SAK + S2A ) poly log (SAHK) ) regret, approaching the $\Omega$ (√SAK ) lower bound of contextual bandits. Notably, this result 1) exponentially improves the state-of-the-art polynomial-time algorithms by Dann et al. [2019], Zanette et al. [2019] and Zhang et al. [2020] in terms of the dependency on H, and 2) exponentially improves the running time in [Wang et al. 2020] and significantly improves the dependency on S, A and K in sample complexity.},
archivePrefix = {arXiv},
arxivId = {2009.13503},
author = {Zhang, Zihan and Ji, Xiangyang and Du, Simon S.},
eprint = {2009.13503},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Ji, Du - 2020 - Is reinforcement learning more difficult than bandits A near-optimal algorithm escaping the curse of horizon.pdf:pdf},
issn = {23318422},
journal = {arXiv preprint arXiv:2009.13503},
pages = {1--28},
title = {{Is reinforcement learning more difficult than bandits? A near-optimal algorithm escaping the curse of horizon}},
url = {https://arxiv.org/pdf/2009.13503.pdf},
year = {2020}
}
@inproceedings{bregere2019contextual,
abstract = {We propose a contextual-bandit approach for demand side management by offering price incentives. More precisely, a target mean consumption is set at each round and the mean consumption is modeled as a complex function of the distribution of prices sent and of some contextual variables such as the temperature, weather, and so on. The performance of our strategies is measured in quadratic losses through a regret criterion. We offer {\$}T{\^{}}{\{}2/3{\}}{\$} upper bounds on this regret (up to poly-logarithmic terms)---and even faster rates under stronger assumptions---for strategies inspired by standard strategies for contextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real data set gathered by UK Power Networks, in which price incentives were offered, show that our strategies are effective and may indeed manage demand response by suitably picking the price levels.},
author = {Br{\'{e}}g{\`{e}}re, Margaux and Gaillard, Pierre and Goude, Yannig and Stoltz, Gilles},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Br{\'{e}}g{\`{e}}re et al. - 2019 - Target tracking for contextual bandits Application to demand side management.pdf:pdf},
title = {{Target tracking for contextual bandits: Application to demand side management}},
url = {https://hal.archives-ouvertes.fr/hal-01994144},
year = {2019}
}
@article{hadiji2019,
abstract = {In the context of stochastic continuum-armed bandits, we present an algorithm that adapts to the unknown smoothness of the objective function. We exhibit and compute a polynomial cost of adaptation to the H{\{}$\backslash$"o{\}}lder regularity for regret minimization. To do this, we first reconsider the recent lower bound of Locatelli and Carpentier [20], and define and characterize admissible rate functions. Our new algorithm matches any of these minimal rate functions. We provide a finite-time analysis and a thorough discussion about asymptotic optimality.},
archivePrefix = {arXiv},
arxivId = {1905.10221},
author = {Hadiji, H{\'{e}}di},
eprint = {1905.10221},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hadiji - 2019 - Polynomial Cost of Adaptation for X -Armed Bandits.pdf:pdf},
journal = {arXiv preprint arXiv:1905.10221},
title = {{Polynomial Cost of Adaptation for X -Armed Bandits}},
url = {http://arxiv.org/pdf/1905.10221.pdf},
year = {2019}
}
@article{lu2018convex,
author = {Lu, Haihao and Freund, Robert M. and Nesterov, Yurii},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Freund, Nesterov - 2018 - Relatively-smooth convex optimization by first-order methods , and applications.pdf:pdf},
journal = {SIAM Journal of Optimization},
number = {1},
pages = {333--354},
title = {{Relatively-smooth convex optimization by first-order methods , and applications}},
url = {https://epubs.siam.org/doi/pdf/10.1137/16M1099546},
volume = {28},
year = {2018}
}
@inproceedings{jun2016atlucb,
abstract = {We introduce anytime Explore-m, a pure exploration problem for multi-armed bandits (MAB) that requires making a prediction of the top-m arms at every time step. Anytime Explore-m is more practical than fixed budget or fixed confidence formulations of the top-m problem, since many applications involve a finite, but unpredictable, budget. However, the development and analysis of anytime algorithms present many challenges. We propose AT-LUCB (AnyTime Lower and Upper Confidence Bound), the first nontrivial algorithm that provably solves anytime Explore-m. Our analysis shows that the sample complexity of AT-LUCB is competitive to anytime variants of existing algorithms. Moreover, our empirical evaluation on AT-LUCB shows that AT-LUCB performs as well as or better than state-of-the-art baseline methods for anytime Explore-m.},
author = {Jun, Kwang-Sung and Nowak, Robert},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jun, Nowak - 2016 - Anytime exploration for multi-armed bandits using confidence information(2).pdf:pdf},
isbn = {9781510829008},
pages = {974--982},
title = {{Anytime exploration for multi-armed bandits using confidence information}},
url = {http://proceedings.mlr.press/v48/jun16.pdf},
year = {2016}
}
@inproceedings{zela2018,
abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
archivePrefix = {arXiv},
arxivId = {1807.06906},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
booktitle = {5th Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
eprint = {1807.06906},
title = {{Towards automated deep learning: Efficient joint neural architecture and hyperparameter search}},
url = {http://arxiv.org/pdf/1807.06906.pdf},
year = {2018}
}
@article{pacchiano2020linear,
abstract = {We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of T rounds is maximum, and each has an expected cost below a certain threshold $\tau$. We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove an Oe($\tau$d−√cT0 ) bound on its T-round regret, where the denominator is the difference between the constraint threshold and the cost of a known feasible action. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting. We prove a regret bound of Oe(√$\tau$−KTc0 ) for this algorithm in K-armed bandits, which is a √K improvement over the regret bound we obtain by simply casting multi-armed bandits as an instance of contextual linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.},
archivePrefix = {arXiv},
arxivId = {2006.10185},
author = {Pacchiano, Aldo and Ghavamzadeh, Mohammad and Bartlett, Peter and Jiang, Heinrich},
eprint = {2006.10185},
issn = {23318422},
journal = {arXiv preprint arXiv:2006.10185},
title = {{Stochastic Bandits with Linear Constraints}},
url = {https://arxiv.org/pdf/2006.10185.pdf},
year = {2020}
}
@inproceedings{fiechter1994efficient,
abstract = {In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework. In our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a "reset" operation that interrupts the current sequence of experiments and starts a new one (from the initial state). We do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is c-close to that of the optimal policy, with probability no less than 1-$\delta$. For this model, we describe an algorithm that produces such an (ϵ, $\delta$)-optimal policy, for any environment, in time polynomial in N, K, 1/ϵ, 1/$\delta$, 1/(1-$\beta$) and rmax, where N is the number of states of the environment, K is the maximum number of actions in a state, $\beta$ is the discount factor and rmax is the maximum reward on any transition.},
author = {Fiechter, Claude Nicolas},
booktitle = {Proceedings of the 7th Annual Conference on Learning Theory (CoLT)},
doi = {10.1145/180139.181019},
isbn = {0897916557},
pages = {88--97},
title = {{Efficient reinforcement learning}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7F5F8FCD1AA7ED07356410DDD5B384FE?doi=10.1.1.49.8652{\&}rep=rep1{\&}type=pdf},
year = {1994}
}
@article{berkenkamp2019bo,
abstract = {Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. We slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical data efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.},
archivePrefix = {arXiv},
arxivId = {1901.03357},
author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
eprint = {1901.03357},
journal = {arXiv preprint arXiv:1901.03357},
title = {{No-regret Bayesian optimization with unknown hyperparameters}},
url = {http://arxiv.org/pdf/1901.03357.pdf},
year = {2019}
}
@book{russo2018,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000070},
eprint = {1707.02038},
isbn = {9781680833683},
issn = {1935-8237},
pages = {1--96},
publisher = {Mike Casey},
title = {{A Tutorial on Thompson Sampling}},
url = {https://web.stanford.edu/{~}bvr/pubs/TS{\_}Tutorial.pdf},
volume = {11},
year = {2018}
}
@inproceedings{chen2014combinatorial,
abstract = {We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a nontrivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.},
author = {Chen, Shouyuan and Lin, Tian and King, Irwin and Lyu, Michael R. and Chen, Wei},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
issn = {10495258},
pages = {379--387},
title = {{Combinatorial pure exploration of multi-armed bandits}},
url = {https://papers.nips.cc/paper/5433-combinatorial-pure-exploration-of-multi-armed-bandits.pdf},
year = {2014}
}
@inproceedings{karnin2013sha,
abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplica-tive gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale ap-plications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target con-fidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with ex-periments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karnin, Koren, Somekh - 2013 - Almost optimal exploration in multi-armed bandits.pdf:pdf},
pages = {1238--1246},
title = {{Almost optimal exploration in multi-armed bandits}},
url = {http://proceedings.mlr.press/v28/karnin13.pdf},
year = {2013}
}
@article{still2012information,
abstract = {We provide a fresh look at the problem of exploration in reinforcement learning, drawing on ideas from information theory. First, we show that Boltzmann-style exploration, one of the main exploration methods used in reinforcement learning, is optimal from an information-theoretic point of view, in that it optimally trades expected return for the coding cost of the policy. Second, we address the problem of curiosity-driven learning. We propose that, in addition to maximizing the expected return, a learner should choose a policy that also maximizes the learner's predictive power. This makes the world both interesting and exploitable. Optimal policies then have the form of Boltzmann-style exploration with a bonus, containing a novel exploration-exploitation trade-off which emerges naturally from the proposed optimization principle. Importantly, this exploration-exploitation trade-off persists in the optimal deterministic policy, i.e., when there is no exploration due to randomness. As a result, exploration is understood as an emerging behavior that optimizes information gain, rather than being modeled as pure randomization of action choices. {\textcopyright} 2012 Springer-Verlag.},
author = {Still, Susanne and Precup, Doina},
doi = {10.1007/s12064-011-0142-z},
issn = {14317613},
journal = {Theory in Biosciences},
keywords = {Adaptive behavior,Curiosity,Exploration-exploitation trade-off,Information theory,Rate distortion theory,Reinforcement learning},
number = {3},
pages = {139--148},
pmid = {22791268},
title = {{An information-theoretic approach to curiosity-driven reinforcement learning}},
url = {https://www2.hawaii.edu/{~}sstill/StillPrecup2011.pdf},
volume = {131},
year = {2012}
}
@article{corbett-davies2018,
abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
archivePrefix = {arXiv},
arxivId = {1808.00023},
author = {Corbett-Davies, Sam and Goel, Sharad},
eprint = {1808.00023},
journal = {arXiv preprint arXiv:1808.00023},
title = {{The measure and mismeasure of fairness: A critical review of fair machine learning}},
url = {http://arxiv.org/pdf/1808.00023.pdf},
year = {2018}
}
@inproceedings{sutton2000ac,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and deter- mining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, indepen- dent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor$\backslash$textendash{\{}{\}}critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
author = {Sutton, Richard S and Llester, David Mc A and Singh, Satinder and Mansour, Yishay},
booktitle = {Advances in Neural Information Processing Systems 13 (NIPS)},
pages = {1057--1063},
title = {{Policy gradient methods for reinforcement learning with function approximation}},
url = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf},
year = {2000}
}
@inproceedings{bartlett2019simple,
abstract = {We study the problem of optimizing a function under a $\backslash$emph{\{}budgeted number of evaluations{\}}. We only assume that the function is $\backslash$emph{\{}locally{\}} smooth around one of its global optima. The difficulty of optimization is measured in terms of 1) the amount of $\backslash$emph{\{}noise{\}} {\$}b{\$} of the function evaluation and 2) the local smoothness, {\$}d{\$}, of the function. A smaller {\$}d{\$} results in smaller optimization error. We come with a new, simple, and parameter-free approach. First, for all values of {\$}b{\$} and {\$}d{\$}, this approach recovers at least the state-of-the-art regret guarantees. Second, our approach additionally obtains these results while being $\backslash$textit{\{}agnostic{\}} to the values of both {\$}b{\$} and {\$}d{\$}. This leads to the first algorithm that naturally adapts to an $\backslash$textit{\{}unknown{\}} range of noise {\$}b{\$} and leads to significant improvements in a moderate and low-noise regime. Third, our approach also obtains a remarkable improvement over the state-of-the-art $\backslash$SOO algorithm when the noise is very low which includes the case of optimization under deterministic feedback ({\$}b=0{\$}). There, under our minimal local smoothness assumption, this improvement is of exponential magnitude and holds for a class of functions that covers the vast majority of functions that practitioners optimize ({\$}d=0{\$}). We show that our algorithmic improvement is also borne out in the numerical experiments, where we empirically show faster convergence on common benchmark functions.},
archivePrefix = {arXiv},
arxivId = {1810.00997},
author = {Bartlett, Peter L. and Gabillon, Victor and Valko, Michal},
booktitle = {Proceedings of the 30th International Conference on Algorithmic Learning Theory (ALT)},
eprint = {1810.00997},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartlett, Gabillon, Valko - 2019 - A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assump.pdf:pdf},
keywords = {deterministic feedback,optimization,stochastic feedback,tree search},
title = {{A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assumption}},
url = {http://arxiv.org/pdf/1810.00997.pdf},
year = {2019}
}
@article{Author2020a,
abstract = {The evaluation of hyperparameters, neural architectures, or data augmentation policies becomes a critical model selection problem in advanced deep learning with a large hyperparameter search space. In this paper, we propose an efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the scenario of hyperparameter search evaluation. It evaluates the potential of hyperparameters by the sub-samples of observations and is theoretically proved to be optimal under the criterion of cumulative regret. We further combine SS with Bayesian Optimization and develop a novel hyperparameter optimization algorithm called BOSS. Empirical studies validate our theoretical arguments of SS and demonstrate the superior performance of BOSS on a number of applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).},
archivePrefix = {arXiv},
arxivId = {2007.05670},
author = {Author, Anonymous and Address, Affiliation},
eprint = {2007.05670},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization.pdf:pdf},
title = {{An asymptotically optimal multi-armed bandit algorithm and hyperparameter optimization}},
url = {http://arxiv.org/abs/2007.05670},
year = {2020}
}
@article{amani2020decentralized,
abstract = {The classical multi-armed bandit is a class of sequential decision making problems where selecting actions incurs costs that are sampled independently from an unknown underlying distribution. Bandit algorithms have many applications in safety critical systems, where several constraints must be respected during the run of the algorithm in spite of uncertainty about problem parameters. This paper formulates a generalized linear stochastic multi-armed bandit problem with generalized linear safety constraints that depend on an unknown parameter vector. In this setting, we propose a Safe UCB-GLM algorithm for which we provide general and problem-dependent regret bounds.},
archivePrefix = {arXiv},
arxivId = {2012.00314},
author = {Amani, Sanae and Thrampoulidis, Christos},
doi = {10.1109/ICASSP40776.2020.9054063},
eprint = {2012.00314},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amani, Thrampoulidis - 2020 - Decentralized multi-agent linear bandits with safety constraints.pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {arXiv preprint arXiv:2012.00314},
keywords = {Exploration-Exploitation,Linear Bandit,Multi-armed bandit,Safe learning},
title = {{Decentralized multi-agent linear bandits with safety constraints}},
url = {https://arxiv.org/pdf/2012.00314.pdf},
year = {2020}
}
@article{chernoff1959,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Chernoff, Herman},
doi = {10.1214/aoms/1177706205},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {755--770},
title = {{Sequential design of experiments}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoms/1177706205},
volume = {30},
year = {1959}
}
@inproceedings{franceschi2017,
abstract = {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparame-ters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two methods of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclau-rin et al. (2015) but does not require reversible dynamics. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speed up hyperparameter optimization on large datasets. We present experiments on data cleaning and on learning task interactions. We also present one large-scale experiment where the use of previous gradient-based methods would be prohibitive.},
archivePrefix = {arXiv},
arxivId = {1703.01785},
author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.01785},
isbn = {9781510855144},
pages = {1903--1913},
title = {{Forward andreverse gradient-based hyperparameter optimization}},
url = {https://arxiv.org/pdf/1703.01785.pdf},
volume = {3},
year = {2017}
}
@inproceedings{joseph2016fairness,
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
archivePrefix = {arXiv},
arxivId = {1605.07139},
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
doi = {10.1145/2090236.2090255},
eprint = {1605.07139},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph et al. - 2016 - Fairness in learning Classic and contextual bandits.pdf:pdf},
isbn = {9781450311151},
issn = {10495258},
keywords = {Esgotamento Profissional,Sa{\'{u}}de Mental,Sa{\'{u}}de do Trabalhador},
pages = {325--333},
title = {{Fairness in learning: Classic and contextual bandits}},
url = {https://papers.nips.cc/paper/6355-fairness-in-learning-classic-and-contextual-bandits.pdf},
year = {2016}
}
@inproceedings{schmidhuber1991implement,
abstract = {Wie k{\"{o}}nnen Maschinen zielgerichtet und selbstgesteuert lernen? Durch Neugier. Je gr{\"{o}}{\ss}er die M{\"{o}}glichkeit, bereits bestehende Data-Compressors zu verbessern, desto gr{\"{o}}{\ss}er die Neugier.},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats},
doi = {10.7551/mitpress/3115.003.0030},
title = {{A possibility for implementing curiosity and boredom in model-building neural controllers}},
url = {https://dl.acm.org/doi/10.5555/116517.116542},
year = {1991}
}
@article{chen2019automl,
abstract = {Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.},
archivePrefix = {arXiv},
arxivId = {1907.08908},
author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
eprint = {1907.08908},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Song, Hu - 2019 - Techniques for automated machine learning.pdf:pdf},
journal = {arXiv preprint arXiv:1907.08908},
title = {{Techniques for automated machine learning}},
url = {http://arxiv.org/pdf/1907.08908.pdf},
year = {2019}
}
@article{fernandez-delgado2014,
abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifi ers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1{\%} of the maximum accuracy overcoming 90{\%} in the 84.3{\%} of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3{\%} of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classi ers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
author = {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and Barro, Sen{\'{e}}n and Amorim, Dinani},
doi = {10.1117/1.JRS.11.015020},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern{\'{a}}ndez-Delgado et al. - 2014 - Do we need hundreds of classifiers to solve real world classification problems.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian classifiers,Classification,Decision trees,Discriminant analysis,Ensembles,Generalized linear models,Logistic and multinomial regression,Multiple adaptive regression splines,Nearest-neighbors,Neural networks,Partial least squares and principal component regr,Random forest,Rule-based classifiers,Support vector machine,UCI data base},
pages = {3133--3181},
title = {{Do we need hundreds of classifiers to solve real world classification problems?}},
url = {https://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf},
volume = {15},
year = {2014}
}
@inproceedings{lagree2016multiple-play,
author = {Lagr{\'{e}}e, Paul and Vernade, Claire and Capp{\'{e}}, Olivier},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagr{\'{e}}e, Vernade, Capp{\'{e}} - 2016 - Multiple-play bandits in the position-based model(2).pdf:pdf},
title = {{Multiple-play bandits in the position-based model}},
url = {https://papers.nips.cc/paper/6546-multiple-play-bandits-in-the-position-based-model.pdf},
year = {2016}
}
@article{lepski1997,
abstract = {The problem of optimal adaptive estimation of a function at a given point from noisy data is considered. Two procedures are proved to be asymptotically optimal for different settings. First we study the problem of bandwidth selection for nonparametric pointwise kernel estimation with a given kernel. We propose a bandwidth selection procedure and prove its optimality in the asymptotic sense. Moreover, this optimality is stated not only among kernel estimators with a variable bandwidth. The resulting estimator is asymptotically optimal among all feasible estimators. The important feature of this procedure is that it is fully adaptive and it "works" for a very wide class of functions obeying a mild regularity restriction. With it the attainable accuracy of estimation depends on the function itself and is expressed in terms of the "ideal adaptive bandwidth" corresponding to this function and a given kernel. The second procedure can be considered as a specialization of the first one under the qualitative assumption that the function to be estimated belongs to some H{\"{o}}lder class $\Sigma$($\beta$,L) with unknown parameters $\beta$,L. This assumption allows us to choose a family of kernels in an optimal way and the resulting procedure appears to be asymptotically optimal in the adaptive sense in any range of adaptation with $\beta$≤2.},
author = {Lepski, Oleg V. and Spokoiny, Vladimir G.},
doi = {10.1214/aos/1030741083},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bandwidth selection,Holder-type constraints,Pointwise adaptive estimation},
number = {6},
pages = {2512--2546},
title = {{Optimal pointwise adaptive methods in nonparametric estimation}},
url = {https://www.wias-berlin.de/people/spokoiny/publications/6{\_}Spokoiny{\_}a1{\_}97/1030741083.pdf},
volume = {25},
year = {1997}
}
@inproceedings{domhan2015,
abstract = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
booktitle = {Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domhan, Springenberg, Hutter - 2015 - Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of l(2).pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {3460--3468},
title = {{Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves}},
url = {https://ml.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation{\_}of{\_}Learning{\_}Curves.pdf},
year = {2015}
}
@article{chan2018cbt,
abstract = {The infinite arms bandit problem was initiated by Berry et al. (1997). They derived a regret lower bound of all solutions for Bernoulli rewards, and proposed various bandit strategies based on success runs, but which do not achieve this bound. We propose here a confidence bound target (CBT) algorithm that achieves extensions of their regret lower bound for general reward distributions and distribution priors. The algorithm does not require information on the reward distributions, for each arm we require only the mean and standard deviation of its rewards to compute a confidence bound. We play the arm with the smallest confidence bound provided it is smaller than a target mean. If the confidence bounds are all larger, then we play a new arm. We show how the target mean can be computed from the prior so that the smallest asymptotic regret, among all infinite arms bandit algorithms, is achieved. We also show that in the absence of information on the prior, the target mean can be determined empirically, and that the regret achieved is comparable to the smallest regret. Numerical studies show that CBT is versatile and outperforms its competitors.},
archivePrefix = {arXiv},
arxivId = {1805.11793},
author = {Chan, Hock Peng and Hu, Shouri},
eprint = {1805.11793},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hu - 2018 - Infinite arms bandit Optimality via confidence bounds.pdf:pdf},
journal = {arXiv preprint arXiv:1805.11793},
title = {{Infinite arms bandit: Optimality via confidence bounds}},
url = {http://arxiv.org/pdf/1805.11793.pdf},
year = {2018}
}
@book{jurafsky2014,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jurafsky, Daniel and Martin, James H.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
publisher = {London Pearson},
title = {{Speech and Language Processing}},
url = {http://www.cs.colorado.edu/{~}martin/SLP/Updates/1.pdf},
year = {2014}
}
@inproceedings{jin2020rf,
abstract = {Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new “reward-free RL” framework. In the exploration phase, the agent first collects trajectories from an MDP M without a pre-specified reward function. After exploration, it is tasked with computing near-optimal policies under for M for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior. We give an efficient algorithm that conducts {\~{O}}(S2Apoly(H)/ǫ2) episodes of exploration and returns ǫ-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that visit each “significant” state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. We also give a nearly-matching $\Omega$(S2AH2/ǫ2) lower bound, demonstrating the near-optimality of our algorithm in this setting.},
archivePrefix = {arXiv},
arxivId = {2002.02794},
author = {Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
eprint = {2002.02794},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2020 - Reward-free exploration for reinforcement learning(2).pdf:pdf},
title = {{Reward-free exploration for reinforcement learning}},
url = {https://arxiv.org/pdf/2002.02794.pdf},
year = {2020}
}
@article{Author2021a,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2021 - Asymptotically optimal algorithm for best arm identification based on subgradients.pdf:pdf},
number = {Icml},
title = {{Asymptotically optimal algorithm for best arm identification based on subgradients}},
year = {2021}
}
@inproceedings{kegl2018ramp,
abstract = {The RAMP (Rapid Analytics and Model Prototyping) is a software and project management tool developed by the Paris-Saclay Center for Data Science. The original goal was to accelerate the adoption of high-quality data science solu-tions for domain science problems by running rapid collaborative prototyping sessions. Today it is a full-blown data science project management tool promoting reproducibility, fair and transparent model evaluation, and democratization of data science. We have used the framework for setting up and solving about twenty scientific problems, for organizing scientific sub-communities around these events, and for training novice data scientists.},
author = {K{\'{e}}gl, Bal{\'{a}}zs and Boucaud, Alexandre and Cherti, Mehdi and Kazak{\c{c}}ı, Akın and Gramfort, Alexandre and Lemaitre, Guillaume and {Van Den Bossche}, Joris and Benbouzid, Djalel and Marini, Camille},
booktitle = {2nd Workshop on Reproducibility in Machine Learning at International Conference on Machine Learning (ICML-RML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\'{e}}gl et al. - Unknown - The RAMP framework From reproducibility to transparency in the design and optimization of scientific workflows.pdf:pdf},
title = {{The RAMP framework: From reproducibility to transparency in the design and optimization of scientific workflows}},
url = {https://openreview.net/forum?id=Syg4NHz4eQ}
}
@inproceedings{maher2019smartml,
abstract = {Due to the increasing success of machine learning techniques, nowadays, thay have been widely utilized in almost every domain such as financial applications, marketing, recommender systems and user behavior analytics, just to name a few. In practice, the machine learning model creation process is a highly iterative exploratory process. In particular, an effective machine learning modeling process requires solid knowledge and understanding of the different types of machine learning algorithms. In addition, all machine learning algorithms require user-defined inputs to achieve a balance between accuracy and generalizability. This task is referred to as Hyperparameter Tuning. Thus, in practice, data scientists work hard to find the best model or algorithm that meets the specifications of their problem. Such iterative and explorative nature of the modeling process is commonly tedious and time-consuming. We demonstrate SmartML, a meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms. Being meta learning-based, the framework is able to simulate the role of the machine learning expert. In particular, the framework is equipped with a continuously updated knowledge base that stores information about the meta-features of all processed datasets along with the associated performance of the different classifiers and their tuned parameters. Thus, for any new dataset, SmartML automatically extracts its meta features and searches its knowledge base for the best performing algorithm to start its optimization process. In addition, SmartML makes use of the new runs to continuously enrich its knowledge base to improve its performance and robustness for future runs. We will show how our approach outperforms the-state-of-the-art techniques in the domain of automated machine learning frameworks.},
author = {Maher, Mohamed and Sakr, Sherif},
booktitle = {Proceedings of the 22nd International Conference on Extending Database Technology (EDBT)},
doi = {10.5441/002/edbt.2019.54},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maher, Sakr - 2019 - SmartML A meta learning-based framework for automated selection and hyperparameter tuning for machine learning algo.pdf:pdf},
isbn = {9783893180813},
issn = {23672005},
pages = {554--557},
title = {{SmartML: A meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms}},
url = {https://openproceedings.org/2019/conf/edbt/EDBT19{\_}paper{\_}235.pdf},
year = {2019}
}
@book{puterman1994,
author = {Puterman, Martin L.},
doi = {10.2307/2983520},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Puterman - 1994 - Markov Decision Processes - Discrete Stochastic Dynamic Programming.pdf:pdf},
isbn = {3175723993},
issn = {09641998},
pages = {666},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Markov Decision Processes - Discrete Stochastic Dynamic Programming}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
year = {1994}
}
@inproceedings{calders2010,
abstract = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data. {\textcopyright} The Author(s) 2010.},
author = {Calders, Toon and Verwer, Sicco},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
doi = {10.1007/s10618-010-0190-x},
issn = {13845810},
keywords = {Discrimination-aware classification,Expectation maximization,Naive bayes},
pages = {277--292},
title = {{Three naive Bayes approaches for discrimination-free classification}},
url = {https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf},
year = {2010}
}
@article{Authorb,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - Optimal designs of Gaussian process with budgets for hyperparameter optimization.pdf:pdf},
isbn = {0782128742},
title = {{Optimal designs of Gaussian process with budgets for hyperparameter optimization}},
year = {2020}
}
@article{daxberger2019mivabo,
abstract = {The optimization of expensive to evaluate, black-box, mixed-variable functions, i.e. functions that have continuous and discrete inputs, is a difficult and yet pervasive problem in science and engineering. In Bayesian optimization (BO), special cases of this problem that consider fully continuous or fully discrete domains have been widely studied. However, few methods exist for mixed-variable domains. In this paper, we introduce MiVaBo, a novel BO algorithm for the efficient optimization of mixed-variable functions that combines a linear surrogate model based on expressive feature representations with Thompson sampling. We propose two methods to optimize its acquisition function, a challenging problem for mixed-variable domains, and we show that MiVaBo can handle complex constraints over the discrete part of the domain that other methods cannot take into account. Moreover, we provide the first convergence analysis of a mixed-variable BO algorithm. Finally, we show that MiVaBo is significantly more sample efficient than state-of-the-art mixed-variable BO algorithms on hyperparameter tuning tasks.},
archivePrefix = {arXiv},
arxivId = {1907.01329},
author = {Daxberger, Erik and Makarova, Anastasia and Turchetta, Matteo and Krause, Andreas},
eprint = {1907.01329},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daxberger et al. - 2019 - Mixed-variable Bayesian optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1907.01329},
title = {{Mixed-variable Bayesian optimization}},
url = {http://arxiv.org/pdf/1907.01329.pdf},
year = {2019}
}
@article{li2019asha,
abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks---PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on two independent experimental runs.},
archivePrefix = {arXiv},
arxivId = {1902.07638},
author = {Li, Liam and Talwalkar, Ameet},
eprint = {1902.07638},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Talwalkar - 2019 - Random search and reproducibility for neural architecture search.pdf:pdf},
journal = {arXiv preprint arXiv:1902.07638},
title = {{Random search and reproducibility for neural architecture search}},
url = {http://arxiv.org/pdf/1902.07638.pdf},
year = {2019}
}
@inproceedings{dumitrascu2018pgts,
abstract = {We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with P{\'{o}}lya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, P{\'{o}}lya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of P{\'{o}}lya-Gamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.},
archivePrefix = {arXiv},
arxivId = {1805.07458},
author = {Dumitrascu, Bianca and Feng, Karen and Engelhardt, Barbara E.},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1805.07458},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumitrascu, Feng, Engelhardt - 2018 - PG-TS Improved Thompson sampling for logistic contextual bandits(2).pdf:pdf},
issn = {10495258},
pages = {4624--4633},
title = {{PG-TS: Improved Thompson sampling for logistic contextual bandits}},
url = {https://papers.nips.cc/paper/7713-pg-ts-improved-thompson-sampling-for-logistic-contextual-bandits.pdf},
year = {2018}
}
@inproceedings{li2017mmdgan,
abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
archivePrefix = {arXiv},
arxivId = {1705.08584},
author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P{\'{o}}czos, Barnab{\'{a}}s},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
doi = {S1074-5521(08)00451-1 [pii]\n10.1016/j.chembiol.2008.11.005},
eprint = {1705.08584},
isbn = {1879-1301 (Electronic)$\backslash$n1074-5521 (Linking)},
issn = {10495258},
pages = {2203--2213},
pmid = {19171307},
title = {{MMD GAN: Towards deeper understanding of moment matching network}},
url = {http://arxiv.org/pdf/1705.08584.pdf},
year = {2017}
}
@article{komiyama2015,
abstract = {We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight asymptotic regret lower bound that is based on the information divergence. An algorithm that is inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the first one with a regret upper bound that matches the lower bound. Experimental comparisons of dueling bandit algorithms show that the proposed algorithm significantly outperforms existing ones.},
archivePrefix = {arXiv},
arxivId = {1506.02550},
author = {Komiyama, Junpei and Honda, Junya and Kashima, Hisashi and Nakagawa, Hiroshi},
eprint = {1506.02550},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Komiyama et al. - 2015 - Regret lower bound and optimal algorithm in dueling bandit problem(2).pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Dueling Bandit Problem,Multi-Armed Bandit Problem,Online Learning},
number = {2015},
pages = {1--14},
title = {{Regret lower bound and optimal algorithm in dueling bandit problem}},
url = {http://proceedings.mlr.press/v40/Komiyama15.pdf},
volume = {40},
year = {2015}
}
@inproceedings{kaufmann2012thompson,
abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
archivePrefix = {arXiv},
arxivId = {1205.4217},
author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT)},
eprint = {1205.4217},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann, Korda, Munos - 2012 - Thompson sampling An asymptotically optimal finite-time analysis.pdf:pdf},
title = {{Thompson sampling: An asymptotically optimal finite-time analysis}},
url = {https://arxiv.org/pdf/1205.4217.pdf},
year = {2012}
}
@inproceedings{damianou2013dgp,
abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
archivePrefix = {arXiv},
arxivId = {1211.0358},
author = {Damianou, Andreas C. and Lawrence, Neil D.},
booktitle = {Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AIStats)},
doi = {10.1002/nme.1296},
eprint = {1211.0358},
isbn = {026218253X},
issn = {15337928},
pages = {207--215},
pmid = {20598705},
title = {{Deep Gaussian processes}},
url = {http://proceedings.mlr.press/v31/damianou13a.pdf},
year = {2013}
}
@article{silver2016alphago,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{wang2018,
abstract = {Deep learning has achieved impressive results on many problems. However, it requires high degree of expertise or a lot of experience to tune well the hyperparameters, and such manual tuning process is likely to be biased. Moreover, it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios, because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network, which usually takes quite long time. Hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning. However, Hyperband algorithm does not utilize history information of previous explored hyperparameter configurations, thus the solution found is suboptimal. We propose to combine Hyperband algorithm with Bayesian optimization (which does not ignore history when sampling next trial configuration). Experimental results show that our combination approach is superior to other hyperparameter optimization approaches including Hyperband algorithm.},
archivePrefix = {arXiv},
arxivId = {1801.01596},
author = {Wang, Jiazhuo and Xu, Jason and Wang, Xuejun},
eprint = {1801.01596},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Xu, Wang - 2018 - Combination of Hyperband and Bayesian optimization for hyperparameter optimization in deep learning(2).pdf:pdf},
journal = {arXiv preprint arXiv:1801.01596},
title = {{Combination of Hyperband and Bayesian optimization for hyperparameter optimization in deep learning}},
url = {http://arxiv.org/pdf/1801.01596.pdf},
year = {2018}
}
@article{lattimore2017optimism,
abstract = {Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. While prior work has mostly been in the worst-case setting, we analyse the asymptotic instance-dependent regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation. For example, for generalised linear bandits and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.04491v1},
author = {Lattimore, Tor and Szepesv{\'{a}}ri, Csaba},
eprint = {arXiv:1610.04491v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lattimore, Szepesv{\'{a}}ri - 2017 - The end of optimism An asymptotic analysis of finite-armed linear bandits.pdf:pdf},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
title = {{The end of optimism? An asymptotic analysis of finite-armed linear bandits}},
url = {https://arxiv.org/pdf/1610.04491.pdf},
year = {2017}
}
@inproceedings{jang2019dbscan,
abstract = {DBSCAN is a classical density-based clustering procedure with tremendous practical relevance. However, DBSCAN implicitly needs to compute the empirical density for each sample point, leading to a quadratic worst-case time complexity, which is too slow on large datasets. We propose DBSCAN++, a simple modification of DBSCAN which only requires computing the densities for a chosen subset of points. We show empirically that, compared to traditional DBSCAN, DBSCAN++ can provide not only competitive performance but also added robustness in the bandwidth hyperparameter while taking a fraction of the runtime. We also present statistical consistency guarantees showing the trade-off between computational cost and estimation rates. Surprisingly, up to a certain point, we can enjoy the same estimation rates while lowering computational cost, showing that DBSCAN++ is a sub-quadratic algorithm that attains minimax optimal rates for level-set estimation, a quality that may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {1810.13105},
author = {Jang, Jennifer and Jiang, Heinrich},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1810.13105},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang, Jiang - 2019 - DBSCAN Towards fast and scalable density clustering.pdf:pdf},
title = {{DBSCAN++: Towards fast and scalable density clustering}},
url = {http://arxiv.org/pdf/1810.13105.pdf},
year = {2019}
}
@inproceedings{esfandiari2019pandora,
abstract = {We consider online variations of the Pandora's box problem (Weitzman. 1979), a standard model for understanding issues related to the cost of acquiring information for decision-making. Our problem generalizes both the classic Pandora's box problem and the prophet inequality framework. Boxes are presented online, each with a random value and cost drew jointly from some known distribution. Pandora chooses online whether to open each box given its cost, and then chooses irrevocably whether to keep the revealed prize or pass on it. We aim for approximation algorithms against adversaries that can choose the largest prize over any opened box, and use optimal offline policies to decide which boxes to open (without knowledge of the value inside). We consider variations where Pandora can collect multiple prizes subject to feasibility constraints, such as cardinality, matroid, or knapsack constraints. We also consider variations related to classic multi-armed bandit problems from reinforcement learning. Our results use a reduction-based framework where we separate the issues of the cost of acquiring information from the online decision process of which prizes to keep. Our work shows that in many scenarios, Pandora can achieve a good approximation to the best possible performance.},
archivePrefix = {arXiv},
arxivId = {1901.10698},
author = {Esfandiari, Hossein and Hajiaghayi, MohammadTaghi and Lucier, Brendan and Mitzenmacher, Michael},
booktitle = {Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {1901.10698},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Esfandiari et al. - 2019 - Online Pandora's Boxes and Bandits.pdf:pdf},
title = {{Online Pandora's boxes and bandits}},
url = {http://arxiv.org/pdf/1901.10698.pdf},
year = {2019}
}
@article{mockus1978,
abstract = {The purpose of this paper is to describe how the Bayesian approach can be applied ot the global optimization of multiextremal functions. The function to be minimized is considered as a realizatin of some stochastic function. The optimization technique based upon the minimization of the expected deviation from the extremum is called Bayesian. The implementation of Bayesian methods is considered. The results of the application to the minimization of some standard test functions are given.},
author = {Mockus, Jonas and Tie{\v{s}}is, Vytautas and {\v{Z}}ilinskas, Antanas},
doi = {10.1007/978-94-009-0909-0_8},
isbn = {9781479932795},
journal = {Towards Global Optimisation 2},
pages = {117--129},
title = {{The application of Bayesian methods for seeking the extremum}},
url = {https://www.researchgate.net/publication/248818761{\_}The{\_}application{\_}of{\_}Bayesian{\_}methods{\_}for{\_}seeking{\_}the{\_}extremum},
year = {1978}
}
@misc{rlberry2021,
author = {Domingues, Omar Darwiche and Flet-Berliac, Yannis and Leurent, Edouard and M{\'{e}}nard, Pierre and Shang, Xuedong and Valko, Michal},
publisher = {GitHub Repository},
title = {{rlberry - A reinforcement learning library for research and education}},
url = {https://github.com/rlberry-py/rlberry},
year = {2021}
}
@article{zaki2020linear,
abstract = {We study the problem of best arm identification in linearly parameterised multi-armed bandits. Given a set of feature vectors X ⊂ R d , a confidence parameter $\delta$ and an unknown vector $\theta$ * , the goal is to identify argmax x∈X x T $\theta$ * , with probability at least 1 − $\delta$, using noisy measurements of the form x T $\theta$ *. For this fixed confidence ($\delta$-PAC) setting, we propose an explicitly implementable and provably order-optimal sample-complexity algorithm to solve this problem. Previous approaches rely on access to minimax optimization oracles. The algorithm, which we call the Phased Elimination Linear Exploration Game (PELEG), maintains a high-probability confidence ellipsoid containing $\theta$ * in each round and uses it to eliminate suboptimal arms in phases. PELEG achieves fast shrinkage of this confidence ellipsoid along the most confusing (i.e., close to, but not optimal) directions by interpreting the problem as a two player zero-sum game, and sequentially converging to its saddle point using low-regret learners to compute players' strategies in each round. We analyze the sample complexity of PELEG and show that it matches, up to order, an instance-dependent lower bound on sample complexity in the linear bandit setting. We also provide numerical results for the proposed algorithm consistent with its theoretical guarantees.},
archivePrefix = {arXiv},
arxivId = {2006.07562v1},
author = {Zaki, Mohammadi and Mohan, Avi and Gopalan, Aditya},
eprint = {2006.07562v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Mohan, Gopalan - 2020 - Explicit best arm identification in linear bandits using no-regret learners.pdf:pdf},
journal = {arXiv preprint arXiv:2006.07562},
title = {{Explicit best arm identification in linear bandits using no-regret learners}},
url = {https://arxiv.org/pdf/2006.07562.pdf},
year = {2020}
}
@article{lakhmiri2019hypernomad,
abstract = {The performance of deep neural networks is highly sensitive to the choice of the hyperparameters that define the structure of the network and the learning process. When facing a new application, tuning a deep neural network is a tedious and time consuming process that is often described as a "dark art". This explains the necessity of automating the calibration of these hyperparameters. Derivative-free optimization is a field that develops methods designed to optimize time consuming functions without relying on derivatives. This work introduces the HyperNOMAD package, an extension of the NOMAD software that applies the MADS algorithm [7] to simultaneously tune the hyperparameters responsible for both the architecture and the learning process of a deep neural network (DNN), and that allows for an important flexibility in the exploration of the search space by taking advantage of categorical variables. This new approach is tested on the MNIST and CIFAR-10 data sets and achieves results comparable to the current state of the art.},
archivePrefix = {arXiv},
arxivId = {1907.01698},
author = {Lakhmiri, Dounia and Digabel, S{\'{e}}bastien Le and Tribes, Christophe},
eprint = {1907.01698},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakhmiri, Digabel, Tribes - 2019 - HyperNOMAD Hyperparameter optimization of deep neural networks using mesh adaptive direct search.pdf:pdf},
journal = {arXiv preprint arXiv:1907.01698},
keywords = {90c56,ams subject classifications,blackbox optimization,categorical variables,deep neural networks,derivative-free optimization,hyperparameter optimiza-,mesh adaptive direct search,neural architecture search,tion},
title = {{HyperNOMAD: Hyperparameter optimization of deep neural networks using mesh adaptive direct search}},
url = {http://arxiv.org/pdf/1907.01698.pdf},
year = {2019}
}
@article{Author2019,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/Documents/xuedong/phd/work/reviews/NeurIPS 2019/Submission 3579/3579$\backslash$$\backslash$Submission 3579.pdf:pdf},
title = {{Efficient pure exploration in adaptive round model}},
year = {2019}
}
@article{ha2018world,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, Jurgen},
doi = {10.1016/b978-0-12-295180-0.50030-6},
eprint = {1803.10122},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Schmidhuber - 2018 - World models.pdf:pdf},
journal = {arXiv preprint arXiv:1803.10122},
title = {{World models}},
url = {https://arxiv.org/pdf/1803.10122.pdf},
year = {2018}
}
@article{Author2019a,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2019 - Bandit-based Monte-Carlo Optimization.pdf:pdf},
title = {{Bandit-based Monte-Carlo optimization}},
year = {2019}
}
@inproceedings{kleindessner2019kcenter,
abstract = {In data summarization we want to choose k prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose k{\_}i prototypes belonging to group i. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as k-center. A natural extension then is to incorporate the fairness constraint into the clustering objective. Existing algorithms for doing so run in time super-quadratic in the size of the data set. This is in contrast to the standard k-center objective that can be approximately optimized in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the k-center problem under the fairness constraint with running time linear in the size of the data set and k. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead. We demonstrate the applicability of our algorithm on both synthetic and real data sets.},
archivePrefix = {arXiv},
arxivId = {1901.08628},
author = {Kleindessner, Matth{\"{a}}us and Awasthi, Pranjal and Morgenstern, Jamie},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1901.08628},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleindessner, Awasthi, Morgenstern - 2019 - Fair k-center clustering for data summarization.pdf:pdf},
title = {{Fair k-center clustering for data summarization}},
url = {http://arxiv.org/pdf/1901.08628.pdf},
year = {2019}
}
@article{millidge2020deep,
abstract = {Active Inference is a theory of action arising from neuroscience which casts action and planning as a bayesian inference problem to be solved by minimizing a single quantity – the variational free energy. Active Inference promises a unifying account of action and perception coupled with a biologically plausible process theory. Despite these potential advantages, current implementations of Active Inference can only handle small, discrete policy and state-spaces and typically require the environmental dynamics to be known. In this paper we propose a novel deep Active Inference algorithm which approximates key densities using deep neural networks as flexible function approximators, which enables Active Inference to scale to significantly larger and more complex tasks. We demonstrate our approach on a suite of OpenAIGym benchmark tasks and obtain performance comparable with common reinforcement learning baselines. Moreover, our algorithm shows similarities with maximum entropy reinforcement learning and the policy gradients algorithm, which reveals interesting connections between the Active Inference framework and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1907.03876},
author = {Millidge, Beren},
eprint = {1907.03876},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Millidge - 2020 - Deep active inference as variational policy gradients(2).pdf:pdf},
journal = {Journal of Mathematical Psychology},
keywords = {Active Inference,Actor-Critic,Neural Networks,OpenAI Gym,Policy Gradients,Predictive Processing,Reinforcement Learning},
title = {{Deep active inference as variational policy gradients}},
url = {https://arxiv.org/pdf/1907.03876.pdf},
volume = {96},
year = {2020}
}
@article{Bach2017,
abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
archivePrefix = {arXiv},
arxivId = {1412.8690},
author = {Bach, Francis},
eprint = {1412.8690},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach - 2017 - Breaking the curse of dimensionality with convex neural networks.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Convex optimization,Convex relaxation,Neural networks,Non-parametric estimation},
pages = {1--53},
title = {{Breaking the curse of dimensionality with convex neural networks}},
url = {http://jmlr.org/papers/volume18/14-546/14-546.pdf},
volume = {18},
year = {2017}
}
@article{domingues2020nonstationary,
abstract = {In this work, we propose KeRNS: an algorithm for episodic reinforcement learning in non-stationary Markov Decision Processes (MDPs) whose state-action set is endowed with a metric. Using a non-parametric model of the MDP built with time-dependent kernels, we prove a regret bound that scales with the covering dimension of the state-action space and the total variation of the MDP with time, which quantifies its level of non-stationarity. Our method generalizes previous approaches based on sliding windows and exponential discounting used to handle changing environments. We further propose a practical implementation of KeRNS, we analyze its regret and validate it experimentally.},
archivePrefix = {arXiv},
arxivId = {2007.05078},
author = {Domingues, Omar Darwiche and M{\'{e}}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
eprint = {2007.05078},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingues et al. - 2020 - A kernel-based approach to non-stationary reinforcement learning in metric spaces.pdf:pdf},
journal = {arXiv preprint arXiv:2007.05078},
title = {{A kernel-based approach to non-stationary reinforcement learning in metric spaces}},
url = {https://arxiv.org/pdf/2007.05078.pdf},
year = {2020}
}
@article{lecun1998gradient,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
volume = {86},
year = {1998}
}
@inproceedings{langford2008epoch,
author = {Langford, John and Zhang, Tong},
booktitle = {Advances in Neural Information Processing Systems 21 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langford, Zhang - 2008 - The Epoch-Greedy algorithm for multi-armed bandits with side information(2).pdf:pdf},
pages = {817--824},
title = {{The epoch-greedy algorithm for multi-armed bandits with side information}},
url = {http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf},
year = {2008}
}
@article{menard2021ucbmq,
author = {M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
journal = {arXiv preprint arXiv:2103.01312},
title = {{UCB momentum Q-learning: Correcting the bias without forgetting}},
url = {https://arxiv.org/pdf/2103.01312.pdf},
year = {2021}
}
@article{peherstorfer2018multifidelity,
abstract = {In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs and varying fidelities. Typically, a computationally expensive high-fidelity model describes the system with the accuracy required by the current application at hand, while lower-fidelity models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop applications, such as optimization, inference, and uncertainty quantification, require multiple model evaluations at many different inputs, which often leads to computational demands that exceed available resources if only the high-fidelity model is used. This work surveys multifidelity methods that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a simplified physics approximation, a reduced model, a data-fit surrogate, etc.) that approximates the same output quantity as the high-fidelity model. The overall premise of these multifidelity methods is that low-fidelity models are leveraged for speedup while the high-fidelity model is kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity methods according to three classes of strategies: adaptation, fusion, and filtering. The paper reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference, and optimization.},
archivePrefix = {arXiv},
arxivId = {1806.10761},
author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
doi = {10.1137/16M1082469},
eprint = {1806.10761},
issn = {0036-1445},
journal = {SIAM Review},
number = {3},
pages = {550--591},
title = {{Survey of multifidelity methods in uncertainty propagation, inference, and optimization}},
url = {http://arxiv.org/pdf/1806.10761.pdf},
volume = {60},
year = {2018}
}
@inproceedings{feurer2015fms,
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
doi = {10.1007/978-3-030-05318-5_6},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feurer et al. - 2015 - Efficient and robust automated machine learning.pdf:pdf},
issn = {10495258},
pages = {2962--2970},
title = {{Efficient and robust automated machine learning}},
url = {https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
year = {2015}
}
@article{graves2013lstm,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {1308.0850},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint arXiv:1308.0850},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/pdf/1308.0850.pdf},
year = {2013}
}
@article{domingues2020regret,
abstract = {We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning problems whose state-action space is endowed with a metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of the MDP and a non-parametric kernel estimator of the rewards and transitions to efficiently balance exploration and exploitation. Unlike existing approaches with regret guarantees, it does not use any kind of partitioning of the state-action space. For problems with K episodes and horizon H, we provide a regret bound of O ( H3Kmax(1 2 , 2d2 +1 d )) , where d is the covering dimension of the joint state-action space. We empirically validate Kernel-UCBVI on discrete and continuous MDPs.},
archivePrefix = {arXiv},
arxivId = {2004.05599},
author = {Domingues, Omar D. and M{\'{e}}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
eprint = {2004.05599},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingues et al. - 2020 - Regret bounds for kernel-based reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:2004.05599},
title = {{Regret bounds for kernel-based reinforcement learning}},
url = {https://arxiv.org/pdf/2004.05599.pdf},
year = {2020}
}
@inproceedings{zemel2013fairness,
author = {Zemel, Richard and Wu, Yu and Swersky, Kevin and Pitassi, Toniann and Dwork, Cynthia},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zemel et al. - 2013 - Learning fair representations.pdf:pdf},
pages = {325--333},
title = {{Learning fair representations}},
url = {https://www.cs.toronto.edu/{~}toni/Papers/icml-final.pdf},
year = {2013}
}
@article{bull2015adaptive,
abstract = {We describe a novel algorithm for noisy global optimisation and continuum-armed bandits, with good convergence properties over any continuous reward function having finitely many polynomial maxima. Over such functions, our algorithm achieves square-root regret in bandits, and inverse-square-root error in optimisation, without prior information. Our algorithm works by reducing these problems to tree-armed bandits, and we also provide new results in this setting. We show it is possible to adaptively combine multiple trees so as to minimise the regret, and also give near-matching lower bounds on the regret in terms of the zooming dimension.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.2489v4},
author = {Bull, Adam D.},
doi = {10.3150/14-BEJ644},
eprint = {arXiv:1302.2489v4},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {bandits on taxonomies,continuum-armed bandits,noisy global optimisation,tree-armed bandits,zooming dimension},
number = {4},
pages = {2289--2307},
title = {{Adaptive-treed bandits}},
url = {https://arxiv.org/pdf/1302.2489.pdf},
volume = {21},
year = {2015}
}
@inproceedings{metevier2019offline,
abstract = {We present RobinHood, an offline contextual bandit algorithm designed to satisfy a broad family of fairness constraints. Our algorithm accepts multiple fairness definitions and allows users to construct their own unique fairness definitions for the problem at hand. We provide a theoretical analysis of RobinHood, which includes a proof that it will not return an unfair solution with probability greater than a user-specified threshold. We validate our algorithm on three applications: a tutoring system in which we conduct a user study and consider multiple unique fairness definitions; a loan approval setting (using the Statlog German credit data set) in which well-known fairness definitions are applied; and criminal recidivism (using data released by ProPublica). In each setting, our algorithm is able to produce fair policies that achieve performance competitive with other offline and online contextual bandit algorithms.},
author = {Metevier, Blossom and Giguere, Stephen and Brockman, Sarah and Kobren, Ari and Brun, Yuriy and Brunskill, Emma and Thomas, Philip S},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Metevier et al. - 2019 - Offline contextual bandits with high probability fairness guarantees.pdf:pdf},
title = {{Offline contextual bandits with high probability fairness guarantees}},
url = {https://people.cs.umass.edu/{~}brun/pubs/pubs/Metevier19neurips.pdf},
year = {2019}
}
@article{kazerouni2019glb,
abstract = {Motivated by drug design, we consider the best-arm identification problem in generalized linear bandits. More specifically, we assume each arm has a vector of covariates, there is an unknown vector of parameters that is common across the arms, and a generalized linear model captures the dependence of rewards on the covariate and parameter vectors. The problem is to minimize the number of arm pulls required to identify an arm that is sufficiently close to optimal with a sufficiently high probability. Building on recent progress in best-arm identification for linear bandits (Xu et al. 2018), we propose the first algorithm for best-arm identification for generalized linear bandits, provide theoretical guarantees on its accuracy and sampling efficiency, and evaluate its performance in various scenarios via simulation.},
archivePrefix = {arXiv},
arxivId = {1905.08224},
author = {Kazerouni, Abbas and Wein, Lawrence M.},
eprint = {1905.08224},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kazerouni, Wein - 2019 - Best arm identification in generalized linear bandits.pdf:pdf},
journal = {arXiv preprint arXiv:1905.08224},
keywords = {best arm identification,generalized linear bandits,sequential clinical trial},
title = {{Best arm identification in generalized linear bandits}},
url = {http://arxiv.org/pdf/1905.08224.pdf},
year = {2019}
}
@inproceedings{fish2016,
abstract = {We study three classical machine learning algorithms in the context of algorithmic fairness: adaptive boosting, support vector machines, and logistic regression. Our goal is to maintain the high accuracy of these learning algorithms while reducing the degree to which they discriminate against individuals because of their membership in a protected group. Our first contribution is a method for achieving fairness by shifting the decision boundary for the protected group. The method is based on the theory of margins for boosting. Our method performs comparably to or outperforms previous algorithms in the fairness literature in terms of accuracy and low discrimination, while simultaneously allowing for a fast and transparent quantification of the trade-off between bias and error. Our second contribution addresses the shortcomings of the bias-error trade-off studied in most of the algorithmic fairness literature. We demonstrate that even hopelessly naive modifications of a biased algorithm, which cannot be reasonably said to be fair, can still achieve low bias and high accuracy. To help to distinguish between these naive algorithms and more sensible algorithms we propose a new measure of fairness, called resilience to random bias (RRB). We demonstrate that RRB distinguishes well between our naive and sensible fairness algorithms. RRB together with bias and accuracy provides a more complete picture of the fairness of an algorithm.},
archivePrefix = {arXiv},
arxivId = {1601.05764},
author = {Fish, Benjamin and Kun, Jeremy and Lelkes, {\'{A}}d{\'{a}}m D.},
booktitle = {Proceedings of the 16th SIAM International Conference on Data Mining (SDM)},
doi = {10.1137/1.9781611974348.17},
eprint = {1601.05764},
isbn = {9781510828117},
pages = {144--152},
title = {{A confidence-based approach for balancing fairness and accuracy}},
url = {https://arxiv.org/pdf/1601.05764.pdf},
year = {2016}
}
@inproceedings{klein2017fast,
author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein et al. - 2017 - Fast Bayesian optimization of machine learning hyperparameters on large datasets(2).pdf:pdf},
title = {{Fast Bayesian optimization of machine learning hyperparameters on large datasets}},
url = {https://arxiv.org/pdf/1605.07079.pdf},
year = {2017}
}
@inproceedings{agrawal2013thompson,
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Goyal - 2013 - Thompson sampling for contextual bandits with linear payoffs.pdf:pdf},
title = {{Thompson sampling for contextual bandits with linear payoffs}},
url = {http://proceedings.mlr.press/v28/agrawal13.pdf},
year = {2013}
}
@inproceedings{kamper2015topdown,
author = {Kamper, Herman and Elsner, Micha and Jansen, Aren and Goldwater, Sharon},
booktitle = {Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2015.7179087},
isbn = {9781467369978},
issn = {15206149},
keywords = {Unsupervised feature extraction,deep neural networks,top-down constraints,zero-resource speech processing},
pages = {5818--5822},
title = {{Unsupervised neural network based feature extraction using weak top-down constraints}},
url = {https://homepages.inf.ed.ac.uk/sgwater/papers/icassp15-correspondenceAE.pdf},
year = {2015}
}
@article{shang2020vector,
author = {Shang, Xuedong and Shao, Han and Qian, Jian},
journal = {arXiv preprint arXiv:2010.08061},
title = {{Stochastic bandits with vector losses: Minimizing $\ell^\infty$-norm of relative losses}},
url = {https://arxiv.org/pdf/2010.08061.pdf},
year = {2020}
}
@article{tang2020online,
abstract = {Off-policy learning algorithms have been known to be sensitive to the choice of hyper-parameters. However, unlike near on-policy algorithms for which hyper-parameters could be optimized via e.g. meta-gradients, similar techniques could not be straightforwardly applied to off-policy learning. In this work, we propose a framework which entails the application of Evolutionary Strategies to online hyper-parameter tuning in off-policy learning. Our formulation draws close connections to meta-gradients and leverages the strengths of black-box optimization with relatively low-dimensional search spaces. We show that our method outperforms state-of-the-art off-policy learning baselines with static hyper-parameters and recent prior work over a wide range of continuous control benchmarks.},
archivePrefix = {arXiv},
arxivId = {2006.07554},
author = {Tang, Yunhao and Choromanski, Krzysztof},
eprint = {2006.07554},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Choromanski - 2020 - Online hyper-parameter tuning in off-policy learning via evolutionary strategies.pdf:pdf},
journal = {arXiv preprint arXiv:2006.07554},
title = {{Online hyper-parameter tuning in off-policy learning via evolutionary strategies}},
url = {http://arxiv.org/pdf/2006.07554.pdf},
year = {2020}
}
@article{little2013pig,
abstract = {Discovering the structure underlying observed data is a recurring problem in machine learning with important applications in neuroscience. It is also a primary function of the brain. When data can be actively collected in the context of a closed actionperception loop, behavior becomes a critical determinant of learning efficiency. Psychologists studying exploration and curiosity in humans and animals have long argued that learning itself is a primary motivator of behavior. However, the theoretical basis of learning-driven behavior is not well understood. Previous computational studies of behavior have largely focused on the control problem of maximizing acquisition of rewards and have treated learning the structure of data as a secondary objective. Here, we study exploration in the absence of external reward feedback. Instead, we take the quality of an agent's learned internal model to be the primary objective. In a simple probabilistic framework, we derive a Bayesian estimate for the amount of information about the environment an agent can expect to receive by taking an action, a measure we term the predicted information gain (PIG). We develop exploration strategies that approximately maximize PIG. One strategy based on value-iteration consistently learns faster, across a diverse range of environments, than previously developed reward-free exploration strategies. Psychologists believe the evolutionary advantage of learning driven exploration lies in the generalized utility of an accurate internal model. Consistent with this hypothesis, we demonstrate that agents which learn more efficiently during exploration are later better able to accomplish a range of goal-directed tasks. We will conclude by discussing how our work elucidates the explorative behaviors of animals and humans, its relationship to other computational models of behavior, and its potential application to experimental design, such as in closed-loop neurophysiology studies. {\textcopyright} 2013 Little and Sommer.},
author = {Little, Daniel Y. and Sommer, Friedrich T.},
doi = {10.3389/fncir.2013.00037},
issn = {16625110},
journal = {Frontiers in Neural Circuits},
keywords = {Behavioral psychology,Computational neuroscience,Control theory,Information theory,Knowledge acquisition,Machine learning},
title = {{Learning and exploration in action-perception loops}},
url = {https://pubmed.ncbi.nlm.nih.gov/23579347/},
year = {2013}
}
@article{agarwal2019policy,
abstract = {Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution (say with a sufficiently rich policy class); how they cope with approximation error due to using a restricted class of parametric policies; or their finite sample behavior. Such characterizations are important not only to compare these methods to their approximate value function counterparts (where such issues are relatively well understood, at least in the worst case), but also to help with more principled approaches to algorithm design. This work provides provable characterizations of computational, approximation, and sample size issues with regards to policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: 1) "tabular" policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy, and 2) restricted policy classes, which may not contain the optimal policy and where we provide agnostic learning results. One insight of this work is in formalizing the importance how a favorable initial state distribution provides a means to circumvent worst-case exploration issues. Overall, these results place policy gradient methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms.},
archivePrefix = {arXiv},
arxivId = {1908.00261},
author = {Agarwal, Alekh and Kakade, Sham M. and Lee, Jason D. and Mahajan, Gaurav},
eprint = {1908.00261},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal et al. - 2019 - Optimality and approximation with policy gradient methods in Markov decision processes.pdf:pdf},
journal = {arXiv preprint arXiv:1908.00261},
title = {{Optimality and approximation with policy gradient methods in Markov decision processes}},
url = {http://arxiv.org/pdf/1908.00261.pdf},
year = {2019}
}
@article{stadie2015curiosity,
abstract = {Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.},
archivePrefix = {arXiv},
arxivId = {1507.00814},
author = {Stadie, Bradly C. and Levine, Sergey and Abbeel, Pieter},
eprint = {1507.00814},
journal = {arXiv preprint arXiv:1507.00814},
title = {{Incentivizing exploration in reinforcement learning with deep predictive models}},
url = {http://arxiv.org/pdf/1507.00814.pdf},
year = {2015}
}
@inproceedings{carpentier2016budget,
abstract = {We consider the problem of $\backslash$textit{\{}best arm identification{\}} with a $\backslash$textit{\{}fixed budget {\$}T{\$}{\}}, in the {\$}K{\$}-armed stochastic bandit setting, with arms distribution defined on {\$}[0,1]{\$}. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity {\$}H{\$}, will misidentify the best arm with probability lower bounded by {\$}{\$}$\backslash$exp$\backslash$Big(-$\backslash$frac{\{}T{\}}{\{}$\backslash$log(K)H{\}}$\backslash$Big),{\$}{\$} where {\$}H{\$} is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by {\$}\backslashexp(-T/H){\$}. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem.},
archivePrefix = {arXiv},
arxivId = {1605.09004},
author = {Carpentier, Alexandra and Locatelli, Andrea},
booktitle = {Proceedings of the 29th Annual Conference on Learning Theory (CoLT)},
eprint = {1605.09004},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carpentier, Locatelli - 2016 - Tight (lower) bounds for the fixed budget best arm identification bandit problem.pdf:pdf},
keywords = {bandit theory,best arm identification,fixed confidence setting,simple regret},
title = {{Tight (lower) bounds for the fixed budget best arm identification bandit problem}},
url = {https://arxiv.org/pdf/1605.09004.pdf},
year = {2016}
}
@inproceedings{garivier2016tracknstop,
abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.},
archivePrefix = {arXiv},
arxivId = {1602.04589},
author = {Garivier, Aur{\'{e}}lien and Kaufmann, Emilie},
booktitle = {Proceedings of the 29th Annual Conference on Learning Theory (CoLT)},
eprint = {1602.04589},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garivier, Kaufmann - 2016 - Optimal best arm identification with fixed confidence.pdf:pdf},
title = {{Optimal best arm identification with fixed confidence}},
url = {http://arxiv.org/pdf/1602.04589.pdf},
year = {2016}
}
@article{thompson1933,
abstract = {IN elaborating the relations of the present conmmunication interest was not centred upon the interpretation of particular data, but grew out of a general interest in problems of research planning. From this point of view there can be no objection to the use of data, however meagre, as a guide to action required before more can be collected; although serious objection can otherwise be raised to argument based upon a small number of observations. Indeed, the fact that such objection can never be eliminated entirely-no matter how great the number of observations-suggested the possible value of seeking other modes of operation than that of taking a large number of observations before analysis or any attemipt to direct our course. This problem is more general than that treated in Section 2, and is directly con-cerned with any case where probability criteria may be established by means of which we judge whether one mode of operation is better than another in some given sense or not. Thus, if, in this sense, P is the probability estimnate that one treatment of a certain class of individuals is better than a second, as judged by data at present available, then we might take some monotone increasing function of P, sayf(p), to fix the fraction of such individuals to be treated in the first manner; until more evidence may be utilised, where 0 {\textless}{\_} fp) {\textless} 1; the remaining fraction of such individuals (1 -f(p)) to be treated in the second manner; or we may establish a probability of treatment by the two methods of f(p) and 1 -f(p), respectively. If such a discipline were adopted, even though it were not the best possible, it seems apparent that a considerable saving of individuals otherwise sacrificed to the inferior treatment might be effected. This would be imnportant in cases where either the rate of accumulation of data is slow or the individuals treated are valuable, or both. If we arbitrarily decide to eliminate the second treatment in favour of the first at this time, then the expectation of sacrifice to the inferior treatment would be (1 -P) for all subsequently treated individuals; whereas, if, for example, we take f(p) = P, the expectation of such sacrifice would be temporartly This content downloaded from 143.89.58.9 on Thu, 30 Nov 2017 08:26:26 UTC All use subject to http://about.jstor.org/terms},
author = {Thompson, William R.},
doi = {10.2307/2332286},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
number = {3/4},
pages = {285},
title = {{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}},
url = {https://www.jstor.org/stable/pdf/2332286.pdf?refreqid=excelsior{\%}3A762c2b32503588b3102b4844358605f2},
volume = {25},
year = {1933}
}
@inproceedings{foster2018,
abstract = {A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded. We present a new technique that has the empiri-cal and computational advantages of realizability- based approaches combined with the flexibility of agnostic methods. Our algorithms leverage the availability of a regression oracle for the value- function class, a more realistic and reasonable oracle than the classification oracles over policies typically assumed by agnostic methods. Our approach generalizes both UCB and LinUCB to far more expressive possible model classes and achieves low regret under certain distributional as-sumptions. In an extensive empirical evaluation, we find that our approach typically matches or outperforms both realizability-based and agnostic baselines.},
archivePrefix = {arXiv},
arxivId = {1803.01088},
author = {Foster, Dylan J. and Agarwal, Alekh and Dudik, Miroslav and Haipeng, Luo and Schapire, Robert E.},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1803.01088},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foster et al. - 2018 - Practical contextual bandits with regression oracles(2).pdf:pdf},
isbn = {9781510867963},
pages = {2482--2517},
title = {{Practical contextual bandits with regression oracles}},
url = {https://arxiv.org/pdf/1803.01088.pdf},
volume = {4},
year = {2018}
}
@article{zhang2016generalization,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {1611.03530},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Understanding deep learning requires rethinking generalization.pdf:pdf},
journal = {arXiv preprint arXiv:1611.03530},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/pdf/1611.03530.pdf},
year = {2016}
}
@phdthesis{shang2017master,
abstract = {Heuristics like Monte-Carlo Tree Search (MCTS), which trades off well exploration and exploitation, are widely used for sequential global optimization problems, and has led to some great success especially in game AI designing. In many cases, the exploration phase follows the famous optimism in the face of uncertainty principle, which is encountered in the so-called multiarmed bandit problem. However, recent studies on these models shows that they are not optimal for the optimization purpose, and that methods based on best arm identification are preferred. During this internship, some approaches based on these new statistic tools are investigated both in a practical and theoretical way},
author = {Shang, Xuedong},
month = {jun},
school = {ENS Cachan (Rennes)},
title = {{Hierarchical bandits for black-box optimization and Monte-Carlo tree search}},
url = {https://xuedong.github.io/static/documents/bandits.pdf},
year = {2017}
}
@inproceedings{thune2018soda,
author = {Thune, Tobias S. and Seldin, Yevgeny},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
pages = {2914--2923},
title = {{Adaptation to easy data in prediction with limited advice}},
url = {https://arxiv.org/pdf/1807.00636.pdf},
year = {2018}
}
@inproceedings{david2015deterministic,
abstract = {We consider a variant of the Multi-Armed Bandit problem which involves a large pool of a priori identical arms (or items). Each arm is associated with a deterministic value, which is sampled from a probability distribution with unknown maximal value, and is revealed once that arm is chosen. At each time instant the agent may choose a new arm (with unknown value), or a previously-chosen arm whose value is already revealed. The goal is to minimize the cumulative regret relative to the best arm in the pool. Previous work has established a lower bound on the regret for this model, depending on the functional form of the tail of the sample distribution, as well as algorithms that attain this bound up to logarithmic terms. Here, we present a more refined algorithm that attains the same order as the lower bound. We further consider several variants of the basic model, involving an anytime algorithm and the case of non-retainable arms. Numerical experiments demonstrate the superior performance of the suggested algorithms.},
author = {David, Yahel and Shimkin, Nahum},
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2015 (ECML-PKDD)},
doi = {10.1007/978-3-319-23528-8_29},
isbn = {9783319235271},
issn = {16113349},
keywords = {Many-armed bandits,Regret minimization},
pages = {464--479},
title = {{Refined algorithms for infinitely many-armed bandits with deterministic rewards}},
url = {https://link.springer.com/chapter/10.1007{\%}2F978-3-319-23528-8{\_}29},
year = {2015}
}
@article{huo2017risk,
abstract = {Sequential portfolio selection has attracted increasing interest in the machine learning and quantitative finance communities in recent years. As amathematical framework for reinforcement learning policies, the stochastic multi-armed bandit problem addresses the primary difficulty in sequential decision-making under uncertainty, namely the exploration versus exploitation dilemma, and therefore provides a natural connection to portfolio selection. In this paper, we incorporate risk awareness into the classic multi-armed bandit setting and introduce an algorithm to construct portfolio. Through filtering assets based on the topological structure of the financial market and combining the optimal multi-armed bandit policy with the minimization of a coherent risk measure, we achieve a balance between risk and return.},
author = {Huo, Xiaoguang and Fu, Feng},
doi = {10.1098/rsos.171377},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Conditional value-at-risk,Graph theory,Multi-armed bandit,Online learning,Portfolio selection,Risk-awareness},
number = {11},
title = {{Risk-aware multi-armed bandit problem with application to portfolio selection}},
url = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.171377},
volume = {4},
year = {2017}
}
@inproceedings{maclaurin2015,
abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
archivePrefix = {arXiv},
arxivId = {1502.03492},
author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
eprint = {1502.03492},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maclaurin, Duvenaud, Adams - 2015 - Gradient-based hyperparameter optimization through reversible learning.pdf:pdf},
pages = {2113--2122},
title = {{Gradient-based hyperparameter optimization through reversible learning}},
url = {http://arxiv.org/pdf/1502.03492.pdf},
year = {2015}
}
@inproceedings{azar2011speedy,
abstract = {We introduce a new convergent variant of Q-learning, called speedy Q-learning (SQL), to address the problem of slow convergence in the standard form of the Q-learning algorithm. We prove a PAC bound on the performance of SQL, which shows that for an MDP with n state-action pairs and the discount factor $\gamma$ only T = O (log(n)/($\epsilon$ 2 (1 - $\gamma$) 4)) steps are required for the SQL algorithm to converge to an $\epsilon$-optimal action-value function with high probability. This bound has a better dependency on 1/$\epsilon$ and 1/(1-$\gamma$), and thus, is tighter than the best available result for Q-learning. Our bound is also superior to the existing results for both model-free and model-based instances of batch Q-value iteration that are considered to be more efficient than the incremental methods like Q-learning.},
author = {Azar, Mohammad Gheshlaghi and Munos, Remi and Ghavamzadeh, Mohammad and Kappen, Hilbert J.},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
isbn = {9781618395993},
issn = {1049-5258},
pages = {2411--2419},
title = {{Speedy Q-learning}},
url = {https://papers.nips.cc/paper/2011/file/ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf},
year = {2011}
}
@inproceedings{kaufmann2013kl,
abstract = {We consider the problem of efficiently exploring the arms of a stochastic bandit to identify the best subset of a specified size. Under the PAC and the fixed-budget formulations, we derive improved bounds by using KL-divergence-based confidence intervals. Whereas the application of a similar idea in the regret setting has yielded bounds in terms of the KL-divergence between the arms, our bounds in the pure-exploration setting involve the " Chernoff information " between the arms. In addition to introducing this novel quantity to the bandits literature, we contribute a comparison between strategies based on uniform and adaptive sampling for pure-exploration problems, finding evidence in favor of the latter.},
author = {Kaufmann, Emilie and Kalyanakrishnan, Shivaram},
booktitle = {Proceedings of the 26th Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann, Kalyanakrishnan - 2013 - Information complexity in bandit subset selection.pdf:pdf},
issn = {15337928},
keywords = {kl-divergence,stochastic multi-armed bandits,subset selection},
pages = {228--251},
title = {{Information complexity in bandit subset selection}},
url = {http://proceedings.mlr.press/v30/Kaufmann13.pdf},
year = {2013}
}
@article{raghavan2018,
abstract = {Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration - the undesirable side effects that the presence of one party may impose on another - under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most {\$}\backslashtilde{\{}O{\}}(T{\^{}}{\{}1/3{\}}){\$}. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish under the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.},
archivePrefix = {arXiv},
arxivId = {1806.00543},
author = {Raghavan, Manish and Slivkins, Aleksandrs and Vaughan, Jennifer W. and Wu, Zhiwei S.},
eprint = {1806.00543},
journal = {arXiv preprint arXiv:1806.00543},
title = {{The externalities of exploration and how data diversity helps exploitation}},
url = {http://arxiv.org/pdf/1806.00543.pdf},
year = {2018}
}
@inproceedings{domingues2020lb,
abstract = {In this paper, we propose new problem-independent lower bounds on the sample complexity and regret in episodic MDPs, with a particular focus on the non-stationary case in which the transition kernel is allowed to change in each stage of the episode. Our main contribution is a novel lower bound of $\Omega$((H3SA/$\epsilon$2) log(1/$\delta$)) on the sample complexity of an ($\epsilon$, $\delta$)-PAC algorithm for best policy identification in a non-stationary MDP. This lower bound relies on a construction of “hard MDPs” which is different from the ones previously used in the literature. Using this same class of MDPs, we also provide a rigorous proof of the $\Omega$(√H3SAT) regret bound for non-stationary MDPs. Finally, we discuss connections to PAC-MDP lower bounds.},
archivePrefix = {arXiv},
arxivId = {2010.03531},
author = {Domingues, Omar Darwiche and M{\'{e}}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
booktitle = {Proceedings of the 32nd International Conference on Algorithmic Learning Theory (ALT)},
eprint = {2010.03531},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingues et al. - 2021 - Episodic reinforcement learning in finite MDPs Minimax lower bounds revisited.pdf:pdf},
issn = {23318422},
keywords = {Episodic,Lower bounds,Reinforcement learning},
title = {{Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited}},
url = {https://arxiv.org/pdf/2010.03531.pdf},
year = {2021}
}
@inproceedings{duan2016benchmark,
abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
archivePrefix = {arXiv},
arxivId = {1604.06778},
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
eprint = {1604.06778},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2016 - Benchmarking deep reinforcement learning for continuous control.pdf:pdf},
isbn = {9781510829008},
pages = {2001--2014},
title = {{Benchmarking deep reinforcement learning for continuous control}},
url = {https://arxiv.org/pdf/1604.06778.pdf},
year = {2016}
}
@article{russo2014posterior,
abstract = {We present a model that explains how a cluster moves through a life cycle and why this movement differs from the industry life cycle. The model is based on three key processes: the changing heterogeneity in the cluster describes the movement of the cluster through the life cycle; the geographical absorptive capacity enables clustered companies to take advantage of a larger diversity of knowledge and the stronger convergence of clustered companies compared to non-clustered companies results in a reduction of heterogeneity. We apply these processes to four stages of the cluster life cycle: emergence, growth, sustainment and decline.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.2609v5},
author = {Russo, Daniel and van Roy, Benjamin},
doi = {10.1287/xxxx.0000.0000},
eprint = {arXiv:1301.2609v5},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russo, van Roy - 2014 - Learning to optimize via posterior sampling(2).pdf:pdf},
isbn = {1817257722},
journal = {Mathematics of Operations Research},
keywords = {2013,62l05,93e35,armed bandits,decision analysis,history,ms subject classification,msc2000 subject classification,multi,online optimization,or,primary,received february 26,revised november 21,secondary,sequential,thompson sampling},
number = {4},
pages = {1221--1243},
title = {{Learning to optimize via posterior sampling}},
url = {https://www.jstor.org/stable/pdf/24541007.pdf},
volume = {39},
year = {2014}
}
@article{collobert2011nlp,
abstract = {We propose a unified neural network architecture and learning algorithmthat can be applied to var- ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re- quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural language processing (almost) from scratch(2).pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural language processing (almost) from scratch}},
url = {http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf},
volume = {12},
year = {2011}
}
@article{bottou2013counterfactual,
abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems. This work is illustrated by experiments carried out on the ad placement system associated with the Bing search engine.},
archivePrefix = {arXiv},
arxivId = {1209.2355},
author = {Bottou, L{\'{e}}on and Peters, Jonas and Qui{\~{n}}onero-Candela, Joaquin and Charles, Denis X. and Chickering, Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
doi = {10.1145/2740908.2742564},
eprint = {1209.2355},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {3207--3260},
pmid = {1404661},
title = {{Counterfactual reasoning and learning systems}},
url = {http://arxiv.org/pdf/1209.2355.pdf},
volume = {14},
year = {2013}
}
@article{kandasamy2019dragonfly,
abstract = {Bayesian Optimisation (BO), refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently find the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.},
archivePrefix = {arXiv},
arxivId = {1903.06694},
author = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabas and Xing, Eric P.},
eprint = {1903.06694},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kandasamy et al. - 2019 - Tuning hyperparameters without grad students Scalable and robust Bayesian optimisation with Dragonfly.pdf:pdf},
journal = {arXiv preprint arXiv:1903.06694},
title = {{Tuning hyperparameters without grad students: Scalable and robust Bayesian optimisation with Dragonfly}},
url = {http://arxiv.org/odf/1903.06694.pdf},
year = {2019}
}
@inproceedings{falkner2018bohb,
abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
archivePrefix = {arXiv},
arxivId = {1807.01774},
author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1807.01774},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falkner, Klein, Hutter - 2018 - BOHB Robust and efficient hyperparameter optimization at scale(2).pdf:pdf},
title = {{BOHB: Robust and efficient hyperparameter optimization at scale}},
url = {https://arxiv.org/pdf/1807.01774.pdf},
year = {2018}
}
@inproceedings{sutskever2014seq2seq,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
isbn = {1409.3215},
issn = {09205691},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://arxiv.org/pdf/1409.3215.pdf},
year = {2014}
}
@inproceedings{chen2021combinatorial,
abstract = {In this paper, we propose the novel model of combinatorial pure exploration with partial linear feedback (CPE-PL). In CPE-PL, given a combinatorial action space X ⊆ {\{}0, 1{\}}d, in each round a learner chooses one action x ∈ X to play, obtains a random (possibly nonlinear) reward related to x and an unknown latent vector $\theta$ ∈ Rd, and observes a partial linear feedback Mx($\theta$ + $\eta$), where $\eta$ is a zero-mean noise vector and Mx is a transformation matrix for x. The objective is to identify the optimal action with the maximum expected reward using as few rounds as possible. We also study the important subproblem of CPE-PL, i.e., combinatorial pure exploration with full-bandit feedback (CPE-BL), in which the learner observes full-bandit feedback (i.e. Mx = xT) and gains linear expected reward xT$\theta$ after each play. In this paper, we first propose a polynomial-time algorithmic framework for the general CPE-PL problem with novel sample complexity analysis. Then, we propose an adaptive algorithm dedicated to the subproblem CPE-BL with better sample complexity. Our work provides a novel polynomial-time solution to simultaneously address limited feedback, general reward function and combinatorial action space including matroids, matchings, and s-t paths.},
archivePrefix = {arXiv},
arxivId = {arXiv:2006.07905v2},
author = {Chen, Wei and Du, Yihan and Kuroki, Yuko},
booktitle = {Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {arXiv:2006.07905v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Du, Kuroki - 2021 - Combinatorial pure exploration with partial or full-bandit linear feedback.pdf:pdf},
issn = {23318422},
title = {{Combinatorial pure exploration with partial or full-bandit linear feedback}},
url = {https://arxiv.org/pdf/2006.07905.pdf},
year = {2021}
}
@article{li2016,
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with state-of-the-art methods on a suite of hyperparameter optimization problems. We observe that Hyperband provides five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems.},
archivePrefix = {arXiv},
arxivId = {1603.06560},
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
eprint = {1603.06560},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Hyperband A Novel Bandit-Based Approach to Hyperparameter Optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1603.06560},
month = {mar},
title = {{Hyperband: A novel bandit-based approach to hyperparameter optimization}},
url = {http://arxiv.org/pdf/1603.06560.pdf},
year = {2016}
}
@article{hansen2001cmaes,
abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
author = {Hansen, N. and Ostermeier, A.},
doi = {10.1162/106365601750190398},
issn = {10636560},
journal = {Evolutionary Computation},
number = {2},
pages = {159--195},
pmid = {11382355},
title = {{Completely derandomized self-adaptation in evolution strategies.}},
url = {https://dl.acm.org/doi/10.1162/106365601750190398},
volume = {9},
year = {2001}
}
@inproceedings{sui2018safe,
abstract = {Enforcing safety is a key aspect of many problems pertaining to sequential decision making under uncertainty, which require the decisions made at every step to be both informative of the optimal decision and also safe. For example, we value both efficacy and comfort in medical therapy, and efficiency and safety in robotic control. We consider this problem of optimizing an unknown utility function with absolute feedback or preference feedback subject to unknown safety constraints. We develop an efficient safe Bayesian optimization algorithm, STAGEOPT, that separates safe region expansion and utility function maximization into two distinct stages. Compared to existing approaches which interleave between expansion and optimization, we show that STA-GEOPT is more efficient and naturally applicable to a broader class of problems. We provide theoretical guarantees for both the satisfaction of safety constraints as well as convergence to the optimal utility value. We evaluate STAGEOPT on both a variety of synthetic experiments, as well as in clinical practice. We demonstrate that STAGEOPT is more effective than existing safe optimization approaches, and is able to safely and effectively optimize spinal cord stimulation therapy in our clinical experiments.},
archivePrefix = {arXiv},
arxivId = {1806.07555},
author = {Sui, Yanan and Zhuang, Vincent and Burdick, Joel W. and Yue, Yisong},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1806.07555},
isbn = {9781510867963},
issn = {1938-7228},
pages = {7602--7613},
title = {{Stagewise safe Bayesian optimization with Gaussian processes}},
url = {https://proceedings.mlr.press/v80/sui18a/sui18a.pdf},
year = {2018}
}
@inproceedings{agrawal2013further,
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of {\$}(1+\backslashepsilon)\backslashsum{\_}i \backslashfrac{\{}\backslashln T{\}}{\{}\backslashDelta{\_}i{\}}+O(\backslashfrac{\{}N{\}}{\{}\backslashepsilon{\^{}}2{\}}){\$} and the first near-optimal problem-independent bound of {\$}O(\backslashsqrt{\{}NT\backslashln T{\}}){\$} on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].},
archivePrefix = {arXiv},
arxivId = {1209.3353},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AIStats)},
doi = {10.1145/3088510},
eprint = {1209.3353},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Goyal - 2013 - Further optimal regret bounds for Thompson sampling.pdf:pdf},
issn = {15337928},
pages = {99--107},
title = {{Further optimal regret bounds for Thompson sampling}},
url = {http://arxiv.org/pdf/1209.3353.pdf},
year = {2013}
}
@inproceedings{degenne2019pure,
abstract = {We determine the sample complexity of pure exploration bandit problems with multiple good answers. We derive a lower bound using a new game equilibrium argument. We show how continuity and convexity properties of single-answer problems ensures that the Track-and-Stop algorithm has asymptotically optimal sample complexity. However, that convexity is lost when going to the multiple-answer setting. We present a new algorithm which extends Track-and-Stop to the multiple-answer case and has asymptotic sample complexity matching the lower bound.},
archivePrefix = {arXiv},
arxivId = {1902.03475},
author = {Degenne, R{\'{e}}my and Koolen, Wouter M.},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1902.03475},
title = {{Pure exploration with multiple correct answers}},
url = {https://arxiv.org/pdf/1902.03475.pdf},
year = {2019}
}
@inproceedings{benureau2015diversity,
abstract = {We consider a scenario where an agent has multiple available strategies to explore an unknown environment. For each new interaction with the environment, the agent must select which exploration strategy to use. We provide a new strategy-agnostic method that treat the situation as a Multi- Armed Bandits problem where the reward signal is the diversity of effects that each strategy produces. We test the method empirically on a simulated planar robotic arm, and establish that the method is both able discriminate between strategies of dissimilar quality, even when the differences are tenuous, and that the resulting performance is competitive with the best fixed mixture of strategies.},
author = {Benureau, Fabien and Oudeyer, Pierre Yves},
booktitle = {Proceedings of the 5th Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
doi = {10.1109/DEVLRN.2015.7346130},
isbn = {9781467393201},
pages = {135--142},
title = {{Diversity-driven selection of exploration strategies in multi-armed bandits}},
url = {https://arxiv.org/pdf/1808.07739.pdf},
year = {2015}
}
@inproceedings{begin2016renyi,
author = {B{\'{e}}gin, Luc and Germain, Pascal and Laviolette, Fran{\c{c}}ois and Roy, Jean-francis and B{\'{e}}gin, Luc and Germain, Pascal and Laviolette, Fran{\c{c}}ois and Bounds, Jean-francis Roy Pac-bayesian},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\'{e}}gin et al. - 2016 - PAC-Bayesian bounds based on the R{\'{e}}nyi divergence.pdf:pdf},
pages = {435--444},
title = {{PAC-Bayesian bounds based on the R{\'{e}}nyi divergence}},
url = {https://hal.inria.fr/hal-01384783/document},
year = {2016}
}
@book{cover2006,
author = {Cover, Thomas M. and Thomas, Joy A.},
isbn = {9780387781891},
issn = {08848289},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Elements of Information Theory}},
url = {http://staff.ustc.edu.cn/{~}cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf},
year = {2006}
}
@article{ranganath2014bbvi,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
doi = {10.1016/j.indcrop.2013.11.045},
eprint = {1401.0118},
isbn = {1401.0118},
issn = {15337928},
journal = {Artificial Intelligence and Statistics},
pages = {814--812},
pmid = {16792136},
title = {{Black box variational inference}},
url = {http://arxiv.org/pdf/1401.0118.pdf},
year = {2014}
}
@article{bubeck2010x,
abstract = {We consider a generalization of stochastic bandits where the set of arms, {\$}\backslashcX{\$}, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if {\$}\backslashcX{\$} is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by {\$}\backslashsqrt{\{}n{\}}{\$}, i.e., the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
archivePrefix = {arXiv},
arxivId = {1001.4475},
author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles and Szepesvari, Csaba},
eprint = {1001.4475},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck et al. - 2010 - X-armed bandits.pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {1587--1627},
title = {{X-armed bandits}},
url = {http://arxiv.org/pdf/1001.4475.pdf},
volume = {12},
year = {2010}
}
@inproceedings{haochen2019random,
abstract = {A long-standing problem in the theory of stochastic gradient descent (SGD) is to prove that its without-replacement version RandomShuffle converges faster than the usual with-replacement version. We present the first (to our knowledge) non-asymptotic solution to this problem, which shows that after a "reasonable" number of epochs RandomShuffle indeed converges faster than SGD. Specifically, we prove that under strong convexity and second-order smoothness, the sequence generated by RandomShuffle converges to the optimal solution at the rate O(1/T{\^{}}2 + n{\^{}}3/T{\^{}}3), where n is the number of components in the objective, and T is the total number of iterations. This result shows that after a reasonable number of epochs RandomShuffle is strictly better than SGD (which converges as O(1/T)). The key step toward showing this better dependence on T is the introduction of n into the bound; and as our analysis will show, in general a dependence on n is unavoidable without further changes to the algorithm. We show that for sparse data RandomShuffle has the rate O(1/T{\^{}}2), again strictly better than SGD. Furthermore, we discuss extensions to nonconvex gradient dominated functions, as well as non-strongly convex settings.},
archivePrefix = {arXiv},
arxivId = {1806.10077},
author = {HaoChen, Jeffery Z. and Sra, Suvrit},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1806.10077},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/HaoChen, Sra - 2019 - Random shuffling beats SGD after finite epochs.pdf:pdf},
title = {{Random shuffling beats SGD after finite epochs}},
url = {http://arxiv.org/pdf/1806.10077.pdf},
year = {2019}
}
@inproceedings{geyik2018talent,
abstract = {Previous efforts in recommendation of candidates for talent search followed the general pattern of receiving an initial search criteria and generating a set of candidates utilizing a pre-trained model. Traditionally, the generated recommendations are final, that is, the list of potential candidates is not modified unless the user explicitly changes his/her search criteria. In this paper, we are proposing a candidate recommendation model which takes into account the immediate feedback of the user, and updates the candidate recommendations at each step. This setting also allows for very uninformative initial search queries, since we pinpoint the user's intent due to the feedback during the search session. To achieve our goal, we employ an intent clustering method based on topic modeling which separates the candidate space into meaningful, possibly overlapping, subsets (which we call intent clusters) for each position. On top of the candidate segments, we apply a multi-armed bandit approach to choose which intent cluster is more appropriate for the current session. We also present an online learning scheme which updates the intent clusters within the session, due to user feedback, to achieve further personalization. Our offline experiments as well as the results from the online deployment of our solution demonstrate the benefits of our proposed methodology.},
archivePrefix = {arXiv},
arxivId = {1809.06488},
author = {Geyik, Sahin Cem and Dialani, Vijay and Meng, Meng and Smith, Ryan},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM)},
eprint = {1809.06488},
title = {{In-session personalization for talent search}},
url = {http://arxiv.org/pdf/1809.06488.pdf},
year = {2018}
}
@article{joulani2017,
abstract = {Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based on a new regret decomposition and a generalization of Bregman divergences, we provide a self-contained, modular analysis of the two workhorses of online learning: (general) adaptive versions of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms. The analysis is done with extra care so as not to introduce assumptions not needed in the proofs and allows to combine, in a straightforward way, different algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning settings (e.g., strongly convex or composite objectives). This way we are able to reprove, extend and refine a large body of the literature, while keeping the proofs concise. The second contribution is a byproduct of this careful analysis: We present algorithms with improved variational bounds for smooth, composite objectives, including a new family of optimistic MD algorithms with only one projection step per round. Furthermore, we provide a simple extension of adaptive regret bounds to practically relevant non-convex problem settings with essentially no extra effort.},
archivePrefix = {arXiv},
arxivId = {1709.02726},
author = {Joulani, Pooria and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, Csaba},
eprint = {1709.02726},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulani, Gy{\"{o}}rgy, Szepesv{\'{a}}ri - 2017 - A modular analysis of adaptive (non-)convex optimization Optimism, composite objectives, and vari.pdf:pdf},
journal = {Proceedings of the 28th International Conference on Algorithmic Learning Theory (ALT)},
keywords = {ada-,follow-the-regularized-leader,grad,implicit updates,mirror-descent,non-convex optimization,online learning,optimistic online,stochastic optimization},
title = {{A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, and variational bounds}},
url = {https://arxiv.org/pdf/1709.02726.pdf},
year = {2017}
}
@inproceedings{aziz2018confidence,
abstract = {We consider the problem of near-optimal arm identification in the fixed confidence setting of the infinitely armed bandit problem when nothing is known about the arm reservoir distribution. We (1) introduce a PAC-like framework within which to derive and cast results; (2) derive a sample complexity lower bound for near-optimal arm identification; (3) propose an algorithm that identifies a nearly-optimal arm with high probability and derive an upper bound on its sample complexity which is within a log factor of our lower bound; and (4) discuss whether our log{\^{}}2(1/delta) dependence is inescapable for "two-phase" (select arms first, identify the best later) algorithms in the infinite setting. This work permits the application of bandit models to a broader class of problems where fewer assumptions hold.},
archivePrefix = {arXiv},
arxivId = {1803.04665},
author = {Aziz, Maryam and Anderton, Jesse and Kaufmann, Emilie and Aslam, Javed},
booktitle = {Proceedings of the 29th International Conference on Algorithmic Learning Theory (ALT)},
eprint = {1803.04665},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aziz et al. - 2018 - Pure exploration in infinitely-armed bandit models with fixed-confidence.pdf:pdf},
keywords = {infinitely-armed bandit models,pure exploration},
title = {{Pure exploration in infinitely-armed bandit models with fixed-confidence}},
url = {http://arxiv.org/pdf/1803.04665.pdf},
year = {2018}
}
@inproceedings{grant2019sensors,
abstract = {We consider the problem of adaptively placing sensors along an interval to detect stochastically-generated events. We present a new formulation of the problem as a continuum-armed bandit problem with feedback in the form of partial observations of realisations of an inhomogeneous Poisson process. We design a solution method by combining Thompson sampling with nonparametric inference via increasingly granular Bayesian histograms and derive an {\$}\backslashtilde{\{}O{\}}(T{\^{}}{\{}2/3{\}}){\$} bound on the Bayesian regret in {\$}T{\$} rounds. This is coupled with the design of an efficent optimisation approach to select actions in polynomial time. In simulations we demonstrate our approach to have substantially lower and less variable regret than competitor algorithms.},
archivePrefix = {arXiv},
arxivId = {1905.06821},
author = {Grant, James A. and Boukouvalas, Alexis and Griffiths, Ryan-Rhys and Leslie, David S. and Vakili, Sattar and de Cote, Enrique Munoz},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1905.06821},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grant et al. - 2019 - Adaptive sensor placement for continuous spaces.pdf:pdf},
title = {{Adaptive sensor placement for continuous spaces}},
url = {http://arxiv.org/pdf/1905.06821.pdf},
year = {2019}
}
@article{usmanova2019safe,
abstract = {We address the problem of minimizing a convex smooth function f(x) over a compact polyhedral set D given a stochastic zeroth-order constraint feedback model. This problem arises in safety-critical machine learning applications, such as personalized medicine and robotics. In such cases, one needs to ensure constraints are satisfied while exploring the decision space to find optimum of the loss function. We propose a new variant of the Frank-Wolfe algorithm, which applies to the case of uncertain linear constraints. Using robust optimization, we provide the convergence rate of the algorithm while guaranteeing feasibility of all iterates, with high probability.1},
archivePrefix = {arXiv},
arxivId = {1903.04626},
author = {Usmanova, Ilnura and Krause, Andreas and Kamgarpour, Maryam},
eprint = {1903.04626},
issn = {23318422},
journal = {arXiv preprint arXiv:1903.04626},
title = {{Safe convex learning under uncertain constraints}},
url = {https://arxiv.org/pdf/1903.04626.pdf},
year = {2019}
}
@inproceedings{wang2014bamsoo,
abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
archivePrefix = {arXiv},
arxivId = {1402.7005},
author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and de Freitas, Nando},
booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1402.7005},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Bayesian multi-scale optimistic optimization.pdf:pdf},
title = {{Bayesian multi-scale optimistic optimization}},
url = {http://arxiv.org/pdf/1402.7005.pdf},
year = {2014}
}
@inproceedings{wang2008ucbv,
abstract = {We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matches (up to a logarithmic factor) the upper-bound in some cases.},
author = {Wang, Yizao and Audibert, Jean-Yves and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 21 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Audibert, Munos - 2008 - Algorithms for infinitely many-armed bandits.pdf:pdf},
isbn = {9781605609492},
keywords = {Computational,Information-Theoretic Learning with Statistics,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1729--1736},
title = {{Algorithms for infinitely many-armed bandits}},
url = {https://papers.nips.cc/paper/3452-algorithms-for-infinitely-many-armed-bandits.pdf},
year = {2008}
}
@article{fisher1922,
abstract = {IX. On the illathematical Foundations of Theoretical Statistics. 1By RA , MA, Fellow of Gonville and Caims College, Cambridge, Chief Statistician, Rothamsted Experimental Station, Harpenden. 5. Examples of the Use of Criterion of Consistency ...},
author = {Fisher, Ronald A.},
doi = {10.1098/rsta.1922.0009},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
number = {594-604},
pages = {309--368},
title = {{On the mathematical foundations of theoretical statistics}},
url = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.1922.0009},
volume = {222},
year = {1922}
}
@inproceedings{kaufmann2018murphy,
abstract = {Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.},
archivePrefix = {arXiv},
arxivId = {1806.00973},
author = {Kaufmann, Emilie and Koolen, Wouter M. and Garivier, Aur{\'{e}}lien},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1806.00973},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann, Koolen, Garivier - 2018 - Sequential test for the lowest mean From Thompson to Murphy sampling.pdf:pdf},
issn = {10495258},
pages = {6332--6342},
title = {{Sequential test for the lowest mean: From Thompson to Murphy sampling}},
url = {https://papers.nips.cc/paper/7870-sequential-test-for-the-lowest-mean-from-thompson-to-murphy-sampling.pdf},
year = {2018}
}
@book{berry1985,
author = {Berry, Donald A. and Fristedt, Bert},
publisher = {Springer Science {\&} Business Media},
title = {{Bandit Problems: Sequential Allocations of Experiments}},
url = {https://link.springer.com/content/pdf/10.1007/978-94-015-3711-7.pdf},
year = {1985}
}
@inproceedings{vernade2017stochastic,
abstract = {Online advertising and product recommendation are important domains of applications for multi-armed bandit methods. In these fields, the reward that is immediately available is most often only a proxy for the actual outcome of interest, which we refer to as a conversion. For instance, in web advertising, clicks can be observed within a few seconds after an ad display but the corresponding sale --if any-- will take hours, if not days to happen. This paper proposes and investigates a new stochas-tic multi-armed bandit model in the framework proposed by Chapelle (2014) --based on empirical studies in the field of web advertising-- in which each action may trigger a future reward that will then happen with a stochas-tic delay. We assume that the probability of conversion associated with each action is unknown while the distribution of the conversion delay is known, distinguishing between the (idealized) case where the conversion events may be observed whatever their delay and the more realistic setting in which late conversions are censored. We provide performance lower bounds as well as two simple but efficient algorithms based on the UCB and KLUCB frameworks. The latter algorithm, which is preferable when conversion rates are low, is based on a Poissonization argument, of independent interest in other settings where aggregation of Bernoulli observations with different success probabilities is required.},
archivePrefix = {arXiv},
arxivId = {1706.09186},
author = {Vernade, Claire and Capp{\'{e}}, Olivier and Perchet, Vianney},
booktitle = {Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI)},
eprint = {1706.09186},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vernade, Capp{\'{e}}, Perchet - 2017 - Stochastic bandit models for delayed conversions(2).pdf:pdf},
title = {{Stochastic bandit models for delayed conversions}},
url = {https://arxiv.org/pdf/1706.09186.pdf},
year = {2017}
}
@article{cesa-bianchi2015,
abstract = {We show a regret minimization algorithm for setting the reserve price in a sequence of second-price auctions, under ? the assumption that all bids are independently drawn from the same unknown and arbitrary distribution. Our algorithm is computationally efficient, and achieves a regret of O(√T) in a sequence of T auctions. This holds even when the number of bidders is stochastic with a known distribution.},
author = {Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mansour, Yishay},
doi = {10.1109/TIT.2014.2365772},
isbn = {9781611972511},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Prediction theory,semi-supervised learning,sequential analysis,statistical learning},
number = {1},
pages = {549--564},
title = {{Regret minimization for reserve prices in second-price auctions}},
url = {https://ieeexplore.ieee.org/document/6939698/},
volume = {61},
year = {2015}
}
@article{wang2020sample,
abstract = {Learning to plan for long horizons is a central challenge in episodic reinforcement learning problems. A fundamental question is to understand how the difficulty of the problem scales as the horizon increases. Here the natural measure of sample complexity is a normalized one: we are interested in the number of episodes it takes to provably discover a policy whose value is $\epsilon$ near to that of the optimal value, where the value is measured by the normalized cumulative reward in each episode. In a COLT 2018 open problem, Jiang and Agarwal conjectured that, for tabular, episodic reinforcement learning problems, there exists a sample complexity lower bound which exhibits a polynomial dependence on the horizon - a conjecture which is consistent with all known sample complexity upper bounds. This work refutes this conjecture, proving that tabular, episodic reinforcement learning is possible with a sample complexity that scales only logarithmically with the planning horizon. In other words, when the values are appropriately normalized (to lie in the unit interval), this results shows that long horizon RL is no more difficult than short horizon RL, at least in a minimax sense. Our analysis introduces two ideas: (i) the construction of an $\epsilon$-net for optimal policies whose log-covering number scales only logarithmically with the planning horizon, and (ii) the Online Trajectory Synthesis algorithm, which adaptively evaluates all policies in a given policy class using sample complexity that scales with the log-covering number of the given policy class. Both may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {2005.00527},
author = {Wang, Ruosong and Du, Simon S. and Yang, Lin F. and Kakade, Sham M.},
eprint = {2005.00527},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Is long horizon reinforcement learning more difficult than short horizon reinforcement learning.pdf:pdf},
issn = {23318422},
journal = {arXiv preprint arXiv:2005.00527},
pages = {1--19},
title = {{Is long horizon reinforcement learning more difficult than short horizon reinforcement learning?}},
url = {https://arxiv.org/pdf/2005.00527.pdf},
year = {2020}
}
@article{menard2020fast,
abstract = {Realistic environments often provide agents with very limited feedback. When the environment is initially unknown, the feedback, in the beginning, can be completely absent, and the agents may first choose to devote all their effort on exploring efficiently. The exploration remains a challenge while it has been addressed with many hand-tuned heuristics with different levels of generality on one side, and a few theoretically-backed exploration strategies on the other. Many of them are incarnated by intrinsic motivation and in particular explorations bonuses. A common rule of thumb for exploration bonuses is to use 1/√n bonus that is added to the empirical estimates of the reward, where n is a number of times this particular state (or a state-action pair) was visited. We show that, surprisingly, for a pure-exploration objective of reward-free exploration, bonuses that scale with 1/n bring faster learning rates, improving the known upper bounds with respect to the dependence on the horizon H. Furthermore, we show that with an improved analysis of the stopping time, we can improve by a factor H the sample complexity in the best-policy identification setting, which is another pure-exploration objective, where the environment provides rewards but the agent is not penalized for its behavior during the exploration phase.},
archivePrefix = {arXiv},
arxivId = {2007.13442},
author = {M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
eprint = {2007.13442},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\'{e}}nard et al. - 2021 - Fast active learning for pure exploration in reinforcement learning(2).pdf:pdf},
journal = {arXiv preprint arXiv:2007.13442},
title = {{Fast active learning for pure exploration in reinforcement learning}},
url = {https://arxiv.org/pdf/2007.13442.pdf},
year = {2021}
}
@book{munos2014,
abstract = {This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees.$\backslash$nThe main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations.$\backslash$nOur main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces$\backslash$n(such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the “smoothness” of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.$\backslash$nIn order to be self-contained, we start with a brief introduction to the stochastic multi-armed bandit problem in Chapter 1 and describe the UCB (Upper Confidence Bound) strategy and several extensions. In Chapter 2 we present the Monte-Carlo Tree Search method applied to computer-go and show the limitations of previous algorithms such as UCT (UCB applied to Trees). This provides motivation for designing theoretically well-founded optimistic optimization algorithms. The main contributions on hierarchical optimistic optimization are described in Chapters 3 and 4 where the general setting of a semi-metric space is introduced and algorithms designed for optimizing a function assumed to be locally smooth (around its maxima) with respect to a$\backslash$nsemi-metric are presented and analyzed. Chapter 3 considers the case when the semi-metric is known and can be used by the algorithm,$\backslash$nwhereas Chapter 4 considers the case when it is not known and describes an adaptive technique that does almost as well as when it is known. Finally in Chapter 5 we describe optimistic strategies for a specific structured problem, namely the planning problem in Markov decision processes with infinite horizon and discounted rewards setting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.0952v2},
author = {Munos, R{\'{e}}mi},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000038},
eprint = {arXiv:1408.0952v2},
isbn = {9781601987662},
issn = {1935-8237},
pages = {1--129},
publisher = {Mike Casey},
title = {{From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning}},
url = {https://hal.archives-ouvertes.fr/hal-00747575v4/document},
volume = {7},
year = {2014}
}
@inproceedings{cesa-bianchi2018composite,
abstract = {We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.},
author = {Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mansour, Yishay},
booktitle = {Proceedings of the 31st Annual Conference on Learning Theory (CoLT)},
keywords = {Nonstochastic bandits,bandit convex optimiza-tion,composite losses,delayed feedback},
pages = {750--773},
title = {{Nonstochastic bandits with composite anonymous feedback}},
url = {http://proceedings.mlr.press/v75/cesa-bianchi18a/cesa-bianchi18a.pdf},
volume = {75},
year = {2018}
}
@techreport{paquet2018rmhmc,
author = {Paquet, Ulrich and Fraccaro, Marco},
title = {{An efficient implementation of Riemannian manifold Hamiltonian Monte Carlo for Gaussian process models}},
url = {https://arxiv.org/pdf/1810.11893.pdf},
year = {2018}
}
@article{moradipari2019safe,
abstract = {The design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional linear safety constraints that need to be satisfied at each round. We provide a new safe algorithm based on linear Thompson Sampling (TS) for this problem and show a frequentist regret of order O(d3/2 log1/2 d {\textperiodcentered} T 1/2 log3/2 T ), which remarkably matches the results provided by [Abeille et al., 2017] for the standard linear TS algorithm in the absence of safety constraints. We compare the performance of our algorithm with a UCB-based safe algorithm and highlight how the inherently randomized nature of TS leads to a superior performance in expanding the set of safe actions the algorithm has access to at each round.},
author = {Moradipari, Ahmadreza and Amani, Sanae and Alizadeh, Mahnoosh and Thrampoulidis, Christos},
issn = {23318422},
journal = {arXiv preprint arXiv:1911.02156},
title = {{Safe linear thompson sampling}},
url = {https://arxiv.org/pdf/1911.02156.pdf},
year = {2019}
}
@article{candes2011rpca,
author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
doi = {10.1162/neco.2009.02-08-706},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Candes et al. - 2011 - Robust principal component analysis.pdf:pdf},
issn = {0899-7667},
journal = {Journal of ACM},
keywords = {Algorithms,Humans,Linear Models,Models,Principal Component Analysis,Principal Component Analysis: methods,Reproducibility of Results,Statistical},
number = {3},
pages = {11},
pmid = {19686071},
title = {{Robust principal component analysis?}},
url = {https://arxiv.org/pdf/0912.3599.pdf},
volume = {58},
year = {2011}
}
@article{miller2018detailed,
abstract = {Doob's theorem provides guarantees of consistent estimation and posterior consistency under very general conditions. Despite the limitation that it only guarantees consistency on a set with prior probability 1, for many models arising in practice, Doob's theorem is an easy way of showing that consistency will hold almost everywhere. In this article, we give a detailed proof of Doob's theorem.},
archivePrefix = {arXiv},
arxivId = {1801.03122},
author = {Miller, Jeffrey W.},
eprint = {1801.03122},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2018 - A detailed treatment of Doob's theorem.pdf:pdf},
journal = {arXiv preprint arXiv:1801.03122},
title = {{A detailed treatment of Doob's theorem}},
url = {http://arxiv.org/pdf/1801.03122.pdf},
year = {2018}
}
@article{kaufmann2017survey,
abstract = {Over the past few years, the multi-armed bandit model has become increasingly popular in the machine learning community, partly because of applications including online content optimization. This paper reviews two different sequential learning tasks that have been considered in the bandit literature ; they can be formulated as (sequentially) learning which distribution has the highest mean among a set of distributions, with some constraints on the learning process. For both of them (regret minimization and best arm identification) we present recent, asymptotically optimal algorithms. We compare the behaviors of the sampling rule of each algorithm as well as the complexity terms associated to each problem.},
author = {Kaufmann, Emilie and Garivier, Aur{\'{e}}lien},
doi = {10.1051/proc/201760114},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann, Garivier - 2017 - Learning the distribution with largest mean two bandit frameworks.pdf:pdf},
journal = {ESAIM: Proceedings and Surveys},
pages = {114--131},
title = {{Learning the distribution with largest mean: two bandit frameworks}},
url = {https://hal.archives-ouvertes.fr/hal-01449822/document},
volume = {60},
year = {2017}
}
@book{pearl2018why,
abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
author = {Pearl, Judea and Mackenzie, Dana},
doi = {10.1090/noti1912},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl, Mackenzie - 2018 - The Book of Why The New Science of Cause and Effect.pdf:pdf},
isbn = {9780465097616},
issn = {0002-9920},
pages = {432},
publisher = {Basic Books},
title = {{The Book of Why: The New Science of Cause and Effect}},
url = {https://www.amazon.fr/Book-Why-Science-Cause-Effect/dp/046509760X},
year = {2018}
}
@misc{dua2017,
abstract = {The UCI Machine Learning Repository is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The archive was created as an ftp archive in 1987 by David Aha and fellow graduate students at UC Irvine. Since that time, it has been widely used by students, educators, and researchers all over the world as a primary source of machine learning data sets. As an indication of the impact of the archive, it has been cited over 1000 times, making it one of the top 100 most cited "papers" in all of computer science. The current version of the web site was designed in 2007 by Arthur Asuncion and David Newman, and this project is in collaboration with Rexa.info at the University of Massachusetts Amherst. Funding support from the National Science Foundation is gratefully acknowledged.},
author = {Dua, Dheeru and Taniskidou, Karra E.},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml/},
year = {2017}
}
@article{arora2018sense,
abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 "discourse atoms" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.},
archivePrefix = {arXiv},
arxivId = {1601.03764},
author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
doi = {10.1542/peds.2011-1773},
eprint = {1601.03764},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora et al. - 2018 - Linear algebraic structure of word senses, with applications to polysemy.pdf:pdf},
isbn = {1601.03764},
issn = {1098-4275},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
pages = {483--495},
pmid = {22473365},
title = {{Linear algebraic structure of word senses, with applications to polysemy}},
url = {https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320},
volume = {6},
year = {2018}
}
@inproceedings{begin2014pac,
abstract = {We propose a PAC-Bayesian analysis of the transductive learning setting, introduced by Vapnik [1998], by proposing a family of new bounds on the generalization error. Some of them are derived from their counterpart in the inductive setting, and others are new. We also compare their behavior.},
author = {B{\'{e}}gin, Luc and Germain, Pascal and Laviolette, Fran{\c{c}}ois and Roy, Jean-Francis},
booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\'{e}}gin et al. - 2014 - PAC-Bayesian theory for transductive learning.pdf:pdf},
issn = {15337928},
pages = {105--113},
title = {{PAC-Bayesian theory for transductive learning}},
url = {http://jmlr.org/proceedings/papers/v33/begin14.html},
year = {2014}
}
@article{gretton2012kernel,
abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {723--773},
title = {{A kernel two-sample test}},
url = {http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf},
volume = {13},
year = {2012}
}
@article{Real2020,
abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
archivePrefix = {arXiv},
arxivId = {2003.03384},
author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
eprint = {2003.03384},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Real et al. - 2020 - AutoML-Zero Evolving machine learning algorithms from scratch.pdf:pdf},
journal = {arXiv preprint arXiv:2003.03384},
title = {{AutoML-Zero: Evolving machine learning algorithms from scratch}},
url = {https://arxiv.org/pdf/2003.03384.pdf},
year = {2020}
}
@article{zhang2021sslhpt,
abstract = {Hyper-parameters of time series models play an important role in time series analysis. Slight differences in hyper-parameters might lead to very different forecast results for a given model, and therefore, selecting good hyper-parameter values is indispensable. Most of the existing generic hyper-parameter tuning methods, such as Grid Search, Random Search, Bayesian Optimal Search, are based on one key component - search, and thus they are computationally expensive and cannot be applied to fast and scalable time-series hyper-parameter tuning (HPT). We propose a self-supervised learning framework for HPT (SSL-HPT), which uses time series features as inputs and produces optimal hyper-parameters. SSL-HPT algorithm is 6-20x faster at getting hyper-parameters compared to other search based algorithms while producing comparable accurate forecasting results in various applications.},
archivePrefix = {arXiv},
arxivId = {2102.05740},
author = {Zhang, Peiyi and Jiang, Xiaodong and Holt, Ginger M and Laptev, Nikolay Pavlovich and Komurlu, Caner and Gao, Peng and Yu, Yang},
eprint = {2102.05740},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2021 - Self-supervised learning for fast and scalable time series hyper-parameter tuning.pdf:pdf},
journal = {arXiv preprint arXiv:2102.05740},
keywords = {acm reference format,hyper-parameter tuning,learning,multi-task neural network,self-supervised,self-supervised learning,time series forecasting},
publisher = {Association for Computing Machinery},
title = {{Self-supervised learning for fast and scalable time series hyper-parameter tuning}},
url = {http://arxiv.org/pdf/2102.05740.pdf},
year = {2021}
}
@inproceedings{auer2007improved,
abstract = {Considering one-dimensional continuum-armed bandit problems, we propose an improvement of an algorithm of Kleinberg and a new set of conditions which give rise to improved rates. In particular, we introduce a novel assumption that is complementary to the previous smoothness conditions, while at the same time smoothness of the mean payoff function is required only at the maxima. Under these new assumptions new bounds on the expected regret are derived. In particular, we show that apart from logarithmic factors, the expected regret scales with the square-root of the number of trials, provided that the mean payoff function has finitely many maxima and its second derivatives are continuous and non-vanishing at the maxima. This improves a previous result of Cope by weakening the assumptions on the function. We also derive matching lower bounds. To complement the bounds on the expected regret, we provide high probability bounds which exhibit similar scaling.},
author = {Auer, Peter and Ortner, Ronald and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory (CoLT)},
doi = {10.1007/978-3-540-72927-3_33},
isbn = {978-3-540-72925-9},
issn = {03029743},
pages = {454--468},
title = {{Improved rates for the stochastic continuum-armed bandit problem}},
url = {http://dx.doi.org/10.1007/978-3-540-72927-3{\_}33{\%}5Cnhttp://www.springerlink.com/index/440117663L6X5X77.pdf},
volume = {4539},
year = {2007}
}
@inproceedings{shang2019dttts,
author = {Shang, Xuedong and Kaufmann, Emilie and Valko, Michal},
booktitle = {6th Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
title = {{A simple dynamic bandit algorithm for hyper-parameter tuning}},
url = {https://xuedong.github.io/static/documents/shang2019dttts.pdf},
year = {2019}
}
@inproceedings{liu2019tranfer,
abstract = {Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distributions of both domains. However, a theoretical prerequisite of domain adaptation is the adaptability measured by the expected risk of an ideal joint hypothesis over the source and target domains. In this respect, adversarial feature adaptation may potentially deteriorate the adaptability, since it distorts the original feature distributions when suppressing domain-specific variations. To this end, we propose Transferable Adversarial Training (TAT) to enable the adaptation of deep classifiers. The approach generates transferable examples to fill in the gap between the source and target domains, and adversarially trains the deep classifiers to make consistent predictions over the transferable examples. Without learning domain-invariant representations at the expense of distorting the feature distributions, the adaptability in the theoretical learning bound is algorithmically guaranteed. A series of experiments validate that our approach advances the state of the arts on a variety of domain adaptation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.},
author = {Liu, Hong and Long, Mingsheng and Wang, Jianmin and Jordan, Michael},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Transferable adversarial training A general approach to adapting deep classifiers.pdf:pdf},
pages = {4013--4022},
title = {{Transferable adversarial training: A general approach to adapting deep classifiers}},
url = {http://proceedings.mlr.press/v97/liu19b.html{\%}0Ahttps://github.com/thuml/Transferable-Adversarial-Training},
year = {2019}
}
@article{colson2007bilevel,
abstract = {This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present fields of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints).},
author = {Colson, Beno{\^{i}}t and Marcotte, Patrice and Savard, Gilles},
doi = {10.1007/s10479-007-0176-2},
isbn = {0254-5330},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Bilevel programming,Mathematical programs with equilibrium constraints,Nonlinear programming,Optimal pricing},
number = {1},
pages = {235--256},
pmid = {4472240},
title = {{An overview of bilevel optimization}},
url = {https://www.iro.umontreal.ca/{~}marcotte/ARTIPS/AOR2007.pdf},
volume = {153},
year = {2007}
}
@article{alquier2020nonexp,
abstract = {We tackle the problem of online optimization with a general, possibly unbounded, loss function. It is well known that the exponentially weighted aggregation strategy (EWA) leads to a regret in {\$}\backslashsqrt{\{}T{\}}{\$} after {\$}T{\$} steps, under the assumption that the loss is bounded. The online gradient algorithm (OGA) has a regret in {\$}\backslashsqrt{\{}T{\}}{\$} when the loss is convex and Lipschitz. In this paper, we study a generalized aggregation strategy, where the weights do no longer necessarily depend exponentially on the losses. Our strategy can be interpreted as the minimization of the expected losses plus a penalty term. When the penalty term is the Kullback-Leibler divergence, we obtain EWA as a special case, but using alternative divergences lead to a regret bounds for unbounded, not necessarily convex losses. However, the cost is a worst regret bound in some cases.},
archivePrefix = {arXiv},
arxivId = {2009.03017},
author = {Alquier, Pierre},
eprint = {2009.03017},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alquier - 2020 - Non-exponentially weighted aggregation Regret bounds for unbounded loss functions.pdf:pdf},
journal = {arXiv preprint arXiv:2009.03017},
pages = {1--23},
title = {{Non-exponentially weighted aggregation: Regret bounds for unbounded loss functions}},
url = {http://arxiv.org/pdf/2009.03017.pdf},
year = {2020}
}
@article{lam2017feature,
abstract = {Feature engineering is one of the most important and time consuming tasks in predictive analytics projects. It involves understanding domain knowledge and data exploration to discover relevant hand-crafted features from raw data. In this paper, we introduce a system called One Button Machine, or OneBM for short, which automates feature discovery in relational databases. OneBM automatically performs a key activity of data scientists, namely, joining of database tables and applying advanced data transformations to extract useful features from data. We validated OneBM in Kaggle competitions in which OneBM achieved performance as good as top 16{\%} to 24{\%} data scientists in three Kaggle competitions. More importantly, OneBM outperformed the state-of-the-art system in a Kaggle competition in terms of prediction accuracy and ranking on Kaggle leaderboard. The results show that OneBM can be useful for both data scientists and non-experts. It helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.},
archivePrefix = {arXiv},
arxivId = {1706.00327},
author = {Lam, Hoang Thanh and Thiebaut, Johann-Michael and Sinn, Mathieu and Chen, Bei and Mai, Tiep and Alkan, Oznur},
eprint = {1706.00327},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lam et al. - 2017 - One button machine for automating feature engineering in relational databases.pdf:pdf},
journal = {arXiv preprint arXiv:1706.00327},
title = {{One button machine for automating feature engineering in relational databases}},
url = {http://arxiv.org/pdf/1706.00327.pdf},
year = {2017}
}
@inproceedings{yu2006active,
abstract = {This paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y = f(x). We propose a novel and simple concept for active learning, transductive experimental design, that explores available unmeasured experiments (i.e.,unlabeled data) and has a better sealability in comparison with classic experimental design methods. Our in-depth analysis shows that the new method tends to favor experiments that are on the one side hard-to-predict and on the other side representative for the rest of the experiments. Efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search. Extensive experimental results on synthetic problems and three real-world tasks, including questionnaire design for preference learning, active learning for text categorization, and spatial sensor placement, highlight the advantages of the proposed approaches.},
author = {Yu, Kai and Bi, Jinbo and Tresp, Volker},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Bi, Tresp - 2006 - Active learning via transductive experimental design.pdf:pdf},
isbn = {1595933832},
pages = {1081--1088},
title = {{Active learning via transductive experimental design}},
url = {https://dl.acm.org/doi/pdf/10.1145/1143844.1143980?download=true},
year = {2006}
}
@inproceedings{xu2018linear,
abstract = {We propose the first fully-adaptive algorithm for pure exploration in linear bandits---the task to find the arm with the largest expected reward, which depends on an unknown parameter linearly. While existing methods partially or entirely fix sequences of arm selections before observing rewards, our method adaptively changes the arm selection strategy based on past observations at each round. We show our sample complexity matches the achievable lower bound up to a constant factor in an extreme case. Furthermore, we evaluate the performance of the methods by simulations based on both synthetic setting and real-world data, in which our method shows vast improvement over existing methods.},
author = {Xu, Liyuan and Honda, Junya and Sugiyama, Masashi},
booktitle = {Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Honda, Sugiyama - 2018 - A fully adaptive algorithm for pure exploration in linear bandits.pdf:pdf},
pages = {843--851},
title = {{A fully adaptive algorithm for pure exploration in linear bandits}},
url = {http://proceedings.mlr.press/v84/xu18d/xu18d.pdf},
year = {2018}
}
@inproceedings{xia2015budgeted,
abstract = {Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is {\$}O(\backslashln B){\$}, where {\$}B{\$} denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {1505.00146},
author = {Xia, Yingce and Li, Haifang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
booktitle = {Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)},
eprint = {1505.00146},
isbn = {9781577357384},
issn = {10450823},
month = {may},
pages = {3960--3966},
title = {{Thompson sampling for budgeted multi-armed Bandits}},
url = {http://arxiv.org/pdf/1505.00146.pdf},
year = {2015}
}
@article{kwon2017sparse,
abstract = {In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with sqrt(d) in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s {\textless} d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound - at least for a wide and pertinent range of parameters that we determine - and by evaluating its performance on simulated data.},
archivePrefix = {arXiv},
arxivId = {1706.01383},
author = {Kwon, Joon and Perchet, Vianney and Vernade, Claire},
eprint = {1706.01383},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwon, Perchet, Vernade - 2017 - Sparse stochastic bandits(2).pdf:pdf},
journal = {arXiv preprint arXiv:1706.01383},
title = {{Sparse stochastic bandits}},
url = {http://arxiv.org/pdf/1706.01383.pdf},
year = {2017}
}
@book{lattimore2018bandits,
abstract = {After nearly two years since starting to write the blog we have at last completed a first draft of the book, which is to be published by Cambridge University Press. The book is available for free as a PDF and will remain so after publication. We're grateful to Cambridge for allowing this. Without further ado, here is the link. Although we still have a few things we want to do, the manuscript is sufficiently polished to be useful. Of course we would greatly appreciate any comments you might have, including typos, errors in the proofs, missing references, confusing explanations or anything else you might notice. We will periodically update the book, so it would be helpful if you could quote the revision number on the cover when sending us your comments (banditalgs@gmail.com). The manuscript includes a lot of material not in the blog. The last seven chapters are all new, covering combinatorial (semi-)bandits, non-stationary bandits, ranking, pure exploration, Bayesian methods, Thompson sampling, partial monitoring and an introduction to learning in Markov decision processes. Those chapters that are based on blog posts have been cleaned up and often we have added significant depth. There is a lot of literature that we have not covered. Some of these missing topics are discussed in extreme brevity in the introduction to Part VII. It really is amazing how large the bandit literature has become and we're sorry not to have found space for everything. The book includes around 250 exercises, some of which have solutions. On average the exercises have been proofread less carefully than the rest of the book, so some caution is advised. The solutions to selected exercises are available here. Finally, we're very thankful for all the feedback already received, both on the blog and early drafts of the book.},
annote = {- Section 3.2: $\backslash$mu(.|x) instead of $\backslash$mu(x|.) according to the previous notation of probability kernel.
- Exercise 3.6.e: X{\_}m instead of X{\_}t.},
author = {Lattimore, Tor and Szepesvari, Csaba},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lattimore, Szepesvari - 2018 - Bandit Algorithms(2).pdf:pdf},
pages = {542},
publisher = {Cambridge University Press},
title = {{Bandit Algorithms}},
url = {http://downloads.tor-lattimore.com/book.pdf},
year = {2018}
}
@article{aziz2018infinite,
author = {Aziz, Maryam and Jamieson, Kevin and Aslam, Javed},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aziz, Jamieson, Aslam - 2018 - Pure-exploration for infinite-armed bandits with general arm reservoirs(2).pdf:pdf},
journal = {arXiv preprint arXiv:1811.06149},
title = {{Pure-exploration for infinite-armed bandits with general arm reservoirs}},
url = {https://arxiv.org/pdf/1811.06149.pdf},
year = {2018}
}
@article{frank1956algorithm,
author = {Frank, Marguerite and Wolfe, Philip},
journal = {Naval Research Logistics Quarterly},
number = {1-2},
pages = {95--110},
title = {{An algorithm for quadratic programming}},
url = {https://onlinelibrary.wiley.com/doi/epdf/10.1002/nav.3800030109},
volume = {3},
year = {1956}
}
@inproceedings{espeholt2018impala,
abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and ex- ; tended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses j resources more efficiently in single-machine train- ' ing but also scales to thousands of machines with- ; out sacrificing data efficiency or resource utilisation. We achieve stable learning at high through- ' put by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on • DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari- 57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better ; performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
archivePrefix = {arXiv},
arxivId = {1802.01561},
author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymyr and Ward, Tom and Yotam, Boron and Vlad, Firoiu and Tim, Harley and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1802.01561},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Espeholt et al. - 2018 - IMPALA Scalable distributed deep-RL with importance weighted actor-learner architectures.pdf:pdf},
isbn = {9781510867963},
pages = {2263--2284},
title = {{IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures}},
url = {https://arxiv.org/pdf/1802.01561.pdf},
year = {2018}
}
@inproceedings{grover2018,
abstract = {We propose a generalization of the best arm identification problem in stochastic multi-armed bandits (MAB) to the setting where every pull of an arm is associated with delayed feedback. The delay in feedback increases the effective sample complexity of standard algorithms, but can be offset if we have access to partial feedback received before a pull is completed. We propose a general framework to model the relationship between partial and delayed feedback, and as a special case we introduce efficient algorithms for settings where the partial feedback are biased or unbiased estimators of the delayed feedback. Additionally, we propose a novel extension of the algorithms to the parallel MAB setting where an agent can control a batch of arms. Our experiments in real-world settings, involving policy search and hyperparameter optimization in computational sustainability domains for fast charging of batteries and wildlife corridor construction, demonstrate that exploiting the structure of partial feedback can lead to significant improvements over baselines in both sequential and parallel MAB.},
archivePrefix = {arXiv},
arxivId = {1803.10937},
author = {Grover, Aditya and Markov, Todor and Attia, Peter and Jin, Norman and Perkins, Nicholas and Cheong, Bryan and Chen, Michael and Yang, Zi and Harris, Stephen and Chueh, William and Ermon, Stefano},
booktitle = {Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1803.10937},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grover et al. - 2018 - Best arm identification in multi-armed bandits with delayed feedback.pdf:pdf},
pages = {833--842},
title = {{Best arm identification in multi-armed bandits with delayed feedback}},
url = {https://arxiv.org/pdf/1803.10937.pdf},
volume = {84},
year = {2018}
}
@article{yao2018automl,
abstract = {Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, human experts are heavily involved in every aspect of machine learning. In order to make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. Finally, we provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.},
archivePrefix = {arXiv},
arxivId = {1810.13306},
author = {Yao, Quanming and Wang, Mengshuo and Chen, Yuqiang and Dai, Wenyuan and Li, Yu-Feng and Tu, Wei-Wei and Yang, Qiang and Yu, Yang},
eprint = {1810.13306},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2018 - Taking Human out of Learning Applications A Survey on Automated Machine Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1810.13306},
title = {{Taking human out of learning applications: A survey on automated machine learning}},
url = {https://arxiv.org/pdf/1810.13306.pdf},
year = {2018}
}
@incollection{bertsekas2011approximate,
abstract = {This approach at deriving control strategies is successful under the assumption that the system dynamics can be modeled and are not changing over time. Optimal controllers fall in this category as they are generally determined considering a fixed given model of the system. The goal of an optimal control strategy is the minimization of a cost index, which reflects the amount of energy used for control purposes and the distance between the present and desired performance of the controlled system. Though they have good robustness properties relative to possible changes in the system dynamics, optimal controllers are neither adaptive nor are they determined considering possible unmodeled dynamics. From this perspective one can generally say that an optimal controller is just as close to optimality as the model of the system, used during the design phase, is close to the real plant to be controlled.},
author = {Bertsekas, Dimitri P.},
booktitle = {Dynamic Programming and Optimal Control},
doi = {10.1201/b10384},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsekas - 2011 - Approximate dynamic programming.pdf:pdf},
isbn = {9781420073652},
pages = {327--552},
title = {{Approximate dynamic programming}},
url = {https://web.mit.edu/dimitrib/www/dpchapter.pdf},
volume = {II},
year = {2011}
}
@inproceedings{locatello2019desentangled,
abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look on recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties `encouraged' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
archivePrefix = {arXiv},
arxivId = {1811.12359},
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"{a}}tsch, Gunnar and Gelly, Sylvain and Sch{\"{o}}lkopf, Bernhard and Bachem, Olivier},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1811.12359},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locatello et al. - 2019 - Challenging common assumptions in the unsupervised learning of disentangled representations.pdf:pdf},
title = {{Challenging common assumptions in the unsupervised learning of disentangled representations}},
url = {http://arxiv.org/pdf/1811.12359.pdf},
year = {2019}
}
@inproceedings{samothrakis2013,
author = {Samothrakis, Spyridon and Perez, Diego and Lucas, Simon},
booktitle = {Workshop on Causality at Neural Information Processing Systems (NIPS-Causality)},
title = {{Training gradient boosting machines using curve-fitting and information-theoretic features for causal direction detection}},
url = {http://ssamot.me/papers/Samothrakis-NIPS2013-causality.pdf},
year = {2013}
}
@inproceedings{dai2019bo,
author = {Dai, Zhongxiang and Yu, Haibin and Kian, Bryan and Low, Hsiang and Jaillet, Patrick},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2019 - Bayesian optimization meets Bayesian optimal stopping.pdf:pdf},
title = {{Bayesian optimization meets Bayesian optimal stopping}},
url = {http://proceedings.mlr.press/v97/dai19a/dai19a.pdf},
year = {2019}
}
@article{pedregosa2011sklearn,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine learning in Python}},
url = {http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
volume = {12},
year = {2011}
}
@inproceedings{Cavazza2018,
abstract = {Regularization for matrix factorization (MF) and approximation problems has been carried out in many different ways. Due to its popularity in deep learning, dropout has been applied also for this class of problems. Despite its solid empirical performance, the theoretical properties of dropout as a regularizer remain quite elusive for this class of problems. In this paper, we present a theoretical analysis of dropout for MF, where Bernoulli random variables are used to drop columns of the factors. We demonstrate the equivalence between dropout and a fully deterministic model for MF in which the factors are regularized by the sum of the product of squared Euclidean norms of the columns. Additionally, we inspect the case of a variable sized factorization and we prove that dropout achieves the global minimum of a convex approximation problem with (squared) nuclear norm regularization. As a result, we conclude that dropout can be used as a low-rank regularizer with data dependent singular-value thresholding.},
archivePrefix = {arXiv},
arxivId = {1710.05092},
author = {Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Ren{\'{e}}},
booktitle = {Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1710.05092},
pages = {435--444},
title = {{Dropout as a low-rank regularizer for matrix factorization}},
url = {http://arxiv.org/pdf/1710.05092.pdf},
year = {2018}
}
@article{jedra2020linear,
archivePrefix = {arXiv},
arxivId = {arXiv:2006.16073v1},
author = {Jedra, Yassir and Prouti{\`{e}}re, Alexandre},
eprint = {arXiv:2006.16073v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jedra, Prouti{\`{e}}re - 2020 - Optimal best-arm identification in linear bandits.pdf:pdf},
journal = {arXiv preprint arXiv:2006.16073},
title = {{Optimal best-arm identification in linear bandits}},
url = {https://arxiv.org/pdf/2006.16073.pdf},
year = {2020}
}
@inproceedings{horgan2018distributed,
abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: The actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
archivePrefix = {arXiv},
arxivId = {1803.00933},
author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and {Van Hasselt}, Hado and Silver, David},
booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR)},
eprint = {1803.00933},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horgan et al. - 2018 - Distributed prioritized experience replay(2).pdf:pdf},
title = {{Distributed prioritized experience replay}},
url = {https://arxiv.org/pdf/1803.00933.pdf},
year = {2018}
}
@article{schmidt2019differential,
abstract = {Automated hyperparameter tuning aspires to facilitate the application of machine learning for non-experts. In the literature, different optimization approaches are applied for that purpose. This paper investigates the performance of Differential Evolution for tuning hyperparameters of supervised learning algorithms for classification tasks. This empirical study involves a range of different machine learning algorithms and datasets with various characteristics to compare the performance of Differential Evolution with Sequential Model-based Algorithm Configuration (SMAC), a reference Bayesian Optimization approach. The results indicate that Differential Evolution outperforms SMAC for most datasets when tuning a given machine learning algorithm - particularly when breaking ties in a first-to-report fashion. Only for the tightest of computational budgets SMAC performs better. On small datasets, Differential Evolution outperforms SMAC by 19{\%} (37{\%} after tie-breaking). In a second experiment across a range of representative datasets taken from the literature, Differential Evolution scores 15{\%} (23{\%} after tie-breaking) more wins than SMAC.},
archivePrefix = {arXiv},
arxivId = {1904.06960},
author = {Schmidt, Mischa and Safarani, Shahd and Gastinger, Julia and Jacobs, Tobias and Nicolas, Sebastien and Sch{\"{u}}lke, Anett},
eprint = {1904.06960},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt et al. - 2019 - On the Performance of Differential Evolution for Hyperparameter Tuning.pdf:pdf},
journal = {arXiv preprint arXiv:1904.06960},
title = {{On the performance of differential evolution for hyperparameter tuning}},
url = {http://arxiv.org/pdf/1904.06960.pdf},
year = {2019}
}
@unpublished{allesiardo2017nonstationary,
author = {Allesiardo, Robin and F{\'{e}}raud, Rapha{\"{e}}l and Maillard, Odalric-Ambrym},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Allesiardo, F{\'{e}}raud, Maillard - 2017 - The non-stationary stochastic multi-armed bandit problem(2).pdf:pdf},
title = {{The non-stationary stochastic multi-armed bandit problem}},
url = {https://hal.archives-ouvertes.fr/hal-01575000/document},
year = {2017}
}
@inproceedings{katz2016feature,
abstract = {—Feature generation is one of the challenging aspects of machine learning. We present ExploreKit, a framework for automated feature generation. ExploreKit generates a large set of candidate features by combining information in the original features, with the aim of maximizing predictive performance according to user-selected criteria. To overcome the exponential growth of the feature space, ExploreKit uses a novel ma-chine learning-based feature selection approach to predict the usefulness of new candidate features. This approach enables efficient identification of the new features and produces superior results compared to existing feature selection solutions. We demonstrate the effectiveness and robustness of our approach by conducting an extensive evaluation on 25 datasets and 3 different classification algorithms. We show that ExploreKit can achieve classification-error reduction of 20{\%} overall. Our code is available at https://github.com/giladkatz/ExploreKit.},
author = {Katz, Gilad and Shin, Eui Chul Richard and Song, Dawn},
booktitle = {Proceedings of the 16th IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/icdm.2016.0123},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz, Shin, Song - 2016 - ExploreKit Automatic feature generation and selection.pdf:pdf},
pages = {979--984},
title = {{ExploreKit: Automatic feature generation and selection}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=7837936},
year = {2016}
}
@book{gentle2009,
abstract = {The field of computational statistics is a major subdiscipline of statistics that includes two distinct but overlapping areas of study. The first, which often goes by the name statistical computing, covers numerical, graphical, and data methods to implement statistical methods. Statistical computing includes numerical analysis and database techniques for applications in statistics. The second major area of computational statistics includes statistical methods that are computationally intensive. Many of the more recently developed methods of statistics are included in the field of computational statistics. These statistical methods may make use of resampling and simulation or they may involve unsupervised exploration of massive data. Many of the methods of computational statistics are nonparametric or are less dependent on distributional assumptions than are more standard statistical methods. These methods, therefore, are often used in educational research because many of the variables of interest cannot be assumed to have some specific distribution. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Gentle, James E.},
doi = {10.1016/B978-0-08-044894-7.01316-6},
isbn = {9780080448947},
keywords = {Bayesian methods,Bootstrap,Data mining,Machine learning,Markov chain monte carlo,Monte carlo,Numerical analysis,Statistical computing,Statistical learning},
publisher = {Springer-Verlag New York},
title = {{Computational Statistics}},
url = {https://link.springer.com/book/10.1007{\%}2F978-0-387-98144-4},
year = {2009}
}
@article{lu2019hpo,
abstract = {We study a budgeted hyper-parameter tuning problem, where we optimize the tuning result under a hard resource constraint. We propose to solve it as a sequential decision making problem, such that we can use the partial training progress of configurations to dynamically allocate the remaining budget. Our algorithm combines a Bayesian belief model which estimates the future performance of configurations, with an action-value function which balances exploration-exploitation tradeoff, to optimize the final output. It automatically adapts the tuning behaviors to different constraints, which is useful in practice. Experiment results demonstrate superior performance over existing algorithms, including the-state-of-the-art one, on real-world tuning tasks across a range of different budgets.},
archivePrefix = {arXiv},
arxivId = {1902.00532},
author = {Lu, Zhiyun and Chiang, Chao-Kai and Sha, Fei},
eprint = {1902.00532},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Chiang, Sha - 2019 - Hyper-parameter tuning under a budget constraint.pdf:pdf},
journal = {arXiv preprint arXiv:1902.00532},
title = {{Hyper-parameter tuning under a budget constraint}},
url = {http://arxiv.org/pdf/1902.00532.pdf},
year = {2019}
}
@inproceedings{chaudhuri2017,
author = {Chaudhuri, Arghya Roy and Kalyanakrishnan, Shivaram},
booktitle = {Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)},
keywords = {Machine Learning Methods},
pages = {1777--1783},
title = {{PAC identification of a bandit arm relative to a reward quantile}},
url = {https://www.cse.iitb.ac.in/{~}shivaram/papers/rk{\_}aaai{\_}2017.pdf},
year = {2017}
}
@inproceedings{schulman2015trpo,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
isbn = {0375-9687},
issn = {2158-3226},
pages = {1889--1897},
title = {{Trust region policy optimization}},
url = {http://arxiv.org/pdf/1502.05477.pdf},
year = {2015}
}
@article{levine2018,
abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {1805.00909},
author = {Levine, Sergey},
eprint = {1805.00909},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine - 2018 - Reinforcement learning and control as probabilistic inference Tutorial and review.pdf:pdf},
journal = {arXiv preprint arXiv:1805.00909},
title = {{Reinforcement learning and control as probabilistic inference: Tutorial and review}},
url = {https://arxiv.org/pdf/1805.00909.pdf},
year = {2018}
}
@inproceedings{sidford2018variance,
abstract = {In this paper we provide faster algorithms for approximately solving discounted Markov Decision Processes in multiple parameter regimes. Given a discounted Markov Decision Process (DMDP) with |S| states, |A| actions, discount factor $\gamma$ ∈ (0, 1), and rewards in the range [-M,M], we show how to compute an ϵ-optimal policy, with probability 1 - $\delta$ in time {\~{O}}((|S|2|A| + |S||A|/(1 - $\gamma$)3) log (M/ϵ) log (1/$\delta$)). This contribution reflects the first nearly linear time, nearly linearly convergent algorithm for solving DMDPs for intermediate values of $\gamma$. We also show how to obtain improved sublinear time algorithms provided we can sample from the transition function in O(1) time. Under this assumption we provide an algorithm which computes an ϵ-optimal policy with probability 1 - $\delta$ in time {\~{O}}(|S||A|M2/(1 - $\gamma$)4ϵ2log (1/$\delta$)). Lastly, we extend both these algorithms to solve finite horizon MDPs. Our algorithms improve upon the previous best for approximately computing optimal policies for fixed-horizon MDPs in multiple parameter regimes. Interestingly, we obtain our results by a careful modification of approximate value iteration. We show how to combine classic approximate value iteration analysis with new techniques in variance reduction. Our fastest algorithms leverage further insights to ensure that our algorithms make monotonic progress towards the optimal value. This paper is one of few instances in using sampling to obtain a linearly convergent linear programming algorithm and we hope that the analysis may be useful more broadly.},
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
booktitle = {Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
issn = {23318422},
title = {{Variance reduced value iteration and faster algorithms for solving Markov decision processes}},
url = {https://arxiv.org/pdf/1710.09988.pdf},
year = {2018}
}
@inproceedings{loshchilov2016cmaes,
abstract = {Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy example comparing CMA-ES and state-of-the-art Bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel.},
archivePrefix = {arXiv},
arxivId = {1604.07269},
author = {Loshchilov, Ilya and Hutter, Frank},
booktitle = {Workshop Track of the 4th International Conference on Learning Representations},
eprint = {1604.07269},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loshchilov, Hutter - 2016 - CMA-ES for hyperparameter optimization of deep neural networks(2).pdf:pdf},
title = {{CMA-ES for hyperparameter optimization of deep neural networks}},
url = {http://arxiv.org/pdf/1604.07269.pdf},
year = {2016}
}
@inproceedings{bonald2013bernoulli,
author = {Bonald, Thomas and Prouti{\`{e}}re, Alexandre},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
issn = {10495258},
pages = {2184--2192},
title = {{Two-target algorithms for infinite-armed bandits with Bernoulli rewards}},
url = {https://papers.nips.cc/paper/5109-two-target-algorithms-for-infinite-armed-bandits-with-bernoulli-rewards.pdf},
year = {2013}
}
@inproceedings{mellor2013nonstationary,
abstract = {Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.},
archivePrefix = {arXiv},
arxivId = {1302.3721},
author = {Mellor, Joseph and Shapiro, Jonathan},
booktitle = {Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1302.3721},
issn = {15337928},
pages = {442--450},
title = {{Thompson sampling in switching environments with Bayesian online change point detection}},
url = {http://arxiv.org/pdf/1302.3721.pdf},
year = {2013}
}
@article{Author2020d,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - Recycling sub-optimal hyperparameter optimization models to generate efficient ensemble deep learning.pdf:pdf},
isbn = {8176260770},
title = {{Recycling sub-optimal hyperparameter optimization models to generate efficient ensemble deep learning}},
year = {2020}
}
@article{scarlett2018,
abstract = {We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time {\$}T{\$} behaves as {\$}\backslashOmega(\backslashsqrt{\{}T{\}}){\$} and {\$}O(\backslashsqrt{\{}T\backslashlog T{\}}){\$}. This gives a tight characterization up to a {\$}\backslashsqrt{\{}\backslashlog T{\}}{\$} factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat$\backslash$'ern-{\$}\backslashnu{\$} kernels, with the latter requiring {\$}\backslashnu {\textgreater} 2{\$}. Our results certify the near-optimality of existing bounds (Srinivas {\{}$\backslash$em et al.{\}}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat$\backslash$'ern kernel with {\$}\backslashnu {\textgreater} 2{\$}.},
archivePrefix = {arXiv},
arxivId = {1805.11792},
author = {Scarlett, Jonathan},
eprint = {1805.11792},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scarlett - 2018 - Tight regret bounds for Bayesian optimization in one dimension.pdf:pdf},
journal = {arXiv preprint arXiv:1805.11792},
title = {{Tight regret bounds for Bayesian optimization in one dimension}},
url = {http://arxiv.org/pdf/1805.11792.pdf},
year = {2018}
}
@article{teraoka2014mc,
abstract = {We consider Monte Carlo tree search problem, a variant of Min-Max tree search problem where the score of each leaf is the expectation of some Bernoulli variables and not explicitly given but can be estimated through (random) playouts. The goal of this problem is, given a game tree and an oracle that returns an outcome of a playout, to find a child node of the root which attains an approximate min-max score. This problem arises in two player games such as computer Go. We propose a simple and efficient algorithm for Monte Carlo tree search problem. Copyright {\textcopyright} 2014 The Institute of Electronics, Information and Communication Engineers.},
author = {Teraoka, Kazuki and Hatano, Kohei and Takimoto, Eiji},
doi = {10.1587/transinf.E97.D.392},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Game,Monte Carlo tree search,Random sampling,UCT},
number = {3},
pages = {392--398},
title = {{Efficient sampling method for monte carlo tree search problem}},
url = {https://pdfs.semanticscholar.org/30c9/85d7eb0deb9e40fa63ec5a7df818efc85952.pdf},
volume = {E97-D},
year = {2014}
}
@inproceedings{bellemare2016count,
abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
archivePrefix = {arXiv},
arxivId = {1606.01868},
author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
eprint = {1606.01868},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare et al. - 2016 - Unifying count-based exploration and intrinsic motivation.pdf:pdf},
issn = {10495258},
pages = {1479--1487},
title = {{Unifying count-based exploration and intrinsic motivation}},
url = {https://arxiv.org/pdf/1606.01868.pdf},
year = {2016}
}
@inproceedings{kandasamy2018parallel,
author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and P{\'{o}}czos, Barnab{\'{a}}s},
booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AIStats)},
pages = {133--142},
title = {{Parallelised Bayesian optimisation via Thompson sampling}},
url = {http://proceedings.mlr.press/v84/kandasamy18a/kandasamy18a.pdf},
year = {2018}
}
@article{haarnoja2018sac,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Soft actor-critic algorithms and applications.pdf:pdf},
issn = {23318422},
journal = {arXiv preprint arXiv:1105.1812.05905},
title = {{Soft actor-critic algorithms and applications}},
url = {https://arxiv.org/pdf/1812.05905.pdf},
year = {2018}
}
@book{hutter2019automl,
author = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter, Kotthoff, Vanschoren - 2019 - Automated Machine Learning Methods, Systems, Challenges(2).pdf:pdf},
publisher = {Springer-Verlag},
title = {{Automated Machine Learning: Methods, Systems, Challenges}},
url = {https://www.automl.org/wp-content/uploads/2019/04/automl{\_}book.pdf},
year = {2019}
}
@inproceedings{carpentier2015siri,
abstract = {We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter {\$}\backslashbeta{\$} characterizing the distribution of the near-optimal arms. We prove that depending on {\$}\backslashbeta{\$}, our algorithm is minimax optimal either up to a multiplicative constant or up to a {\$}\backslashlog(n){\$} factor. We also provide extensions to several important cases: when {\$}\backslashbeta{\$} is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.},
archivePrefix = {arXiv},
arxivId = {1505.04627},
author = {Carpentier, Alexandra and Valko, Michal},
booktitle = {Proceedings of the 32nd International conference on Machine Learning (ICML)},
eprint = {1505.04627},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carpentier, Valko - 2015 - Simple regret for infinitely many armed bandits(2).pdf:pdf},
isbn = {9781510810587},
pages = {1133--1141},
title = {{Simple regret for infinitely many armed bandits}},
url = {http://arxiv.org/pdf/1505.04627.pdf},
year = {2015}
}
@article{sitzmann2020sine,
abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
archivePrefix = {arXiv},
arxivId = {2006.09661},
author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
eprint = {2006.09661},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sitzmann et al. - 2020 - Implicit neural representations with periodic activation functions.pdf:pdf},
journal = {arXiv preprint arXiv:2006.09661},
title = {{Implicit neural representations with periodic activation functions}},
url = {http://arxiv.org/pdf/2006.09661.pdf},
year = {2020}
}
@article{jaderberg2017pbt,
abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present $\backslash$emph{\{}Population Based Training (PBT){\}}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
archivePrefix = {arXiv},
arxivId = {1711.09846},
author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
eprint = {1711.09846},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2017 - Population based training of neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.09846},
title = {{Population based training of neural networks}},
url = {http://arxiv.org/pdf/1711.09846.pdf},
year = {2017}
}
@article{banks1992,
author = {Banks, Jeffrey S. and Sundaram, Rangarajan K.},
journal = {Econometrica},
number = {5},
pages = {1071--1096},
title = {{Denumerable-armed bandits}},
url = {https://authors.library.caltech.edu/67326/1/2951539.pdf},
volume = {60},
year = {1992}
}
@article{heckel2019,
abstract = {We consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of prespecified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items.We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does depend on whether or not the underlying pairwise probability matrix, satisfies a particular structural property, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question as to whether or not imposing these parametric assumptions allows for improved ranking algorithms. For stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. This shows, perhaps surprisingly, that these popular parametric modeling choices offer at most logarithmic gains for stochastic comparisons.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.08842v2},
author = {Heckel, Reinhard and Shah, Nihar B. and Ramchandran, Kannan and Wainwright, Martin J.},
doi = {10.1214/18-AOS1774},
eprint = {arXiv:1606.08842v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckel et al. - 2019 - Active ranking from pairwise comparisons and when parametric assumptions do not help.pdf:pdf},
issn = {21688966},
journal = {Annals of Statistics},
keywords = {Active learning,Bradley-Terry-Luce model,Multiarmed bandits,Online learning,Pairwise comparisons,Ranking},
number = {6},
pages = {3099--3126},
title = {{Active ranking from pairwise comparisons and when parametric assumptions do not help}},
url = {https://arxiv.org/pdf/1606.08842.pdf},
volume = {47},
year = {2019}
}
@article{mnih2015drl,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning(2).pdf:pdf},
isbn = {9783030042387},
issn = {16113349},
journal = {Nature},
keywords = {Algorithms,Artificial Intelligence,Humans,Psychological,Reinforcement (Psychology,Reward,Video Games},
number = {7540},
pages = {529},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html},
volume = {518},
year = {2015}
}
@article{lai1985,
abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {\textless}e1{\textgreater}n{\textless}/e1{\textgreater}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated},
author = {Lai, Tze-Leung and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
isbn = {0196-8858},
issn = {10902074},
journal = {Advances in Applied Mathematics},
number = {1},
pages = {4--22},
title = {{Asymptotically efficient adaptive allocation rules}},
url = {http://www.rci.rutgers.edu/{~}mnk/papers/Lai{\_}robbins85.pdf},
volume = {6},
year = {1985}
}
@article{kang2018,
author = {Kang, Qiyu and Tay, Wee Peng},
journal = {arXiv preprint arXiv:1807.10444},
title = {{Task recommendation in crowdsourcing based on learning preferences and reliabilities}},
url = {https://arxiv.org/pdf/1807.10444.pdf},
year = {2018}
}
@article{komiyama2018cherry,
abstract = {Statistical hypothesis testing serves as statistical evidence for scientific innovation. However, if the reported results are intentionally biased, hypothesis testing no longer controls the rate of false discovery. In particular , we study such selection bias in machine learning models where the reporter is motivated to promote an algorithmic innovation. When the number of possible configurations (e.g., datasets) is large, we show that the reporter can falsely report an innovation even if there is no improvement at all. We propose a "post-reporting" solution to this issue where the bias of the reported results is verified by another set of results. The theoretical findings are supported by experimental results with synthetic and real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1810.04996v1},
author = {Komiyama, Junpei and Maehara, Takanori},
eprint = {1810.04996v1},
journal = {arXiv preprint arXiv:1810.04996},
title = {{A simple way to deal with cherry-picking}},
url = {https://arxiv.org/pdf/1810.04996.pdf},
year = {2018}
}
@inproceedings{zhou2017nash,
abstract = {We study the problem on how to learn the pure Nash Equilibrium of a two-player zero-sum static game with random payoffs under unknown distributions via efficient payoff queries. We introduce a multi-armed bandit model to this problem due to its ability to find the best arm efficiently among random arms and propose two algorithms for this problem-LUCB-G based on the confidence bounds and a racing algorithm based on successive action elimination. We provide an analysis on the sample complexity lower bound when the Nash Equilibrium exists.},
author = {Zhou, Yichi and Li, Jialian and Zhu, Jun},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
isbn = {9781510855144},
pages = {6297--6310},
title = {{Identify the Nash equilibrium in static games with random payoffs}},
url = {http://proceedings.mlr.press/v70/zhou17b/zhou17b.pdf},
volume = {8},
year = {2017}
}
@article{wang2019pucb,
abstract = {Stochastic bandit algorithms can be used for challenging non-convex optimization problems. Hyperparameter tuning of neural networks is particularly challenging, necessitating new approaches. To this end, we present a method that adaptively partitions the combined space of hyperparameters, context, and training resources (e.g., total number of training iterations). By adaptively partitioning the space, the algorithm is able to focus on the portions of the hyperparameter search space that are most relevant in a practical way. By including the resources in the combined space, the method tends to use fewer training resources overall. Our experiments show that this method can surpass state-of-the-art methods in tuning neural networks on benchmark datasets. In some cases, our implementations can achieve the same levels of accuracy on benchmark datasets as existing state-of-the-art approaches while saving over 50{\%} of our computational resources (e.g. time, training iterations).},
archivePrefix = {arXiv},
arxivId = {1901.09277},
author = {Wang, Tianyu and Geng, Dawei and Rudin, Cynthia},
eprint = {1901.09277},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Geng, Rudin - 2019 - A practical bandit method with advantages in neural network tuning(2).pdf:pdf},
journal = {arXiv preprint arXiv:1901.09277},
title = {{A practical bandit method with advantages in neural network tuning}},
url = {http://arxiv.org/pdf/1901.09277.pdf},
year = {2019}
}
@inproceedings{kawaguchi2015bo,
abstract = {This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.},
archivePrefix = {arXiv},
arxivId = {1604.01348},
author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Lozano-P{\'{e}}rez, Tom{\'{a}}s},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
eprint = {1604.01348},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawaguchi, Kaelbling, Lozano-P{\'{e}}rez - 2015 - Bayesian optimization with exponential convergence.pdf:pdf},
pages = {2809--2817},
title = {{Bayesian optimization with exponential convergence}},
url = {http://arxiv.org/pdf/1604.01348.pdf},
year = {2015}
}
@article{schumann2019group,
abstract = {We propose a novel formulation of group fairness in the contextual multi-armed bandit (CMAB) setting. In the CMAB setting a sequential decision maker must at each time step choose an arm to pull from a finite set of arms after observing some context for each of the potential arm pulls. In our model arms are partitioned into two or more sensitive groups based on some protected feature (e.g., age, race, or socio-economic status). Despite the fact that there may be differences in expected payout between the groups, we may wish to ensure some form of fairness between picking arms from the various groups. In this work we explore two definitions of fairness: equal group probability, wherein the probability of pulling an arm from any of the protected groups is the same; and proportional parity, wherein the probability of choosing an arm from a particular group is proportional to the size of that group. We provide a novel algorithm that can accommodate these notions of fairness for an arbitrary number of groups, and provide bounds on the regret for our algorithm. We then validate our algorithm using synthetic data as well as two real-world datasets for intervention settings wherein we want to allocate resources fairly across protected groups.},
archivePrefix = {arXiv},
arxivId = {1912.03802},
author = {Schumann, Candice and Lang, Zhi and Mattei, Nicholas and Dickerson, John P.},
eprint = {1912.03802},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schumann et al. - 2019 - Group fairness in bandit arm selection.pdf:pdf},
journal = {arXiv preprint arXiv:1912.03802},
title = {{Group fairness in bandit arm selection}},
url = {http://arxiv.org/pdf/1912.03802.pdf},
year = {2019}
}
@book{szeliski2011,
abstract = {Na.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Szeliski, Richard},
booktitle = {Springer Science {\&} Business Media},
doi = {10.1007/978-1-84882-256-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-84882-255-9},
issn = {02569574},
pages = {823},
pmid = {20529422},
title = {{Computer Vision Algorithms and Applications}},
url = {http://link.springer.com/10.1007/978-1-84882-256-6},
volume = {42},
year = {2011}
}
@article{ecoffet2019goexplore,
abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to encourage exploration and improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember states that have previously been visited, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning. The combined effect of these principles generates dramatic performance improvements on hard-exploration problems. On Montezuma's Revenge, without being provided any domain knowledge, Go-Explore scores over 43,000 points, almost 4 times the previous state of the art. Go-Explore can also easily harness human-provided domain knowledge, and when augmented with it Go-Explore scores a mean of over 650,000 points on Montezuma's Revenge. Its max performance of 18 million surpasses the human world record by an order of magnitude, thus meeting even the strictest definition of “superhuman” performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean performance of almost 60,000 points also exceeds expert human performance. Because Go-Explore can produce many high-performing demonstrations automatically and cheaply, it also outperforms previous imitation learning work in which the solution was provided in the form of a human demonstration. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in a variety of domains, especially the many that often harness a simulator during training (e.g. robotics).},
archivePrefix = {arXiv},
arxivId = {1901.10995},
author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1901.10995},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ecoffet et al. - 2019 - Go-Explore A new approach for hard-exploration problems.pdf:pdf},
journal = {arXiv preprint arXiv:1901.10995},
title = {{Go-Explore: A new approach for hard-exploration problems}},
url = {https://arxiv.org/pdf/1901.10995.pdf},
year = {2019}
}
@inproceedings{valko2014spectral,
abstract = {Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each recommended item is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens nodes evaluations.},
author = {Valko, Michal and Munos, R{\'{e}}mi and Kveton, Branislav and Koc{\'{a}}k, Tom{\'{a}}{\v{s}}},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valko et al. - 2014 - Spectral bandits for smooth graph functions.pdf:pdf},
isbn = {9781577356738},
title = {{Spectral bandits for smooth graph functions}},
url = {http://proceedings.mlr.press/v32/valko14.pdf},
year = {2014}
}
@article{strehl2008count,
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Strehl, Alexander L. and Littman, Michael L.},
doi = {10.1016/j.jcss.2007.08.009},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {Learning theory,Markov Decision Processes,Reinforcement learning},
number = {8},
pages = {1309--1331},
title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
volume = {74},
year = {2008}
}
@inproceedings{devlin2019bert,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
eprint = {1810.04805},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of deep bidirectional transformers for language understanding.pdf:pdf},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
url = {http://arxiv.org/pdf/1810.04805.pdf},
year = {2019}
}
@inproceedings{agrawal2019mdp,
abstract = {We consider a stochastic inventory control problem under censored demands, lost sales, and positive lead times. This is a fundamental problem in inventory management, with significant literature establishing near-optimality of a simple class of policies called ``base-stock policies'' for the underlying Markov Decision Process (MDP), as well as convexity of long run average-cost under those policies. We consider the relatively less studied problem of designing a learning algorithm for this problem when the underlying demand distribution is unknown. The goal is to bound regret of the algorithm when compared to the best base-stock policy. We utilize the convexity properties and a newly derived bound on bias of base-stock policies to establish a connection to stochastic convex bandit optimization. Our main contribution is a learning algorithm with a regret bound of {\$}\backslashtilde{\{}O{\}}(L\backslashsqrt{\{}T{\}}+D){\$} for the inventory control problem. Here {\$}L{\$} is the fixed and known lead time, and {\$}D{\$} is an unknown parameter of the demand distribution described roughly as the number of time steps needed to generate enough demand for depleting one unit of inventory. Notably, even though the state space of the underlying MDP is continuous and {\$}L{\$}-dimensional, our regret bounds depend linearly on {\$}L{\$}. Our results significantly improve the previously best known regret bounds for this problem where the dependence on {\$}L{\$} was exponential and many further assumptions on demand distribution were required. The techniques presented here may be of independent interest for other settings that involve large structured MDPs but with convex cost functions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.04337v1},
author = {Agrawal, Shipra and Jia, Randy},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation (EC)},
doi = {10.1145/3328526.3329565},
eprint = {arXiv:1905.04337v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Jia - 2019 - Learning in structured MDPs with convex cost functions Improved regret bounds for inventory management.pdf:pdf},
isbn = {9781450367929},
keywords = {Censored demand,Exploration-exploitation,Inventory control problem},
pages = {743--744},
title = {{Learning in structured MDPs with convex cost functions: Improved regret bounds for inventory management}},
url = {https://arxiv.org/pdf/1905.04337.pdf},
year = {2019}
}
@inproceedings{falkner2017,
abstract = {Proper hyperparameter optimization is computationally very costly for expensive machine learning methods, such as deep neural networks; the same holds true for neural architecture search. Recently, the bandit-based strategy Hyperband has shown superior performance to vanilla Bayesian optimization methods that are limited to the traditional problem formulation of expensive blackbox optimization. However, while Hyperband has strong anytime performance for finding configura-tions with acceptable results, it relies on random search and therefore does not find the best configurations quickly. We propose to combine Hyperband with Bayesian optimization by maintaining a probabilistic model that captures the density of good configurations in the input space and samples from this model instead of sampling uniformly at random. We empirically show that our new method combines Hy-perband's strong anytime performance with the strong eventual performance of Bayesian optimization.},
author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
booktitle = {7th Workshop on Bayesian Optimization at Neural Information Processing Systems (NIPS-BayesOpt)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falkner, Klein, Hutter - 2017 - Combining Hyperband and Bayesian optimization(2).pdf:pdf},
title = {{Combining Hyperband and Bayesian optimization}},
url = {http://ml.informatik.uni-freiburg.de/papers/17-BayesOpt-BOHB.pdf},
year = {2017}
}
@inproceedings{franceschi2018bilevel,
abstract = {We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.},
archivePrefix = {arXiv},
arxivId = {1806.04910},
author = {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Pontil, Massimilano},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1806.04910},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franceschi et al. - 2018 - Bilevel programming for hyperparameter optimization and meta-learning(2).pdf:pdf},
issn = {1938-7228},
title = {{Bilevel programming for hyperparameter optimization and meta-learning}},
url = {http://arxiv.org/pdf/1806.04910.pdf},
year = {2018}
}
@article{hinton2006reduction,
author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
doi = {10.1021/ac071891x},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {0003-2700},
journal = {Science},
number = {5786},
pages = {504--507},
title = {{Reducing the dimensionality of data with neural networks}},
url = {https://www.cs.toronto.edu/{~}hinton/science.pdf},
volume = {313},
year = {2006}
}
@article{zoller2019automl,
abstract = {Machine learning has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to automatically build machine learning applications without extensive knowledge of statistics and machine learning. In this survey, we summarize the recent developments in academy and industry regarding AutoML. First, we introduce a holistic problem formulation. Next, approaches for solving various subproblems of AutoML are presented. Finally, we provide an extensive empirical evaluation of the presented approaches on synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {1904.12054},
author = {Z{\"{o}}ller, Marc-Andr{\'{e}} and Huber, Marco F.},
eprint = {1904.12054},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Z{\"{o}}ller, Huber - 2019 - Survey on automated machine learning.pdf:pdf},
journal = {arXiv preprint arXiv:1904.12054},
keywords = {automated machine learning,automl,hyperparameter,machine learning,optimization,survey},
title = {{Survey on automated machine learning}},
url = {http://arxiv.org/pdf/1904.12054.pdf},
year = {2019}
}
@inproceedings{abbasi-yadkori2018best,
abstract = {We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards? First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.},
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter L. and Gabillon, Victor and Malek, Alan and Valko, Michal},
booktitle = {Proceedings of the 31st Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbasi-Yadkori et al. - 2018 - Best of both worlds Stochastic {\&} adversarial best-arm identification.pdf:pdf},
title = {{Best of both worlds: Stochastic {\&} adversarial best-arm identification}},
url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/abbasi-yadkori2018best.pdf},
year = {2018}
}
@article{atwood1969optimal,
abstract = {This paper consists of new results continuing the series of papers on optimal design theory by Kiefer (1959), (1960), (1961), Kiefer and Wolfowitz (1959), (1960), Farrell, Kiefer and Walbran (1965) and Karlin and Studden (1966a). After disposing of the necessary preliminaries in Section 1, we show in Section 2 that in several classes of problems an optimal design for estimating all the parameters is supported only on certain points of symmetry. This is applied to the problem (introduced by Scheffe (1958)) of multilinear regression on the simplex. In Section 3 we consider optimality when nuisance parameters are present. A new sufficient condition for optimality is given. A corrected version is given to the condition which Karlin and Studden (1966a) state as equivalent to optimality, and we prove the natural invariance theorem involving this condition. These results are applied to the problem of multilinear regression on the simplex when estimating only some of the parameters. Section 4 consists primarily o...},
author = {Atwood, Corwin L.},
doi = {10.1214/aoms/1177697374},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {5},
pages = {1570--1602},
title = {{Optimal and efficient designs of experiments}},
volume = {40},
year = {1969}
}
@inproceedings{hyttinen2015docalculus,
abstract = {One of the basic tasks of causal discovery is to estimate the causal effect of some set of variables on another given a statistical data set. In this article we bridge the gap between causal struc-ture discovery and the do-calculus by proposing a method for the identification of causal effects on the basis of arbitrary (equivalence) classes of semi-Markovian causal models. The approach uses a general logical representation of the equiv-alence class of graphs obtained from a causal structure discovery algorithm, the properties of which can then be queried by procedures im-plementing the do-calculus inference for causal effects. We show that the method is more ef-ficient than determining causal effects using a naive enumeration of graphs in the equivalence class. Moreover, the method is complete with respect to the identifiability of causal effects for settings, in which extant methods that do not re-quire knowledge of the true graph, offer only in-complete results. The method is entirely modular and easily adapted for different background set-tings.},
author = {Hyttinen, Antti and Eberhardt, Frederick and Matti, J{\"{a}}rvisalo},
booktitle = {Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI)},
isbn = {9780000000002},
issn = {0163-0571},
pages = {395--404},
title = {{Do-calculus when the true graph Is unknown}},
url = {https://www.its.caltech.edu/{~}fehardt/papers/HEJ{\_}UAI2015.pdf},
year = {2015}
}
@inproceedings{cuturi2013sinkhorn,
abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
archivePrefix = {arXiv},
arxivId = {1306.0895},
author = {Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
eprint = {1306.0895},
isbn = {9781510810587},
issn = {10495258},
pages = {2292--2300},
pmid = {1714571},
title = {{Sinkhorn distances: Lightspeed computation of optimal transportation distances}},
url = {http://arxiv.org/pdf/1306.0895.pdf},
year = {2013}
}
@article{chan2020ssmc,
abstract = {Lai and Robbins (Adv. in Appl. Math. 6 (1985) 4–22) and Lai (Ann. Statist. 15 (1987) 1091–1114) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback–Leibler information of the reward distributions, estimated from specified parametric families. In recent years, there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Nonparametric arm allocation procedures like $\epsilon$-greedy, Boltzmann exploration and BESA were studied, and modified versions of the UCB procedure were also analyzed under nonparametric settings. However, unlike UCB these nonparametric procedures are not efficient under general parametric settings. In this paper, we propose efficient nonparametric procedures.},
author = {Chan, Hock Peng},
doi = {10.1214/18-AOS1809},
issn = {21688966},
journal = {Annals of Statistics},
keywords = {Efficiency,KL-UCB,Subsampling,Thompson sampling,UCB},
number = {1},
pages = {346--373},
title = {{The multi-armed bandit problem: An efficient nonparametric solution}},
url = {https://arxiv.org/pdf/1703.08285.pdf},
volume = {48},
year = {2020}
}
@phdthesis{kaufmann2014thesis,
author = {Kaufmann, Emilie},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann - 2014 - Analyse de strat{\'{e}}gies bay{\'{e}}siennes et fr{\'{e}}quentistes pour l'allocation s{\'{e}}quentielle de ressources.pdf:pdf},
school = {T{\'{e}}l{\'{e}}com ParisTech},
title = {{Analyse de strat{\'{e}}gies bay{\'{e}}siennes et fr{\'{e}}quentistes pour l'allocation s{\'{e}}quentielle de ressources}},
url = {http://www.theses.fr/2014ENST0056},
year = {2014}
}
@article{derooij2014hedge,
abstract = {Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains. {\textcopyright} 2014 Steven de Rooij, Tim van Erven, Peter D. Grnwald and Wouter M.},
archivePrefix = {arXiv},
arxivId = {1301.0534},
author = {de Rooij, Steven and {Van Erven}, Tim and Gr{\"{u}}nwald, Peter D. and Koolen, Wouter M.},
eprint = {1301.0534},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Rooij et al. - 2014 - Follow the leader if you can, hedge if you must.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Hedge,Learning rate,Mixability,Online learning,Prediction with expert advice},
pages = {1281--1316},
title = {{Follow the leader if you can, hedge if you must}},
url = {https://arxiv.org/pdf/1301.0534.pdf},
volume = {15},
year = {2014}
}
@article{zhang2020advantage,
abstract = {We study the reinforcement learning problem in the setting of finite-horizon episodic Markov Decision Processes (MDPs) with S states, A actions, and episode length H. We propose a model-free algorithm UCB-ADVANTAGE and prove that it achieves {\~{O}}p?H2SAT q regret where T “KH and K is the number of episodes to play. Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theoretic lower bound up to logarithmic factors. We also show that UCB-ADVANTAGE achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019].},
archivePrefix = {arXiv},
arxivId = {2004.10019},
author = {Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
eprint = {2004.10019},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou, Ji - 2020 - Almost optimal model-free reinforcement learning via reference-advantage decomposition.pdf:pdf},
issn = {23318422},
journal = {arXiv preprint arXiv:2004.10019},
title = {{Almost optimal model-free reinforcement learning via reference-advantage decomposition}},
url = {https://arxiv.org/pdf/2004.10019.pdf},
year = {2020}
}
@inproceedings{mnih2016a3c,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
eprint = {1602.01783},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous methods for deep reinforcement learning(2).pdf:pdf},
isbn = {9781510829008},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
url = {https://arxiv.org/pdf/1602.01783.pdf},
year = {2016}
}
@article{argall2009demo,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1105.1186v1},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
eprint = {arXiv:1105.1186v1},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
pmid = {21045796},
title = {{A survey of robot learning from demonstration}},
url = {https://ac.els-cdn.com/S0921889008001772/1-s2.0-S0921889008001772-main.pdf?{\_}tid=d96125a0-2115-4702-b15d-287316768479{\&}acdnat=1539794407{\_}0d783abb061431e8cddd51cad3c971bb},
volume = {57},
year = {2009}
}
@inproceedings{kleinberg2004nearly,
abstract = {In the multiarmed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is finite, much less is known when there is an infinite strategy set. Here we consider the case when the set of strategies is a subset of R d , and the cost functions are continuous. In the d = 1 case, we improve on the bestknown upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d {\textgreater} 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multiarmed bandit problem.},
author = {Kleinberg, Robert},
booktitle = {Advances in Neural Information Processing Systems 17 (NIPS)},
isbn = {0262195348},
issn = {10495258},
pages = {697--704},
title = {{Nearly tight bounds for the continuum-armed bandit problem}},
url = {https://papers.nips.cc/paper/2634-nearly-tight-bounds-for-the-continuum-armed-bandit-problem.pdf},
year = {2004}
}
@inproceedings{lin2019mirror,
abstract = {We provide theoretical analyses for two algorithms that solve the regularized optimal transport (OT) problem between two discrete probability measures with at most {\$}n{\$} atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the Greenkhorn algorithm, can be improved to {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}\backslashleft(\backslashfrac{\{}n{\^{}}2{\}}{\{}\backslashvarepsilon{\^{}}2{\}}\backslashright){\$}, improving on the best known complexity bound of {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}\backslashleft(\backslashfrac{\{}n{\^{}}2{\}}{\{}\backslashvarepsilon{\^{}}3{\}}\backslashright){\$}. Notably, this matches the best known complexity bound for the Sinkhorn algorithm and helps explain why the Greenkhorn algorithm can outperform the Sinkhorn algorithm in practice. Our proof technique, which is based on a primal-dual formulation and a novel upper bound for the dual solution, also leads to a new class of algorithms that we refer to as adaptive primal-dual accelerated mirror descent (APDAMD) algorithms. We prove that the complexity of these algorithms is {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}\backslashleft(\backslashfrac{\{}n{\^{}}2\backslashgamma{\^{}}{\{}1/2{\}}{\}}{\{}\backslashvarepsilon{\}}\backslashright){\$}, where {\$}\backslashgamma{\textgreater}0{\$} refers to the inverse of the strong convexity module of Bregman divergence with respect to {\$}\backslashleft\backslash|\backslashcdot\backslashright\backslash|{\_}\backslashinfty{\$}. This implies that the APDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms of {\$}\backslashvarepsilon{\$}. Experimental results on synthetic and real datasets demonstrate the favorable performance of the Greenkhorn and APDAMD algorithms in practice.},
archivePrefix = {arXiv},
arxivId = {1901.06482},
author = {Lin, Tianyi and Ho, Nhat and Jordan, Michael I.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1901.06482},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Ho, Jordan - 2019 - On efficient optimal transport An analysis of greedy and accelerated mirror descent algorithms.pdf:pdf},
title = {{On efficient optimal transport: An analysis of greedy and accelerated mirror descent algorithms}},
url = {http://arxiv.org/pdf/1901.06482.pdf},
year = {2019}
}
@inproceedings{martin2017count,
abstract = {We introduce a new count-based optimistic exploration algorithm for reinforcement learning (RL) that is feasible in environments with highdimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our $\Phi$-pseudocount achieves generalisation by exploiting the same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The $\Phi$-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on highdimensional RL benchmarks.},
archivePrefix = {arXiv},
arxivId = {1706.08090},
author = {Martin, Jarryd and Narayanan, Suraj S. and Everitt, Tom and Hutter, Marcus},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2017/344},
eprint = {1706.08090},
isbn = {9780999241103},
issn = {10450823},
pages = {2471--2478},
title = {{Count-based exploration in feature space for reinforcement learning}},
url = {https://arxiv.org/pdf/1706.08090.pdf},
volume = {0},
year = {2017}
}
@inproceedings{liu2020adambs,
abstract = {Adam is a widely used optimization method for training deep learning models. It computes individual adaptive learning rates for different parameters. In this paper, we propose a generalization of Adam, called Adambs, that allows us to also adapt to different training examples based on their importance in the model's convergence. To achieve this, we maintain a distribution over all examples, selecting a mini-batch in each iteration by sampling according to this distribution, which we update using a multi-armed bandit algorithm. This ensures that examples that are more beneficial to the model training are sampled with higher probabilities. We theoretically show that Adambs improves the convergence rate of Adam---{\$}O(\backslashsqrt{\{}\backslashfrac{\{}\backslashlog n{\}}{\{}T{\}} {\}}){\$} instead of {\$}O(\backslashsqrt{\{}\backslashfrac{\{}n{\}}{\{}T{\}}{\}}){\$} in some cases. Experiments on various models and datasets demonstrate Adambs's fast convergence in practice.},
archivePrefix = {arXiv},
arxivId = {2010.12986},
author = {Liu, Rui and Wu, Tianyi and Mozafari, Barzan},
booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
eprint = {2010.12986},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Wu, Mozafari - 2020 - Adam with bandit sampling for deep learning.pdf:pdf},
title = {{Adam with bandit sampling for deep learning}},
url = {http://arxiv.org/pdf/2010.12986.pdf},
year = {2020}
}
@inproceedings{vanhasselt2016ddqn,
abstract = {We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
booktitle = {Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {1509.06461},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Hasselt, Guez, Silver - 2016 - Deep reinforcement learning with double Q-learning(2).pdf:pdf},
pages = {2801--2809},
title = {{Deep reinforcement learning with double Q-learning}},
url = {https://arxiv.org/pdf/1509.06461.pdf},
year = {2016}
}
@book{schrijver2003,
abstract = {This book offers an in-depth overview of polyhedral methods and efficient algorithms in combinatorial optimization.These methods form a broad, coherent and powerful kernel in combinatorial optimization, with strong links to discrete mathematics, mathematical programming and computer science. In eight parts, various areas are treated, each starting with an elementary introduction to the area, with short, elegant proofs of the principal results, and each evolving to the more advanced methods and results, with full proofs of some of the deepest theorems in the area. Over 4000 references to further research are given, and historical surveys on the basic subjects are presented.},
author = {Schrijver, Alexander},
doi = {10.1007/s10288-004-0035-9},
isbn = {ISBN 978-3-540-44389-6},
issn = {1619-4500},
pmid = {12975452},
publisher = {Springer Science {\&} Business Media},
title = {{Combinatorial Optimization: Polyhedra and Efficiency}},
url = {https://homepages.cwi.nl/{~}lex/files/book.pdf},
volume = {24},
year = {2003}
}
@inproceedings{sani2012risk,
abstract = {Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.},
archivePrefix = {arXiv},
arxivId = {1301.1936},
author = {Sani, Amir and Lazaric, Alessandro and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
eprint = {1301.1936},
isbn = {9781627480031},
issn = {10495258},
pages = {3275--3283},
title = {{Risk-aversion in multi-armed bandits}},
url = {http://arxiv.org/pdf/1301.1936.pdf},
year = {2012}
}
@inproceedings{yin2018dimensionality,
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9702052v2},
author = {Yin, Zi and Shen, Yuanyuan},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
doi = {10.7554/eLife.17210.001},
eprint = {9702052v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin, Shen - 2018 - On the dimensionality of word embedding.pdf:pdf},
isbn = {9780123736604},
issn = {2050084X},
pmid = {26151672},
primaryClass = {arXiv:gr-qc},
title = {{On the dimensionality of word embedding}},
url = {https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf},
year = {2018}
}
@article{chang2011libsvm,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail},
author = {Chang, Chih-Chun and Lin, Chih-Jen},
doi = {10.1145/1961189.1961199},
isbn = {2157-6904},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
number = {3},
pages = {1--27},
pmid = {371},
title = {{LIBSVM: A library for support vector machines}},
url = {https://www.csie.ntu.edu.tw/{~}cjlin/papers/libsvm.pdf},
volume = {2},
year = {2011}
}
@inproceedings{gretton2005hsic,
abstract = {We propose an independence criterion based on the eigen- $\backslash$r$\backslash$nspectrum of covariance operators in reproducing kernel Hilbert spaces $\backslash$r$\backslash$n(RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt $\backslash$r$\backslash$nnorm of the cross-covariance operator (we term this a Hilbert-Schmidt In- $\backslash$r$\backslash$ndependence Criterion, or HSIC). This approach has several advantages, $\backslash$r$\backslash$ncompared with previous kernel-based independence criteria. First, the $\backslash$r$\backslash$nempirical estimate is simpler than any other kernel dependence test, and $\backslash$r$\backslash$nrequires no user-defined regularisation. Second, there is a clearly defined $\backslash$r$\backslash$npopulation quantity which the empirical estimate approaches in the large $\backslash$r$\backslash$nsample limit, with exponential convergence guaranteed between the two: $\backslash$r$\backslash$nthis ensures that independence tests based on HSIC do not suffer from $\backslash$r$\backslash$nslow learning rates. Finally, we show in the context of independent com- $\backslash$r$\backslash$nponent analysis (ICA) that the performance of HSIC is competitive with $\backslash$r$\backslash$nthat of previously published kernel-based criteria, and of other recently $\backslash$r$\backslash$npublished ICA methods.},
author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alexander and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Proceedings of the 16th International Conference on Algorithmic Learning Theory (ALT)},
doi = {10.1007/11564089_7},
isbn = {354029242X},
issn = {03029743},
pages = {63--77},
pmid = {16887492},
title = {{Measuring statistical dependence with Hilbert-Schmidt norms}},
url = {http://www.gatsby.ucl.ac.uk/{~}gretton/papers/GreBouSmoSch05.pdf},
year = {2005}
}
@inproceedings{badia2020ngu,
abstract = {We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0{\%}. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {2002.06038},
author = {Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Mart{\'{i}}n and Pritzel, Alexander and Bolt, Andew and Blundell, Charles},
booktitle = {Proceedings of the 8th International Conference on Learning Representations (ICLR)},
eprint = {2002.06038},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Badia et al. - 2020 - Never give up Learning directed exploration strategies.pdf:pdf},
title = {{Never give up: Learning directed exploration strategies}},
url = {http://arxiv.org/pdf/2002.06038.pdf},
year = {2020}
}
@article{Author2018,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/Documents/xuedong/phd/work/reviews/NeurIPS 2019/Submission 4073/4073 Submission.pdf:pdf},
title = {{Improved deep learning hyperparameter search via sublinear generalization error extrapolation}},
year = {2019}
}
@article{brochu2010bayesian,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
doi = {10.1007/9783642532580?COVERIMAGEURL=HTTPS://STATICCONTENT.SPRINGER.COM/COVER/BOOK/9783642532580.JPG},
eprint = {1012.2599},
isbn = {0671631985},
issn = {09420940},
journal = {arXiv preprint arXiv:1012.2599},
pmid = {27489955},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
url = {http://arxiv.org/pdf/1012.2599.pdf},
year = {2010}
}
@inproceedings{garivier2011klucb,
abstract = {This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free index policy for stochastic bandit problems. We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm satisfies a uniformly better regret bound than UCB or UCB2; second, in the special case of Bernoulli rewards, it reaches the lower bound of Lai and Robbins. Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for specific classes of (possibly unbounded) rewards, including those generated from exponential families of distributions. A large-scale numerical study comparing KL-UCB with its main competitors (UCB, UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better than the basic UCB policy. Our regret bounds rely on deviations results of independent interest which are stated and proved in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB algorithm.},
archivePrefix = {arXiv},
arxivId = {1102.2490},
author = {Garivier, Aur{\'{e}}lien and Capp{\'{e}}, Olivier},
booktitle = {Proceeding of the 25th Annual Conference on Learning Theory (CoLT)},
eprint = {1102.2490},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garivier, Capp{\'{e}} - 2011 - The KL-UCB algorithm for bounded stochastic bandits and beyond.pdf:pdf},
issn = {15337928},
title = {{The KL-UCB algorithm for bounded stochastic bandits and beyond}},
url = {http://arxiv.org/pdf/1102.2490.pdf},
year = {2011}
}
@article{lecun2015,
abstract = {This article presents results from an international collaboration between college students and pre-service teachers in Norway and the UK. This research is part of a large, international project exploring and developing the interrelationship between mobile technology and teachers' perceptions of teaching and learning. Data was collected for this study through an on-line survey of 37 pre-service teachers followed by six semi-structured, in-depth interviews. The data analysis revealed the themes of collaboration, authenticity and professional learning through the use of mobile technology in the data. The collaboration enabled the use of the affordances of mobile technology to enhance the pre-service teachers' professional learning and the data suggested that this enhanced their emergent conceptions of teaching and learning.},
author = {LuCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nmeth.3707},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LuCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {15487105},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/NatureDeepReview.pdf},
volume = {521},
year = {2015}
}
@inproceedings{agarwal2014iloveconbandits,
abstract = {We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of {\$}K{\$} actions in response to the observed context, and observes the reward only for that chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}KT/\backslashlog N{\}}){\$} oracle calls across all {\$}T{\$} rounds, where {\$}N{\$} is the number of policies in the policy class we compete against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.},
archivePrefix = {arXiv},
arxivId = {1402.0555},
author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert E.},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
eprint = {1402.0555},
isbn = {9781634393973},
issn = {10769757},
pages = {1638--1646},
pmid = {17255001},
title = {{Taming the monster: A fast and simple algorithm for contextual bandits}},
url = {http://arxiv.org/pdf/1402.0555.pdf},
year = {2014}
}
@inproceedings{schmidhuber1991curiosity,
abstract = {A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the 1991 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/ijcnn.1991.170605},
isbn = {0780302273},
pages = {1458--1463},
title = {{Curious model-building control systems}},
url = {https://ieeexplore.ieee.org/document/170605},
year = {1991}
}
@article{vieillard2020regularized,
abstract = {Building upon the formalism of regularized Markov decision processes, we study the effect of Kullback-Leibler (KL) and entropy regularization in reinforcement learning. Through an equivalent formulation of the related approximate dynamic programming (ADP) scheme, we show that a KL penalty amounts to averaging q-values. This equivalence allows drawing connections between a priori disconnected methods from the literature, and proving that a KL regularization indeed leads to averaging errors made at each iteration of value function update. With the proposed theoretical analysis, we also study the interplay between KL and entropy regularization. When the considered ADP scheme is combined with neural-network-based stochastic approximations, the equivalence is lost, which suggests a number of different ways to do regularization. Because this goes beyond what we can analyse theoretically, we extensively study this aspect empirically.},
archivePrefix = {arXiv},
arxivId = {2003.14089},
author = {Vieillard, Nino and Kozuno, Tadashi and Scherrer, Bruno and Pietquin, Olivier and Munos, R{\'{e}}mi and Geist, Matthieu},
eprint = {2003.14089},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vieillard et al. - 2020 - Leverage the average an analysis of regularization in RL.pdf:pdf},
journal = {arXiv preprint arXiv:2003.14089},
title = {{Leverage the average: an analysis of regularization in RL}},
url = {http://arxiv.org/pdf/2003.14089.pdf},
year = {2020}
}
@article{auer2002ucb,
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auer, Cesa-Bianchi, Fischer - 2002 - Finite-time analysis of the multi-armed bandit problem.pdf:pdf},
journal = {Machine Learning Journal},
number = {2-3},
pages = {235--256},
title = {{Finite-time analysis of the multi-armed bandit problem}},
url = {https://homes.di.unimi.it/{~}cesabian/Pubblicazioni/ml-02.pdf},
volume = {47},
year = {2002}
}
@article{hinton2006dbn,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/ncfast.pdf},
volume = {18},
year = {2006}
}
@book{rasmussen2006gp,
abstract = {To determine caregiver burden and its repercussions on quality of life, physical and mental health and social life and the use for healthcare resources (frequency).},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl E. and Williams, Christopher K.I.},
doi = {10.4321/S1699-695X2009000200004},
eprint = {026218253X},
isbn = {1696145090},
issn = {1699-695X},
pmid = {24390405},
publisher = {The MIT Press},
title = {{Gaussian Process for Machine Learning}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
year = {2006}
}
@inproceedings{agrawal2017posterior,
abstract = {We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of {\~{O}}(D√/SAT) for any communicating MDP with S states, A actions and diameter D, when T ≥ S5A. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T. This result improves over the best previously known upper bound of {\~{O}}(DS√AT) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of $\Omega$(√DSAT) for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07041v3},
author = {Agrawal, Shipra and Jia, Randy},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {arXiv:1705.07041v3},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Jia - 2017 - Optimistic posterior sampling for reinforcement learning Worst-case regret bounds.pdf:pdf},
issn = {10495258},
pages = {1185--1195},
title = {{Optimistic posterior sampling for reinforcement learning: Worst-case regret bounds}},
url = {https://arxiv.org/pdf/1705.07041.pdf},
year = {2017}
}
@inproceedings{jaggi2013franke-wolfe,
abstract = {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices. We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach. Copyright 2013 by the author(s).},
author = {Jaggi, Martin},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaggi - 2013 - Revisiting Frank-Wolfe Projection-free sparse convex optimization.pdf:pdf},
pages = {427--435},
title = {{Revisiting Frank-Wolfe: Projection-free sparse convex optimization}},
volume = {28},
year = {2013}
}
@inproceedings{Sidford2018near,
abstract = {In this paper we consider the problem of computing an ǫ-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in O(1) time. Given such a DMDP with states S, actions A, discount factor $\gamma$ ∈ (0, 1), and rewards in range [0, 1] we provide an algorithm which computes an ǫ-optimal policy with probability 1 − $\delta$ where both the time spent and number of sample taken are upper bounded by O [(1|S||A|− $\gamma$)3ǫ2 log ((1|S||A|− $\gamma$)$\delta$ǫ ) log ( (1 −1$\gamma$)ǫ )] . For fixed values of ǫ ∈ (0, 1), this improves upon the previous best known bounds by a factor of (1−$\gamma$)−1 and matches the sample complexity lower bounds proved in [AMK13] up to logarithmic factors. We also extend our method to computing ǫ-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.},
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F. and Ye, Yinyu},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
issn = {23318422},
title = {{Near-Optimal time and sample complexities for solving discounted Markov decision process with a generative model}},
url = {https://arxiv.org/pdf/1806.01492.pdf},
year = {2018}
}
@inproceedings{libin2018,
abstract = {Pandemic influenza has the epidemic potential to kill millions of people. While various preventive measures exist (i.a., vaccination and school closures), deciding on strategies that lead to their most effective and efficient use remains challenging. To this end, individual-based epidemiological models are essential to assist decision makers in determining the best strategy to curb epidemic spread. However, individual-based models are computationally intensive and it is therefore pivotal to identify the optimal strategy using a minimal amount of model evaluations. Additionally, as epidemiological modeling experiments need to be planned, a computational budget needs to be specified a priori. Consequently, we present a new sampling technique to optimize the evaluation of preventive strategies using fixed budget best-arm identification algorithms. We use epidemiological modeling theory to derive knowledge about the reward distribution which we exploit using Bayesian best-arm identification algorithms (i.e., Top-two Thompson sampling and BayesGap). We evaluate these algorithms in a realistic experimental setting and demonstrate that it is possible to identify the optimal strategy using only a limited number of model evaluations, i.e., 2-to-3 times faster compared to the uniform sampling method, the predominant technique used for epidemiological decision making in the literature. Finally, we contribute and evaluate a statistic for Top-two Thompson sampling to inform the decision makers about the confidence of an arm recommendation. Code related to this paper is available at: https://plibin-vub.github.io/epidemic-bandits.},
archivePrefix = {arXiv},
arxivId = {1711.06299},
author = {Libin, Pieter J.K. and Verstraeten, Timothy and Roijers, Diederik M. and Grujic, Jelena and Theys, Kristof and Lemey, Philippe and Now{\'{e}}, Ann},
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2018 (ECML-PKDD)},
doi = {10.1007/978-3-030-10997-4_28},
eprint = {1711.06299},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Libin et al. - 2018 - Bayesian best-arm identification for selecting influenza mitigation strategies(2).pdf:pdf},
isbn = {9783030109967},
issn = {16113349},
keywords = {Fixed budget best-arm identification,Individual-based models,Multi-armed bandits,Pandemic influenza,Preventive strategies},
pages = {456--471},
title = {{Bayesian best-arm identification for selecting influenza mitigation strategies}},
url = {https://arxiv.org/pdf/1711.06299.pdf},
year = {2018}
}
@inproceedings{agrawal2016knapsacks,
abstract = {We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn't exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual) [8, 11, 1], bandits with knapsacks (BwK) [3, 9], and the online stochastic packing problem (OSPP) [4, 14]. We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem [5, 10] where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.},
archivePrefix = {arXiv},
arxivId = {1507.06738},
author = {Agrawal, Shipra and Devanur, Nikhil R.},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
eprint = {1507.06738},
issn = {10495258},
pages = {3458--3466},
title = {{Linear contextual bandits with knapsacks}},
url = {https://arxiv.org/pdf/1507.06738.pdf},
year = {2016}
}
@inproceedings{dewancker2016,
abstract = {Empirical analysis serves as an important complement to theoretical analysis for studying practical Bayesian optimization. Often empirical insights expose strengths and weaknesses inaccessible to theoretical analysis. We define two metrics for comparing the performance of Bayesian optimization methods and propose a ranking mechanism for summarizing performance within various genres or strata of test functions. These test functions serve to mimic the complexity of hyperparameter optimization problems, the most prominent application of Bayesian optimization, but with a closed form which allows for rapid evaluation and more predictable behavior. This offers a flexible and efficient way to investigate functions with specific properties of interest, such as oscillatory behavior or an optimum on the domain boundary.},
archivePrefix = {arXiv},
arxivId = {1603.09441},
author = {Dewancker, Ian and McCourt, Michael and Clark, Scott and Hayes, Patrick and Johnson, Alexandra and Ke, George},
booktitle = {3rd Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
eprint = {1603.09441},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dewancker et al. - 2016 - A strategy for ranking optimization methods using multiple criteria(2).pdf:pdf},
pages = {1--12},
title = {{A strategy for ranking optimization methods using multiple criteria}},
url = {http://arxiv.org/abs/1603.09441{\%}5Cnhttps://github.com/sigopt/evalset},
year = {2016}
}
@inproceedings{baransi2014besa,
author = {Baransi, Akram and Maillard, Odalric-ambrym and Mannor, Shie},
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2014 (ECML-PKDD)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baransi, Maillard, Mannor - 2014 - Sub-sampling for multi-armed bandits.pdf:pdf},
title = {{Sub-sampling for multi-armed bandits}},
url = {https://hal.archives-ouvertes.fr/file/index/docid/1025651/filename/BESA2.pdf},
year = {2014}
}
@inproceedings{swersky2013multitask,
abstract = {Bayesian optimization has recently been proposed as a framework for automati- cally tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter set- tings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our al- gorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, en- tropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyper- parameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3896v1},
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
eprint = {arXiv:1406.3896v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swersky, Snoek, Adams - 2013 - Multi-task Bayesian optimization.pdf:pdf},
issn = {10495258},
pages = {2004--2012},
title = {{Multi-task Bayesian optimization}},
url = {http://papers.nips.cc/paper/5086-multi-task-bayesian-optimization.pdf},
year = {2013}
}
@inproceedings{fomin2019pca,
abstract = {Principal component analysis (PCA) is one of the most fundamental procedures in exploratory data analysis and is the basic step in applications ranging from quantitative finance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the applicability of PCA in many real scenarios could be constrained by an "immune deficiency" to outliers such as corrupted observations. We consider the following algorithmic question about the PCA with outliers. For a set of {\$}n{\$} points in {\$}\backslashmathbb{\{}R{\}}{\^{}}{\{}d{\}}{\$}, how to learn a subset of points, say 1{\%} of the total number of points, such that the remaining part of the points is best fit into some unknown {\$}r{\$}-dimensional subspace? We provide a rigorous algorithmic analysis of the problem. We show that the problem is solvable in time {\$}n{\^{}}{\{}O(d{\^{}}2){\}}{\$}. In particular, for constant dimension the problem is solvable in polynomial time. We complement the algorithmic result by the lower bound, showing that unless Exponential Time Hypothesis fails, in time {\$}f(d)n{\^{}}{\{}o(d){\}}{\$}, for any function {\$}f{\$} of {\$}d{\$}, it is impossible not only to solve the problem exactly but even to approximate it within a constant factor.},
archivePrefix = {arXiv},
arxivId = {1905.04124},
author = {Fomin, Fedor V. and Golovach, Petr A. and Panolan, Fahad and Simonov, Kirill},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1905.04124},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fomin et al. - 2019 - Refined complexity of PCA with outliers.pdf:pdf},
title = {{Refined complexity of PCA with outliers}},
url = {http://arxiv.org/pdf/1905.04124.pdf},
year = {2019}
}
@inproceedings{chu2011linucb,
abstract = {Abstract In this paper we study the contextual bandit problem (also known as the multi- armed bandit problem with expert advice) for linear payoff functions . For T rounds, K actions, and d dimensional feature vectors, we prove an O$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2369v1},
author = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert E.},
booktitle = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {arXiv:1106.2369v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - Contextual bandits with linear payoff functions(2).pdf:pdf},
issn = {{\textless}null{\textgreater}},
keywords = {To Read Urgently},
pages = {208--214},
title = {{Contextual bandits with linear payoff functions}},
url = {https://www.cs.princeton.edu/{~}schapire/papers/bandit-lin.pdf{\%}5Cnpapers2://publication/uuid/03F8E2D2-621B-414E-A5DE-3B5E5659E4DD{\%}5Cnhttp://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2011{\_}ChuLRS11.pdf},
year = {2011}
}
@inproceedings{koriche2018online,
author = {Koriche, Frederic},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koriche - 2018 - Compiling combinatorial prediction games(2).pdf:pdf},
pages = {2756--2765},
title = {{Compiling combinatorial prediction games}},
url = {http://proceedings.mlr.press/v80/koriche18a/koriche18a.pdf},
year = {2018}
}
@article{charpagne2019ebsd,
abstract = {A new method has been developed for the correction of the distortions and/or enhanced phase differentiation in Electron Backscatter Diffraction (EBSD) data. Using a multi-modal data approach, the method uses segmented images of the phase of interest (laths, precipitates, voids, inclusions) on images gathered by backscattered or secondary electrons of the same area as the EBSD map. The proposed approach then search for the best transformation to correct their relative distortions and recombines the data in a new EBSD file. Speckles of the features of interest are first segmented in both the EBSD and image data modes. The speckle extracted from the EBSD data is then meshed, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is implemented to distort the mesh until the speckles superimpose. The quality of the matching is quantified via a score that is linked to the number of overlapping pixels in the speckles. The locations of the points of the distorted mesh are compared to those of the initial positions to create pairs of matching points that are used to calculate the polynomial function that describes the distortion the best. This function is then applied to un-distort the EBSD data, and the phase information is inferred using the data of the segmented speckle. Fast and versatile, this method does not require any human annotation and can be applied to large datasets and wide areas. Besides, this method requires very few assumptions concerning the shape of the distortion function. It can be used for the single compensation of the distortions or combined with the phase differentiation. The accuracy of this method is of the order of the pixel size. Some application examples in multiphase materials with feature sizes down to 1 m are presented, including Ti-6Al-4V Titanium alloy, Rene 65 and additive manufactured Inconel 718 Nickel-base superalloys.},
author = {Charpagne, Marie Agathe and Strub, Florian and Pollock, Tresa M.},
issn = {23318422},
journal = {Materials Characterization},
keywords = {CMA-ES,EBSD,distortions,image segmentation,multi-modal data},
pages = {184--198},
title = {{Accurate reconstruction of EBSD datasets by a multimodal data approach using an evolutionary algorithm}},
url = {https://arxiv.org/pdf/1903.02988.pdf},
volume = {150},
year = {2019}
}
@inproceedings{drugan2014,
abstract = {Multi-objective multi-armed bandits (MOMAB) is an extension of the multi-objective multi-armed bandits framework that considers reward vectors instead of scalar reward values. Scalarization functions transform the reward vectors into reward values in order to use the standard multi-armed bandits (MAB) algorithms. However for many applications it is not obvious to come up with a good scalarization set and therefore there is needed to develop MAB that discover the whole Pareto set of arms. Our approach to this multi-objective MAB problem is two folded: i) identify the set of Pareto optimal arms and ii) identify the minimum subset of scalarization functions that optimize the set of Pareto optimal arms. We experimentally compare the proposed MOMAB algorithms on a multi-objective Bernoulli problem.},
author = {Drugan, Madalina M. and Nowe, Ann},
booktitle = {Proceedings of the 2014 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2014.6889484},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drugan, Nowe - 2014 - Scalarization based Pareto optimal set of arms identification algorithms.pdf:pdf},
isbn = {9781479914845},
pages = {2690--2697},
title = {{Scalarization based Pareto optimal set of arms identification algorithms}},
url = {https://ai.vub.ac.be/sites/default/files/scalarizedMOMAB{\_}IEEE{\_}accept{\_}v2.pdf},
year = {2014}
}
@phdthesis{kakade2003thesis,
abstract = {In this paper, we study the sample complexity of weak learning. That is, we ask how many data must be collected from an unknown distribution in order to extract a small but significant advantage in prediction. We show that it is important to distinguish between those learning algorithms that output deterministic hypotheses and those that output randomized hypotheses. We prove that in the weak learning model, any algorithm using deterministic hypotheses to weakly learn a class of Vapnik-Chervonenkis dimension d(n) requires $\Omega$(√ d(n)) examples. In contrast, when randomized hypotheses are allowed, we show that $\Theta$(1) examples suffice in some cases. We then show that there exists an efficient algorithm using deterministic hypotheses that weakly learns against any distribution on a set of size d(n) with only O(d(n)2/3) examples. Thus for the class of symmetric Boolean functions over n variables, where the strong learning sample complexity is $\Theta$(n), the sample complexity for weak learning using deterministic hypotheses is $\Omega$(√ n) and O(n2/3), and the sample complexity for weak learning using randomized hypotheses is $\Theta$(1). Next we prove the existence of classes for which the distribution-free sample size required to obtain a slight advantage in prediction over random guessing is essentially equal to that required to obtain arbitrary accuracy. Finally, for a class of small circuits, namely all parity functions of subsets of n Boolean variables, we prove a weak learning sample complexity of $\Theta$(n). This bound holds even if the weak learning algorithm is allowed to replace random sampling with membership queries, and the target distribution is uniform on (0, 1)n. {\textcopyright} 1995 Academic Press, Inc.},
author = {Kakade, Sham},
doi = {10.1006/inco.1995.1045},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakade - 2003 - On the Sample Complexity of Reinforcement Learning.pdf:pdf},
issn = {10902651},
school = {University College London},
title = {{On the Sample Complexity of Reinforcement Learning}},
url = {https://homes.cs.washington.edu/{~}sham/papers/thesis/sham{\_}thesis.pdf},
year = {2003}
}
@article{audibert2014combinatorial,
abstract = {We address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors. The regret of the decision maker is the difference between her realized loss and the best loss she would have achieved by picking, in hindsight, the best possible action. Our goal is to understand the magnitude of the best possible (minimax) regret. We study the problem under three different assumptions for the feedback the decision maker receives: full information, and the partial information models of the so-called "semi-bandit" and "bandit" problems. Combining the Mirror Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we are able to prove optimal bounds for the semi-bandit case. We also recover the optimal bounds for the full information setting. In the bandit case we discuss existing results in light of a new lower bound, and suggest a conjecture on the optimal regret in that case. Finally we also prove that the standard exponentially weighted average forecaster is provably suboptimal in the setting of online combinatorial optimization.},
archivePrefix = {arXiv},
arxivId = {1204.4710},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Lugosi, G{\'{a}}bor},
doi = {10.1287/moor.2013.0598},
eprint = {1204.4710},
isbn = {0364-765X},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
number = {1},
pages = {31--45},
title = {{Regret in online combinatorial optimization}},
url = {http://arxiv.org/pdf/1204.4710.pdf},
volume = {39},
year = {2014}
}
@article{Authora,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - Model-based asychronous hyperparameter and neural architecture search(3).pdf:pdf},
title = {{Model-based asychronous hyperparameter and neural architecture search}},
year = {2020}
}
@article{watkins1992q,
author = {Watkins, Chris J. and Dayan, Peter},
journal = {Machine Learning},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {https://link.springer.com/content/pdf/10.1007/BF00992698.pdf},
volume = {8},
year = {1992}
}
@article{hoffman2013svi,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1303--1347},
pmid = {19926898},
title = {{Stochastic variational inference}},
url = {http://arxiv.org/pdf/1206.7051.pdf},
volume = {14},
year = {2013}
}
@inproceedings{badanidiyuru2013knapsacks,
author = {Badanidiyuru, Ashwinkumar and Kleinberg, Robert and Slivkins, Aleksandrs},
booktitle = {Proceedings of the 54th IEEE Annual Symposium on Foundations of Computer Science (FOCS)},
doi = {10.1109/focs.2013.30},
pages = {207--216},
title = {{Bandits with Knapsacks}},
url = {https://arxiv.org/pdf/1305.2545.pdf},
year = {2013}
}
@article{guo2019sil,
abstract = {Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezuma's Revenge without using expert demonstrations or resetting to arbitrary states.},
archivePrefix = {arXiv},
arxivId = {1907.10247},
author = {Guo, Yijie and Choi, Jongwook and Moczulski, Marcin and Bengio, Samy and Norouzi, Mohammad and Lee, Honglak},
eprint = {1907.10247},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2019 - Self-imitation learning via trajectory-conditioned policy for hard-exploration tasks(2).pdf:pdf},
journal = {arXiv preprint arXiv:1907.10247},
title = {{Self-imitation learning via trajectory-conditioned policy for hard-exploration tasks}},
url = {http://arxiv.org/pdf/1907.10247.pdf},
year = {2019}
}
@article{bahdanau2014neural,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2014 - Neural machine translation by jointly learning to align and translate.pdf:pdf},
journal = {arXiv preprint arXiv:1409.0473},
title = {{Neural machine translation by jointly learning to align and translate}},
url = {http://arxiv.org/pdf/1409.0473.pdf},
year = {2014}
}
@inproceedings{thune2019delay,
abstract = {We investigate multiarmed bandits with delayed feedback, where the delays need neither be identical nor bounded. We first prove that the "delayed" Exp3 achieves the {\$}O(\backslashsqrt{\{}(KT + D)\backslashln K{\}}){\$} regret bound conjectured by Cesa-Bianchi et al. [2016], in the case of variable, but bounded delays. Here, {\$}K{\$} is the number of actions and {\$}D{\$} is the total delay over {\$}T{\$} rounds. We then introduce a new algorithm that lifts the requirement of bounded delays by using a wrapper that skips rounds with excessively large delays. The new algorithm maintains the same regret bound, but similar to its predecessor requires prior knowledge of {\$}D{\$} and {\$}T{\$}. For this algorithm we then construct a novel doubling scheme that forgoes this requirement under the assumption that the delays are available at action time (rather than at loss observation time). This assumption is satisfied in a broad range of applications, including interaction with servers and service providers. The resulting oracle regret bound is of order {\$}\backslashmin{\_}{\{}\backslashbeta{\}} (|S{\_}\backslashbeta|+\backslashbeta \backslashln K + (KT + D{\_}\backslashbeta)/\backslashbeta){\$}, where {\$}|S{\_}\backslashbeta|{\$} is the number of observations with delay exceeding {\$}\backslashbeta{\$}, and {\$}D{\_}\backslashbeta{\$} is the total delay of observations with delay below {\$}\backslashbeta{\$}. The bound relaxes to {\$}O(\backslashsqrt{\{}(KT + D)\backslashln K{\}}){\$}, but we also provide examples where {\$}D{\_}\backslashbeta \backslashll D{\$} and the oracle bound has a polynomially better dependence on the problem parameters.},
archivePrefix = {arXiv},
arxivId = {1906.00670},
author = {Thune, Tobias Sommer and Cesa-Bianchi, Nicol{\`{o}} and Seldin, Yevgeny},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1906.00670},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thune, Cesa-Bianchi, Seldin - 2019 - Nonstochastic multiarmed bandits with unrestricted delays.pdf:pdf},
title = {{Nonstochastic multiarmed bandits with unrestricted delays}},
url = {http://arxiv.org/pdf/1906.00670.pdf},
year = {2019}
}
@article{katz-samuels2020practical,
archivePrefix = {arXiv},
arxivId = {arXiv:2006.11685v1},
author = {Katz-samuels, Julian and Jain, Lalit and Karnin, Zohar and Jamieson, Kevin},
eprint = {arXiv:2006.11685v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz-samuels et al. - 2020 - An empirical process approach to the union bound Practical algorithms for combinatorial and linear bandits.pdf:pdf},
journal = {arXiv preprint arXiv:2006.11685},
title = {{An empirical process approach to the union bound : Practical algorithms for combinatorial and linear bandits}},
url = {https://arxiv.org/pdf/2006.11685.pdf},
year = {2020}
}
@inproceedings{tao2018alba,
abstract = {We study the best arm identification problem in linear bandits, where the mean reward of each arm depends linearly on an unknown d-dimensional parameter vector 0, and the goal is to identify the arm with the largest expected reward. We first design and analyze a novel randomized 9 estimator based on the solution to the convex relaxation of an optimal G-allocation experiment design problem. Using this estimator, we describe an algorithm whose sample complexity depends linearly on the dimension d, as well as an algorithm with sample complexity dependent on the reward gaps of the best d arms, matching the lower bound arising from the ordinary top-arm identification problem. We finally compare the empirical performance of our algorithms with other state-of-the-art algorithms in terms of both sample complexity and computational time.},
author = {Tao, Chao and Blanco, Saul A. and Zhou, Yuan},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tao, Blanco, Zhou - 2018 - Best arm identification in linear bandits with linear dimension dependency.pdf:pdf},
isbn = {9781510867963},
pages = {7773--7786},
title = {{Best arm identification in linear bandits with linear dimension dependency}},
url = {http://proceedings.mlr.press/v80/tao18a/tao18a.pdf},
year = {2018}
}
@article{cope2009,
abstract = {We consider a class of multi-armed bandit problems where the set of available actions can be mapped to a convex, compact region of ℝd, sometimes denoted as the "continuum-armed bandit" problem. The paper establishes bounds on the efficiency of any arm-selection procedure under certain conditions on the class of possible underlying reward functions. Both finite-time lower bounds on the growth rate of the regret, as well as asymptotic upper bounds on the rates of convergence of the selected control values to the optimum are derived. We explicitly characterize the dependence of these convergence rates on the minimal rate of variation of the mean reward function in a neighborhood of the optimal control. The bounds can be used to demonstrate the asymptotic optimality of the Kiefer-Wolfowitz method of stochastic approximation with regard to a large class of possible mean reward functions. {\textcopyright} 2009 IEEE.},
author = {Cope, Eric W.},
doi = {10.1109/TAC.2009.2019797},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Adaptive control,Sequential decision procedures,Stochastic approximation},
number = {6},
pages = {1243--1253},
title = {{Regret and convergence bounds for a class of continuum-armed bandit problems}},
url = {https://www.semanticscholar.org/paper/Regret-and-Convergence-Bounds-for-a-Class-of-Bandit-Cope/f7ecf6102c14e939ce115a36a1efd02fe5034173},
volume = {54},
year = {2009}
}
@inproceedings{grill2015poo,
abstract = {We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of sqrt(ln n) away from the error of the best known optimization algorithms using the knowledge of the smoothness.},
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grill, Valko, Munos - 2015 - Black-box optimization of noisy functions with unknown smoothness.pdf:pdf},
issn = {10495258},
pages = {667--675},
title = {{Black-box optimization of noisy functions with unknown smoothness}},
url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/grill2015black-box.pdf},
year = {2015}
}
@inproceedings{shang2019adaptive,
abstract = {Hierarchical bandits are an approach for global optimization of emph(extremely) irregular functions. This paper provides new elements regarding POO, an adaptive meta-algorithm that does not require the knowledge of local smoothness of the target function. We first highlight the fact that the subroutine algorithm used in POO should have a small regret under the assumption of emph(local smoothness with respect to the chosen partitioning), which is unknown if it is satisfied by the standard subroutine HOO. In this work, we establish such regret guarantee for HCT, which is another hierarchical optimistic optimization algorithm that needs to know the smoothness. This confirms the validity of POO. We show that POO can be used with HCT as a subroutine with a regret upper bound that matches the one of best-known algorithms using the knowledge of smoothness up to a sqrt(log(n)) factor. On top of that, we further propose a more general wrapper, called GPO, that can cope with algorithms that only have simple regret guarantees.},
author = {Shang, Xuedong and Kaufmann, Emilie and Valko, Michal},
booktitle = {Proceedings of the 30th International Conference on Algorithmic Learning Theory (ALT)},
title = {{General parallel optimization without a metric}},
url = {https://xuedong.github.io/static/documents/shang2019general.pdf},
year = {2019}
}
@inproceedings{mathieu2016future,
abstract = {Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset},
archivePrefix = {arXiv},
arxivId = {1511.05440},
author = {Mathieu, Michael and Couprie, Camille and LeCun, Yann},
booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR)},
eprint = {1511.05440},
title = {{Deep multi-scale video prediction beyond mean square error}},
url = {http://arxiv.org/abs/1511.05440},
year = {2016}
}
@inproceedings{thornton2013autoweka,
abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
archivePrefix = {arXiv},
arxivId = {1208.3719},
author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
doi = {10.1145/2487575.2487629},
eprint = {1208.3719},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thornton et al. - 2013 - Auto-WEKA Combined selection and hyperparameter optimization of classification algorithms.pdf:pdf},
isbn = {9781450321747},
keywords = {Hyperparameter optimization,Model selection,Weka},
pages = {847--855},
title = {{Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms}},
url = {https://dl.acm.org/doi/pdf/10.1145/2487575.2487629},
year = {2013}
}
@inproceedings{mikolov2013negative,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed representations of words and phrases and their compositionality.pdf:pdf},
pages = {3111--3119},
title = {{Distributed representations of words and phrases and their compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@book{mohri2018foundations,
author = {Mohri, Mehryar},
booktitle = {Adaptive Computation and Machine Learning},
edition = {2nd},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohri - 2018 - Foundations of Machine Learning.pdf:pdf},
isbn = {9780262039406},
publisher = {MIT Press},
title = {{Foundations of Machine Learning}},
url = {https://cs.nyu.edu/{~}mohri/mlbook/},
year = {2018}
}
@inproceedings{schnabel2015embedding,
abstract = {We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations ofwords from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.},
author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten},
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/D15-1036},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnabel et al. - 2015 - Evaluation methods for unsupervised word embeddings.pdf:pdf},
isbn = {9781941643327},
issn = {10902104},
pages = {298--307},
pmid = {1847047},
title = {{Evaluation methods for unsupervised word embeddings}},
url = {http://aclweb.org/anthology/D15-1036},
year = {2015}
}
@book{floudas2000,
author = {Floudas, Christodoulos A. and Pardalos, Panos M.},
booktitle = {Nonconvex optimization and its applications},
isbn = {0792361555 (alk. paper)},
keywords = {Chemistry Mathematical models.,Molecular biology Mathematical models.},
number = {40},
pages = {vii, 339 p.},
pmid = {11863955},
title = {{Optimization in computational chemistry and molecular biology : local and global approaches}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-1-4757-3218-4.pdf},
year = {2000}
}
@inproceedings{Zhang2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1907.13359v2},
author = {Zhang, Xiang and Chen, Xiaocong and Yao, Lina and Ge, Chang and Dong, Manqing},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing (ICONIP)},
eprint = {arXiv:1907.13359v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Deep neural network hyperparameter optimization with orthogonal array tuning.pdf:pdf},
keywords = {deep learning,hyper-parameter,orthogonal array},
title = {{Deep neural network hyperparameter optimization with orthogonal array tuning}},
url = {https://arxiv.org/pdf/1907.13359.pdf},
year = {2019}
}
@inproceedings{shang2018adaptive,
abstract = {Hierarchical bandits is an approach for global optimization of extremely irregular functions. This paper provides new elements regarding POO, an adaptive meta-algorithm that does not require the knowledge of local smoothness of the target function. We first highlight the fact that the sub-routine algorithm used in POO should have a small regret under the assumption of local smoothness with respect to the chosen partitioning, which is unknown if it is satisfied by the standard sub-routine HOO. In this work, we establish such regret guarantee for HCT which is another hierarchical optimistic optimization algorithm that needs to know the smoothness. This confirms the validity of POO. We show that POO can be used with HCT as a sub-routine with a regret upper bound that matches that of best-known algorithms using the knowledge of smoothness up to a sqrt(log(n)) factor.},
author = {Shang, Xuedong and Kaufmann, Emilie and Valko, Michal},
booktitle = {14th European Workshop on Reinforcement Learning (EWRL)},
title = {{Adaptive black-box optimization got easier: HCT needs only local smoothness}},
url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/shang2018adaptive.pdf},
year = {2018}
}
@article{he2019automl,
abstract = {Deep learning has penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep learning system for a specific task is not only time-consuming but also requires lots of resources and relies on human expertise, which hinders the development of deep learning in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art AutoML. First, we introduce the AutoML techniques in detail according to the machine learning pipeline. Then we summarize existing Neural Architecture Search (NAS) research, which is one of the most popular topics in AutoML. We also compare the models generated by NAS algorithms with human-designed models. Finally, we present several open problems for future research.},
archivePrefix = {arXiv},
arxivId = {1908.00709},
author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
eprint = {1908.00709},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Zhao, Chu - 2019 - AutoML A Survey of the state-of-the-art.pdf:pdf},
journal = {arXiv preprint arXiv:1908.00709},
keywords = {automated machine learning,automl,deep learning,nas,neural architecture search},
title = {{AutoML: A Survey of the state-of-the-art}},
url = {http://arxiv.org/pdf/1908.00709.pdf},
year = {2019}
}
@book{vaart2000asymptotic,
author = {van der Vaart, Adrianus Willem},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Vaart - 2000 - Asymptotic Statistics.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
url = {https://www.math.leidenuniv.nl/{~}avdvaart/books/asymp-en.html},
volume = {3},
year = {2000}
}
@article{kiefer1959,
author = {Kiefer, J. and Wolfowitz, J.},
doi = {10.1007/978-1-4615-6660-1_2},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {271--294},
title = {{Optimum designs in regression problems}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoms/1177706252},
volume = {30},
year = {1959}
}
@inproceedings{fortunato2018noise,
abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
archivePrefix = {arXiv},
arxivId = {1706.10295},
author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Hessel, Matteo and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
eprint = {1706.10295},
title = {{Noisy networks for exploration}},
url = {https://arxiv.org/pdf/1706.10295.pdf},
year = {2018}
}
@article{littlestone1986,
abstract = {We explore the learnability of two-valued functions from samples us- ing the paradigm of Data Compression. A first algorithm (compression) choses a small subset of the sample which is called the kernel. A second algorithm predicts future values of the function from the kernel, i.e. the algorithm acts as an hypothesis for the function to be learned. The second algorithm must be able to reconstruct the correct function values when given a point of the original sample. We demonstrate that the existence of a suitable data compression scheme is sufficient to ensure learnability. We express the probability that the hypothesis predicts the function correctly on a random sample point as a function of the sample and kernel sizes. No assumptions are made on the probability distributions according to which the sample points are generated. This approach provides an alternative to that of [BEHW86], which uses the Vapnik-Chervonenkis dimension to classify learnable geometric concepts. Our bounds are derived directly from the kernel size of the algorithms rather than from the Vapnik-Chervonenkis dimension of the hypothesis class. The proofs are simpler and the introduced compres- sion scheme provides a rigorous model for studying data compression in connection with machine learning.},
author = {Littlestone, Nick and Warmuth, Manfred K},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Littlestone, Warmuth - 1986 - Relating data compression and learnability.pdf:pdf},
title = {{Relating data compression and learnability}},
url = {https://users.soe.ucsc.edu/{~}manfred/pubs/lrnk-olivier.pdf},
year = {1986}
}
@article{garivier2018explore,
abstract = {We revisit lower bounds on the regret in the case of multiarmed bandit problems. We obtain nonasymptotic, distribution-dependent bounds and provide simple proofs based only on well-known properties of Kullback–Leibler divergences. These bounds show in particular that in the initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the information-theoretic arguments used and they involve no unnecessary complications.},
archivePrefix = {arXiv},
arxivId = {1602.07182},
author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
doi = {10.1287/moor.2017.0928},
eprint = {1602.07182},
issn = {15265471},
journal = {Mathematics of Operations Research},
keywords = {Cumulative regret,Information-theoretic proof techniques,Multiarmed bandits,Nonasymptotic lower bounds},
number = {2},
pages = {377--399},
title = {{Explore first, exploit next: The true shape of regret in bandit problems}},
url = {https://pubsonline.informs.org/doi/pdf/10.1287/moor.2017.0928},
volume = {44},
year = {2018}
}
@article{hui2020,
abstract = {Modern neural architectures for classification tasks are trained using the cross-entropy loss, which is believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be well-founded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.},
archivePrefix = {arXiv},
arxivId = {2006.07322},
author = {Hui, Like and Belkin, Mikhail},
eprint = {2006.07322},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hui, Belkin - 2020 - Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks.pdf:pdf},
journal = {arXiv preprint arXiv:2006.07322},
title = {{Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks}},
url = {http://arxiv.org/pdf/2006.07322.pdf},
year = {2020}
}
@inproceedings{eysenbach2019rf,
abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose “Diversity is All You Need”(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1802.06070},
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1802.06070},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eysenbach et al. - 2019 - Diversity is all you need Learning skills without a reward function(2).pdf:pdf},
title = {{Diversity is all you need: Learning skills without a reward function}},
url = {https://arxiv.org/pdf/1802.06070.pdf},
year = {2019}
}
@book{debnath2006integral,
abstract = {This book is an introduction and reference for the applications of integral transforms to a wide range of common mathematical problems. Emphasis is placed on the development of techniques and on the connection between properties of transforms and the kinds of problems for which they provide tools. Over 400 problems accompany the text, illustrating areas of applications. This third edition has been substantially updated, extended, and reorganized. Graduate students and researchers working in mathematics and physics will find this book useful.},
author = {Debnath, Lokenath and Bhatta, Dambaru},
doi = {10.1016/0377-2217(80)90103-4},
isbn = {9780387953144},
issn = {03772217},
publisher = {CRC Press},
title = {{Integral Transforms and Their Applications}},
url = {https://www.crcpress.com/Integral-Transforms-and-Their-Applications/Debnath-Bhatta/p/book/9781482223576},
year = {2006}
}
@inproceedings{simonyan2015vggnet,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
eprint = {1409.1556},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition.pdf:pdf},
title = {{Very deep convolutional networks for large-scale image recognition}},
url = {http://arxiv.org/pdf/1409.1556.pdf},
year = {2015}
}
@inproceedings{lobato2014pes,
abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2541},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
eprint = {1406.2541},
issn = {10495258},
title = {{Predictive entropy search for efficient global optimization of black-box functions}},
url = {http://arxiv.org/pdf/1406.2541.pdf},
year = {2014}
}
@inproceedings{domingos2000bias,
abstract = {This paper presents a uni ed bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassi cation costs, and other loss functions. The uni ed decomposition sheds light on a number of sig- ni cant issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.'s (1997) notion of margin, and the nature of the trade-o between bias and variance in classi cation. While the bias- variance behavior of zero-one loss and vari- able misclassi cation costs is quite di erent from that of squared loss, this di erence de- rives directly from the di erent de nitions of loss. We have applied the proposed decom- position to decision tree learning, instance- based learning and boosting on a large suite of benchmark data sets, and made several sig- ni cant observations.},
author = {Domingos, Pedro},
booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 2000 - A unified bias-variance decomposition and its applications.pdf:pdf},
isbn = {2065432969},
pages = {231--238},
title = {{A unified bias-variance decomposition and its applications}},
url = {https://homes.cs.washington.edu/{~}pedrod/papers/mlc00a.pdf},
year = {2000}
}
@unpublished{khan2019bayes,
abstract = {Machine-learning algorithms are commonly derived using ideas from optimization and statistics, followed by an extensive empirical efforts to make them practical as there is a lack of underlying principles to guide this process. In this paper, we present a learning rule derived from Bayesian principles, which enables us to connect a wide-variety of learning algorithms. Using this rule, we can derive a wide-range of learning-algorithms in fields such as probabilistic graphical models, continuous optimization, deep learning, reinforcement learning, online learning, and black-box optimization. This includes classical algorithms such as least-squares, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop and Adam. Overall, we show that Bayesian principles not only unify, generalize, and improve existing learning-algorithms, but also help us design new ones. [This is a working draft and a work in progress]},
author = {Khan, Mohammad Emtiyaz and Rue, Haavard},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Rue - 2019 - Learning-algorithms from Bayesian principles(2).pdf:pdf},
title = {{Learning-algorithms from Bayesian principles}},
url = {https://emtiyaz.github.io/papers/learning{\_}from{\_}bayes.pdf},
year = {2019}
}
@article{jones1998ei,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
doi = {10.1023/A:1008306431147},
eprint = {0005074v1},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
number = {4},
pages = {455--492},
pmid = {21858987},
primaryClass = {arXiv:astro-ph},
title = {{Efficient global optimization of expensive black-box functions}},
url = {http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/{\$}FILE/Jones98.pdf},
volume = {13},
year = {1998}
}
@article{metropolis1953,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
journal = {Journal of Chemical Physics},
number = {6},
pages = {1087--1092},
title = {{Equation of state calculations by fast computing machines}},
url = {https://bayes.wustl.edu/Manual/EquationOfState.pdf},
volume = {21},
year = {1953}
}
@inproceedings{gangwani2019sil,
abstract = {The success of popular algorithms for deep reinforcement learning, such as policygradients and Q-learning, relies heavily on the availability of an informative reward signal at each timestep of the sequential decision-making process. When rewards are only sparsely available during an episode, or a rewarding feedback is provided only after episode termination, these algorithms perform sub-optimally due to the difficultly in credit assignment. Alternatively, trajectory-based policy optimization methods, such as cross-entropy method and evolution strategies, do not require per-timestep rewards, but have been found to suffer from high sample complexity by completing forgoing the temporal nature of the problem. Improving the efficiency of RL algorithms in real-world problems with sparse or episodic rewards is therefore a pressing need. In this work, we introduce a self-imitation learning algorithm that exploits and explores well in the sparse and episodic reward settings. We view each policy as a state-action visitation distribution and formulate policy optimization as a divergence minimization problem. We show that with Jensen-Shannon divergence, this divergence minimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results indicate that our algorithm works comparable to existing algorithms in environments with dense rewards, and significantly better in environments with sparse and episodic rewards. We then discuss limitations of self-imitation learning, and propose to solve them by using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate its effectiveness on a challenging variant of continuous-control MuJoCo locomotion tasks.},
archivePrefix = {arXiv},
arxivId = {1805.10309},
author = {Gangwani, Tanmay and Liu, Qiang and Peng, Jian},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1805.10309},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gangwani, Liu, Peng - 2019 - Learning self-imitating diverse policies.pdf:pdf},
title = {{Learning self-imitating diverse policies}},
url = {https://arxiv.org/pdf/1805.10309.pdf},
year = {2019}
}
@inproceedings{sbai2018design,
abstract = {Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) a new loss function that encourages creativity, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture makers). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity loss yields better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61{\%} of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.},
archivePrefix = {arXiv},
arxivId = {1804.00921},
author = {Sbai, Othman and Elhoseiny, Mohamed and Bordes, Antoine and LeCun, Yann and Couprie, Camille},
booktitle = {Proceedings of the 15th European Conference on Computer Vision (ECCV)},
eprint = {1804.00921},
pages = {37--44},
title = {{DeSIGN: Design inspiration from generative networks}},
url = {http://arxiv.org/pdf/1804.00921.pdf},
year = {2018}
}
@article{azar2019world,
abstract = {As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.},
archivePrefix = {arXiv},
arxivId = {1902.07685},
author = {Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo Avila and Grill, Jean-Bastien and Altch{\'{e}}, Florent and Munos, R{\'{e}}mi},
eprint = {1902.07685},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azar et al. - 2019 - World discovery models.pdf:pdf},
journal = {arXiv preprint arXiv:1902.07685},
title = {{World discovery models}},
url = {http://arxiv.org/pdf/1902.07685.pdf},
year = {2019}
}
@inproceedings{gupta2020,
abstract = {We consider a novel multi-armed bandit framework where the rewards obtained by pulling the arms are functions of a common latent random variable. The correlation between arms due to the common random source can be used to design a generalized upper-confidence-bound (UCB) algorithm that identifies certain arms as {\$}non-competitive{\$}, and avoids exploring them. As a result, we reduce a {\$}K{\$}-armed bandit problem to a {\$}C+1{\$}-armed problem, where {\$}C+1{\$} includes the best arm and {\$}C{\$} {\$}competitive{\$} arms. Our regret analysis shows that the competitive arms need to be pulled {\$}\backslashmathcal{\{}O{\}}(\backslashlog T){\$} times, while the non-competitive arms are pulled only {\$}\backslashmathcal{\{}O{\}}(1){\$} times. As a result, there are regimes where our algorithm achieves a {\$}\backslashmathcal{\{}O{\}}(1){\$} regret as opposed to the typical logarithmic regret scaling of multi-armed bandit algorithms. We also evaluate lower bounds on the expected regret and prove that our correlated-UCB algorithm achieves {\$}\backslashmathcal{\{}O{\}}(1){\$} regret whenever possible.},
archivePrefix = {arXiv},
arxivId = {1808.05904},
author = {Gupta, Samarth and Joshi, Gauri and Yagan, Osman},
booktitle = {Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/icassp40776.2020.9054429},
eprint = {1808.05904},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Joshi, Yagan - 2020 - Correlated multi-armed bandits with a latent random source.pdf:pdf},
pages = {3572--3576},
title = {{Correlated multi-armed bandits with a latent random source}},
url = {https://arxiv.org/pdf/1808.05904.pdf},
year = {2020}
}
@inproceedings{zhang2018tropical,
abstract = {We establish, for the first time, connections between feedforward neural networks with ReLU activation and tropical geometry --- we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.},
archivePrefix = {arXiv},
arxivId = {1805.07091},
author = {Zhang, Liwen and Naitzat, Gregory and Lim, Lek-Heng},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1805.07091},
issn = {1938-7228},
title = {{Tropical geometry of deep neural networks}},
url = {https://arxiv.org/pdf/1805.07091.pdf},
year = {2018}
}
@inproceedings{roth2017regularization,
abstract = {Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.},
archivePrefix = {arXiv},
arxivId = {1705.09367},
author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
doi = {10.1007/s00138-014-0623-4},
eprint = {1705.09367},
isbn = {0013801406},
issn = {14321769},
pages = {2018--2028},
title = {{Stabilizing training of generative adversarial networks through regularization}},
url = {http://arxiv.org/pdf/1705.09367.pdf},
year = {2017}
}
@inproceedings{nomura2020msu,
abstract = {How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and somewhat relevant source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization.},
archivePrefix = {arXiv},
arxivId = {2006.10600},
author = {Nomura, Masahiro and Saito, Yuta},
booktitle = {7th Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
eprint = {2006.10600},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nomura, Saito - 2020 - Multi-source unsupervised hyperparameter optimization.pdf:pdf},
title = {{Multi-source unsupervised hyperparameter optimization}},
url = {http://arxiv.org/pdf/2006.10600.pdf},
year = {2020}
}
@inproceedings{deshmukh2019bai,
author = {Deshmukh, Aniket Anand and Sharma, Srinagesh and Cutler, James W. and Moldwin, Mark and Scott, Clayton},
booktitle = {2nd Workshop on Exploration in Reinforcement Learning at International Conference on Machine Learning (ICML-ERL)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deshmukh et al. - 2018 - Simple regret minimization for contextual bandits.pdf:pdf},
title = {{Simple regret minimization for contextual bandits}},
url = {https://arxiv.org/pdf/1810.07371.pdf},
year = {2019}
}
@article{burnetas1996optimal,
author = {Burnetas, Apostolos N. and Katehakis, Micha{\"{e}}l N.},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burnetas, Katehakis - 1996 - Optimal adaptive policies for sequential allocation problems.pdf:pdf},
journal = {Advances in Applied Mathematics},
keywords = {bandits},
mendeley-tags = {bandits},
number = {2},
pages = {122--142},
title = {{Optimal adaptive policies for sequential allocation problems}},
url = {https://core.ac.uk/download/pdf/82424539.pdf},
volume = {17},
year = {1996}
}
@inproceedings{azar2014online,
abstract = {In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time {\$}\backslashmathcal{\{}X{\}}{\$}-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.},
author = {Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azar, Lazaric, Brunskill - 2014 - Online stochastic optimization under correlated bandit feedback.pdf:pdf},
pages = {1557--1565},
title = {{Online stochastic optimization under correlated bandit feedback}},
url = {https://arxiv.org/pdf/1402.0562.pdf},
year = {2014}
}
@article{patil2019,
abstract = {We study an interesting variant of the stochastic multi-armed bandit problem, called the Fair-SMAB problem, where each arm is required to be pulled for at least a given fraction of the total available rounds. We investigate the interplay between learning and fairness in terms of a pre-specified vector denoting the fractions of guaranteed pulls. We define a fairness-aware regret, called {\$}r{\$}-Regret, that takes into account the above fairness constraints and naturally extends the conventional notion of regret. Our primary contribution is characterizing a class of Fair-SMAB algorithms by two parameters: the unfairness tolerance and the learning algorithm used as a black-box. We provide a fairness guarantee for this class that holds uniformly over time irrespective of the choice of the learning algorithm. In particular, when the learning algorithm is UCB1, we show that our algorithm achieves {\$}O(\backslashln T){\$} {\$}r{\$}-Regret. Finally, we evaluate the cost of fairness in terms of the conventional notion of regret.},
archivePrefix = {arXiv},
arxivId = {1907.10516},
author = {Patil, Vishakha and Ghalme, Ganesh and Nair, Vineet and Narahari, Y.},
eprint = {1907.10516},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patil et al. - 2019 - Achieving fairness in the stochastic multi-armed bandit problem.pdf:pdf},
journal = {arXiv preprint arXiv:1105.5041},
title = {{Achieving fairness in the stochastic multi-armed bandit problem}},
url = {http://arxiv.org/pdf/1907.10516.pdf},
year = {2019}
}
@article{degenne2018bridge,
author = {Degenne, R{\'{e}}my and Nedelec, Thomas and Calauz{\`{e}}nes, Cl{\'{e}}ment and Perchet, Vianney},
journal = {arXiv preprint arXiv:1810.04088},
title = {{Bridging the gap between regret minimization and best arm identification, with application to A/B tests}},
url = {https://arxiv.org/pdf/1810.04088.pdf},
year = {2018}
}
@article{bubeck2011pure,
abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck, Munos, Stoltz - 2011 - Pure exploration in finitely-armed and continuous-armed bandits.pdf:pdf},
journal = {Theoretical Computer Science},
keywords = {Continuous-armed bandits,Efficient exploration,Multi-armed bandits,Simple regret},
number = {19},
pages = {1832--1852},
title = {{Pure exploration in finitely-armed and continuous-armed bandits}},
url = {https://hal.archives-ouvertes.fr/hal-00257454v6/document},
volume = {412},
year = {2011}
}
@article{kleinberg2008multi,
author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
journal = {Symposium on Theory of Computing},
title = {{Multi-armed bandit problems in metric spaces}},
url = {https://arxiv.org/pdf/0809.4882.pdf},
year = {2008}
}
@article{Achiam2018,
abstract = {We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1807.10299},
author = {Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
eprint = {1807.10299},
journal = {arXiv},
title = {{Variational option discovery algorithms}},
year = {2018}
}
@inproceedings{jain2018firing,
abstract = {In this paper, we model the problem of optimizing crowdfunding platforms, such as the non-profit Kiva or for-profit KickStarter, as a variant of the multi-armed bandit problem. In our setting, Bernoulli arms emit no rewards until their cumulative number of successes over any number of trials exceeds a fixed threshold and then provides no additional reward for any additional trials-a process reminiscent to that of a neuron firing once it reaches the action potential and then saturates. In the spirit of an infinite armed bandit problem, the player can add new arms whose expected probability of success is drawn iid from an unknown distribution-this endless supply of projects models the harsh reality that the number of projects seeking funding greatly exceeds the total capital available by lenders. Crowdfunding platforms naturally fall under this setting where the arms are potential projects, and their probability of success is the probability that a potential funder decides to fund it after reviewing it. The goal is to play arms (prioritize the display of projects on a webpage) to maximize the number of arms that reach the firing threshold (meet their goal amount) using as few total trials (number of impressions) as possible over all the played arms. We provide an algorithm for this setting and prove sublinear regret bounds.},
author = {Jain, Lalit and Jamieson, Kevin},
booktitle = {Proceedings of the 35th international conference on Machine learning (ICML)},
title = {{Firing bandits : Optimizing crowdfunding}},
url = {https://homes.cs.washington.edu/{~}jamieson/resources/firing{\_}bandits.pdf},
year = {2018}
}
@article{guo2021geometric,
abstract = {Exploration is essential for solving complex Reinforcement Learning (RL) tasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration problem as a well-defined policy optimization problem whose solution aims at visiting all states as uniformly as possible. This is in contrast to standard uncertainty-based approaches where exploration is transient and eventually vanishes. However, existing approaches to MSVE are theoretically justified only for discrete state-spaces as they are oblivious to the geometry of continuous domains. We address this challenge by introducing Geometric Entropy Maximisation (GEM), a new algorithm that maximises the geometry-aware Shannon entropy of state-visits in both discrete and continuous domains. Our key theoretical contribution is casting geometry-aware MSVE exploration as a tractable problem of optimising a simple and novel noise-contrastive objective function. In our experiments, we show the efficiency of GEM in solving several RL problems with sparse rewards, compared against other deep RL exploration approaches.},
archivePrefix = {arXiv},
arxivId = {2101.02055},
author = {Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Saade, Alaa and Thakoor, Shantanu and Piot, Bilal and Pires, Bernardo Avila and Valko, Michal and Mesnard, Thomas and Lattimore, Tor and Munos, R{\'{e}}mi},
eprint = {2101.02055},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2021 - Geometric Entropic Exploration(2).pdf:pdf},
journal = {arXiv preprint arXiv:2101.02055},
title = {{Geometric Entropic Exploration}},
url = {http://arxiv.org/pdf/2101.02055.pdf},
year = {2021}
}
@inproceedings{wu2019multi-fidelity,
abstract = {Bayesian optimization is popular for optimizing time-consuming black-box objectives. Nonetheless, for hyperparameter tuning in deep neural networks, the time required to evaluate the validation error for even a few hyperparameter settings remains a bottleneck. Multi-fidelity optimization promises relief using cheaper proxies to such objectives --- for example, validation error for a network trained using a subset of the training points or fewer iterations than required for convergence. We propose a highly flexible and practical approach to multi-fidelity Bayesian optimization, focused on efficiently optimizing hyperparameters for iteratively trained supervised learning models. We introduce a new acquisition function, the trace-aware knowledge-gradient, which efficiently leverages both multiple continuous fidelity controls and trace observations --- values of the objective at a sequence of fidelities, available when varying fidelity using training iterations. We provide a provably convergent method for optimizing our acquisition function and show it outperforms state-of-the-art alternatives for hyperparameter tuning of deep neural networks and large-scale kernel learning.},
archivePrefix = {arXiv},
arxivId = {1903.04703},
author = {Wu, Jian and Toscano-Palmerin, Saul and Frazier, Peter I. and Wilson, Andrew Gordon},
booktitle = {Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)},
eprint = {1903.04703},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2019 - Practical multi-fidelity Bayesian optimization for hyperparameter tuning.pdf:pdf},
title = {{Practical multi-fidelity Bayesian optimization for hyperparameter tuning}},
url = {http://arxiv.org/pdf/1903.04703.pdf},
year = {2019}
}
@article{ahmed2018lbo,
author = {Ahmed, Mohamed Osama and Vaswani, Sharan and Schmidt, Mark},
journal = {arXiv preprint arXiv:1810.04336},
title = {{Combining Bayesian optimization and Lipschitz optimization}},
url = {https://arxiv.org/pdf/1810.04336.pdf},
year = {2018}
}
@article{judea2009causal,
author = {Judea, Pearl},
journal = {Statistics Surveys},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {https://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@inproceedings{metelli2019policy,
abstract = {Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.},
archivePrefix = {arXiv},
arxivId = {1809.06098},
author = {Metelli, Alberto Maria and Papini, Matteo and Faccio, Francesco and Restelli, Marcello},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1809.06098},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Metelli et al. - 2019 - Optimistic policy optimization via multiple importance sampling.pdf:pdf},
title = {{Optimistic policy optimization via multiple importance sampling}},
url = {http://arxiv.org/pdf/1809.06098.pdf},
year = {2019}
}
@book{bishop2006prml,
author = {Bishop, Christopher M.},
booktitle = {Information Science and Statistics},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
publisher = {Springer-Verlag New York},
title = {{Pattern Recognition and Machine Learning}},
url = {https://dl.acm.org/citation.cfm?id=1162264},
year = {2006}
}
@article{katariya2017,
abstract = {The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-1 matrix. We propose the learning problem of a Bernoulli rank-1 bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-1 bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on "benign" instances. These are the instances where the minimum of the average row and column rewards $\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as $\mu$ → 0. In this paper we propose Rank1ElimKL, which replaces the crude confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences. With the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of $\mu$. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim. Similarly, experiments with models derived from real-data confirm that the improvements are significant across the board, regardless of whether the data is benign or not.},
archivePrefix = {arXiv},
arxivId = {1703.06513},
author = {Katariya, Sumeet and Kveton, Branislav and Szepesv{\'{a}}ri, Csaba and Vernade, Claire and Wen, Zheng},
doi = {10.24963/ijcai.2017/278},
eprint = {1703.06513},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katariya et al. - 2017 - Bernoulli rank-1 bandits for click feedback.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)},
pages = {2001--2007},
title = {{Bernoulli rank-1 bandits for click feedback}},
url = {https://www.ijcai.org/Proceedings/2017/0278.pdf},
year = {2017}
}
@article{arlot2009cv,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its apparent universality. Many results exist on the model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
archivePrefix = {arXiv},
arxivId = {0907.4728},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
eprint = {0907.4728},
isbn = {1935-7516},
issn = {1935-7516},
journal = {Statistics Surveys},
pages = {40--79},
pmid = {1000183221},
title = {{A survey of cross-validation procedures for model selection}},
url = {https://arxiv.org/pdf/0907.4728.pdf},
volume = {4},
year = {2010}
}
@article{Author2021,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2021 - Pure exploration with structured preference feedback.pdf:pdf},
number = {Icml},
title = {{Pure exploration with structured preference feedback}},
year = {2021}
}
@inproceedings{liang2018buffer,
abstract = {We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimates. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization. Our key idea is to express the expected return objective as a weighted sum of two terms: An expectation over the high-reward trajectories inside a memory buffer, and a separate expectation over trajectories outside of the buffer. To design an efficient algorithm based on this idea, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to speed up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WIKITABLEQUESTIONS benchmark, we improve the state-of-the-art by 2:6{\%}, achieving an accuracy of 46:3{\%}. On the WIKISQL benchmark, MAPO achieves an accuracy of 74:9{\%} with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at goo.gl/TXBp4e.},
author = {Liang, Chen and Norouzi, Mohammad and Berant, Jonathan and Le, Quoc and Lao, Ni},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2018 - Memory augmented policy optimization for program synthesis and semantic parsing(2).pdf:pdf},
title = {{Memory augmented policy optimization for program synthesis and semantic parsing}},
url = {https://proceedings.neurips.cc/paper/2018/file/f4e369c0a468d3aeeda0593ba90b5e55-Paper.pdf},
year = {2018}
}
@article{kandasamy2017,
abstract = {We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive, but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making {\$}n{\$} evaluations distributed among {\$}M{\$} workers is essentially equivalent to performing {\$}n{\$} evaluations in sequence. Further, by modeling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronously parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in a hyper-parameter tuning application in convolutional neural networks. In addition to these, the proposed procedure is conceptually and computationally much simpler than existing work for parallel BO.},
archivePrefix = {arXiv},
arxivId = {1705.09236},
author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and Poczos, Barnabas},
eprint = {1705.09236},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kandasamy et al. - 2017 - Asynchronous parallel Bayesian optimisation via Thompson sampling.pdf:pdf},
journal = {arXiv preprint arXiv:1705.09236},
title = {{Asynchronous parallel Bayesian optimisation via Thompson sampling}},
url = {http://arxiv.org/pdf/1705.09236.pdf},
year = {2017}
}
@inproceedings{degenne2020game,
author = {Degenne, R{\'{e}}my and M{\'{e}}nard, Pierre and Shang, Xuedong and Valko, Michal},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
title = {{Gamification of pure exploration for linear bandits}},
url = {https://arxiv.org/pdf/2007.00953.pdf},
year = {2020}
}
@inproceedings{kwon2017blackwell,
abstract = {Blackwell approachability is an online learning setup generalizing the classical problem of regret minimization by allowing for instance multi-criteria optimization, global (online) optimization of a convex loss, or online linear optimization under some cumulative constraint. We consider partial monitoring where the decision maker does not necessarily observe the outcomes of his decision (unlike the traditional regret/bandit literature). Instead, he receives a random signal correlated to the decision–outcome pair, or only to the outcome. We construct, for the first time, approachability algorithms with convergence rate of order O(T−1/2) when the signal is independent of the decision and of order O(T−1/3) in the case of general signals. Those rates are optimal in the sense that they cannot be improved without further assumption on the structure of the objectives and/or the signals.},
author = {Kwon, Joon and Perchet, Vianney},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwon, Perchet - 2017 - Online learning and Blackwell approachability with partial monitoring Optimal convergence rates(2).pdf:pdf},
title = {{Online learning and Blackwell approachability with partial monitoring: Optimal convergence rates}},
url = {http://proceedings.mlr.press/v54/kwon17a/kwon17a.pdf},
volume = {54},
year = {2017}
}
@inproceedings{gregor2017empowerment,
abstract = {We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.},
archivePrefix = {arXiv},
arxivId = {1611.07507},
author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
booktitle = {Workshop Track of the 5th International Conference on Learning Representations},
eprint = {1611.07507},
title = {{Variational intrinsic control}},
url = {https://openreview.net/pdf?id=Skc-Fo4Yg},
year = {2017}
}
@inproceedings{zoran2011restoration,
abstract = {Learning good image priors is of utmost importance for the study of vision, computer vision and image processing applications. Learning priors and optimizing over whole images can lead to tremendous computational challenges. In contrast, when we work with small image patches, it is possible to learn priors and perform patch restoration very efficiently. This raises three questions - do priors that give high likelihood to the data also lead to good performance in restoration? Can we use such patch based priors to restore a full image? Can we learn better patch priors? In this work we answer these questions. We compare the likelihood of several patch models and show that priors that give high likelihood to data perform better in patch restoration. Motivated by this result, we propose a generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated. We show how to derive an appropriate cost function, how to optimize it and how to use it to restore whole images. Finally, we present a generic, surprisingly simple Gaussian Mixture prior, learned from a set of natural images. When used with the proposed framework, this Gaussian Mixture Model outperforms all other generic prior methods for image denoising, deblurring and inpainting.},
author = {Zoran, Daniel and Weiss, Yair},
booktitle = {Proceedings of the 13th IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2011.6126278},
isbn = {9781457711015},
issn = {1550-5499},
pages = {479--486},
pmid = {12684590},
title = {{From learning models of natural image patches to whole image restoration}},
url = {https://people.csail.mit.edu/danielzoran/EPLLICCVCameraReady.pdf},
year = {2011}
}
@article{szita2006cem,
abstract = {The cross-entropy method is an efficient and general optimization algorithm. However, its applicability in reinforcement learning (RL) seems to be limited because it often converges to suboptimal policies. We apply noise for preventing early convergence of the cross-entropy method, using Tetris, a computer game, for demonstration. The resulting policy outperforms previous RL algorithms by almost two orders of magnitude. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Szita, Istv{\'{a}}n and Lorincz, Andr{\'{a}}s},
doi = {10.1162/neco.2006.18.12.2936},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szita, Lorincz - 2006 - Learning tetris using the noisy cross-entropy method.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
keywords = {cross-entropy method,reinforcement learning,tetris},
number = {12},
pages = {2936--2941},
pmid = {17052153},
title = {{Learning tetris using the noisy cross-entropy method}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579{\&}rep=rep1{\&}type=pdf},
volume = {18},
year = {2006}
}
@inproceedings{kannan2018greedy,
abstract = {Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. This raises a fairness concern. In such settings, one might like to run a "greedy" algorithm, which always makes the (myopically) optimal decision for the individuals at hand - but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm. We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve "no regret", perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that "generically" (i.e. in slightly perturbed environments), exploration and exploitation need not be in conflict in the linear setting.},
archivePrefix = {arXiv},
arxivId = {1801.03423},
author = {Kannan, Sampath and Morgenstern, Jamie and Roth, Aaron and Waggoner, Bo and Wu, Zhiwei Steven},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1801.03423},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kannan et al. - 2018 - A smoothed analysis of the greedy algorithm for the linear contextual bandit problem(2).pdf:pdf},
pages = {2231--2241},
title = {{A smoothed analysis of the greedy algorithm for the linear contextual bandit problem}},
url = {http://arxiv.org/pdf/1801.03423.pdf},
year = {2018}
}
@book{csaba2009rl,
abstract = {The purpose of this tutorial is to provide an introduction to reinforcement learning (RL) at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a rigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section 1 presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section 2 the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section 3 gives a description of the most widely used reinforcement learning algorithms. These include TD($\lambda$) and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section 4 some of the ancillary issues in RL are briefly discussed, such as choosing an exploration strategy and an appropriate discount factor. The conclusion is given in Section 5. Finally, Section 6 is a glossary of commonly used terms followed by references in Section 7 and a bibliography of RL applications in Section 8. The tutorial structure is such that each section builds on the information provided in previous sections. It is assumed that the reader has some knowledge of learning algorithms that rely on gradient descent (such as the backpropagation of errors algorithm).},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szepesv{\'{a}}ri - 2009 - Reinforcement Learning A Tutorial Scope of Tutorial.pdf:pdf},
isbn = {9781608454921},
issn = {19394608},
keywords = {Markov Decision Processes,Monte-Carlo methods,PAC-learning,Q-learning,active learning,actor-critic methods,bias-variance tradeoff,function approximation,least-squares methods,natural gradient,online learning,overfitting,planning,policy gradient,reinforcement learning,simulation,simulation optimization,stochastic approximation,stochastic gradient methods,temporal difference learning,two-timescale stochastic approximation},
pages = {1--89},
publisher = {Morgan {\&} Claypool Publishers},
title = {{Algorithms for Reinforcement Learning}},
url = {https://sites.ualberta.ca/{~}szepesva/papers/RLAlgsInMDPs.pdf},
volume = {9},
year = {2009}
}
@article{mikolov2013word2vec,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient estimation of word representations in vector space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {arXiv preprint arXiv:1301.3781},
pmid = {18244602},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/pdf/1301.3781.pdf},
year = {2013}
}
@inproceedings{fu2017exemplar,
abstract = {Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.},
author = {Fu, Justin and Co-Reyes, John D. and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
title = {{EX2: Exploration with exemplar models for deep reinforcement learning}},
url = {https://arxiv.org/pdf/1703.01260.pdf},
year = {2017}
}
@article{arjovsky2017wgan,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1701.07875},
isbn = {9781845937591},
issn = {1701.07875},
journal = {arXiv preprint arXiv:1701.07875},
pmid = {19963286},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/pdf/1701.07875.pdf},
year = {2017}
}
@inproceedings{ostrovski2017count,
abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to nontabular reinforcement learning. This pseudocount was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
archivePrefix = {arXiv},
arxivId = {1703.01310},
author = {Ostrovski, Georg and Bellemare, Marc G. and {Van Den Oord}, Aaron and Munos, Remi},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.01310},
isbn = {9781510855144},
pages = {4161--4175},
title = {{Count-based exploration with neural density models}},
url = {https://arxiv.org/pdf/1703.01310.pdf},
year = {2017}
}
@inproceedings{mackay2019response,
archivePrefix = {arXiv},
arxivId = {1903.03088},
author = {MacKay, Matthew and Vicol, Paul and Lorraine, Jon and Duvenaud, David and Grosse, Roger},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1903.03088},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacKay et al. - 2019 - Self-tuning networks Bilevel Optimization of hyperparameters using structured best-response functions.pdf:pdf},
title = {{Self-tuning networks: Bilevel Optimization of hyperparameters using structured best-response functions}},
url = {https://arxiv.org/pdf/1903.03088.pdf},
year = {2019}
}
@inproceedings{shao2019,
author = {Shao, Weijia and Gei{\ss}ler, Christian and Sivrikaya, Fikret},
booktitle = {6th Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
title = {{Graduated optimisation of black-box function}},
url = {https://www.automl.org/wp-content/uploads/2019/06/automlws2019{\_}Paper16.pdf},
year = {2019}
}
@inproceedings{goodfellow2014gan,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
pages = {2672--2680},
title = {{Generative adversarial nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@inproceedings{blundell2015weight,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
booktitle = {Proceedings of the 32nd International conference on Machine Learning (ICML)},
eprint = {1505.05424},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - Weight uncertainty in neural networks.pdf:pdf},
title = {{Weight uncertainty in neural networks}},
url = {http://arxiv.org/pdf/1505.05424.pdf},
year = {2015}
}
@article{perchet2014blackwell,
abstract = {Blackwell approachability, regret minimization and calibration are three criteria used to evaluate a strategy (or an algorithm) in sequential decision problems, described as repeated games between a player and Nature. Although they have at first sight not much in common, links between them have been discovered: for instance, both consistent and calibrated strategies can be constructed by following, in some auxiliary game, an approachability strategy. We gather seminal and recent results, develop and generalize Blackwell's elegant theory in several directions. The final objectives is to show how approachability can be used as a basic powerful tool to exhibit a new class of intuitive algorithms, based on simple geometric properties. In order to be complete, we also prove that approachability can be seen as a byproduct of the very existence of consistent or calibrated strategies.},
archivePrefix = {arXiv},
arxivId = {1301.2663},
author = {Perchet, Vianney},
doi = {10.3934/jdg.2014.1.181},
eprint = {1301.2663},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perchet - 2014 - Approachability, regret and calibration Implications and equivalences.pdf:pdf},
issn = {21646074},
journal = {Journal of Dynamics and Games},
keywords = {Blackwell's approachability,Calibration,Regret minimization},
number = {2},
pages = {181--254},
title = {{Approachability, regret and calibration: Implications and equivalences}},
url = {https://arxiv.org/pdf/1301.2663.pdf},
volume = {1},
year = {2014}
}
@inproceedings{oh2018sil,
abstract = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.},
archivePrefix = {arXiv},
arxivId = {1806.05635},
author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1806.05635},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh et al. - 2018 - Self-imitation learning.pdf:pdf},
isbn = {9781510867963},
pages = {6214--6223},
title = {{Self-imitation learning}},
url = {https://arxiv.org/pdf/1806.05635.pdf},
year = {2018}
}
@inproceedings{slivkins2011taxonomy,
abstract = {The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploita- tion. In this setting, an online algorithm has a fixed set of alternatives (“arms”), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of re- cent work has focused on multi-armed bandits with (infinitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs. In this paper we consider the more realistic scenario in which the metric space is implicit – it is defined by the available structure but not revealed to the algorithm directly. Specifically, we assume that an algorithm is given a tree-based classification of arms. For any given problem instance such a classification implicitly defines a similarity metric space, but the numerical sim- ilarity information is not available to the algorithm. We provide an algorithm for this setting, whose performance guarantees (almost) match the best known guar- antees for the corresponding instance of the Lipschitz MAB problem.},
author = {Slivkins, Aleksandrs},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
isbn = {9781618395993},
pages = {1602--1610},
title = {{Multi-armed bandits on implicit metric spaces}},
url = {https://papers.nips.cc/paper/4332-multi-armed-bandits-on-implicit-metric-spaces.pdf},
year = {2011}
}
@inproceedings{russo2016ttts,
abstract = {This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation},
archivePrefix = {arXiv},
arxivId = {1602.08448},
author = {Russo, Daniel},
booktitle = {Proceedings of the 29th Annual Conference on Learning Theory (CoLT)},
eprint = {1602.08448},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russo - 2016 - Simple Bayesian algorithms for best arm identification.pdf:pdf},
title = {{Simple Bayesian algorithms for best arm identification}},
url = {http://arxiv.org/pdf/1602.08448.pdf},
year = {2016}
}
@inproceedings{zuluaga2013,
abstract = {In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal trade-offs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm which intelligently samples the de-sign space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accomodate noisy evaluation; (2) a method to carefully choose the next design to evalu-ate to maximize progress; and (3) the abil-ity to control prediction accuracy and sam-pling cost. We provide theoretical bounds on PAL's sampling cost required to achieve a de-sired accuracy. Further, we show an exper-imental evaluation on three real-world data sets. The results show PAL's effectiveness; in particular it improves significantly over a state-of-the-art multi-objective optimization method, saving in many cases about 33{\%} evaluations to achieve the same accuracy.},
author = {Zuluaga, Marcela and Krause, Andreas and Sergent, Guillaume and Puschel, Markus},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuluaga et al. - 2013 - Active learning for multi-criterion optimization.pdf:pdf},
title = {{Active learning for multi-criterion optimization}},
url = {https://las.inf.ethz.ch/files/zuluaga13active-long.pdf},
year = {2013}
}
@article{dwork2006,
abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius‚{\"{A}}{\^{o}} goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy , which, intuitively, captures the increased risk to one‚{\"{A}}{\^{o}}s privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dwork, Cynthia},
doi = {10.1007/11787006_1},
eprint = {arXiv:1011.1669v3},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork - 2006 - Differential privacy.pdf:pdf},
isbn = {3540359079},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {1--12},
pmid = {191},
title = {{Differential privacy}},
volume = {4052 LNCS},
year = {2006}
}
@article{even-dar2006confidence,
abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O ((n/$\epsilon$ 2) log(1/$\delta$)) times to find an $\epsilon$-optimal arm with probability of at least 1 - $\delta$. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over $\epsilon$-greedy Q-learning.},
author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-Dar, Mannor, Mansour - 2006 - Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning prob.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
pages = {1079--1105},
title = {{Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems}},
url = {https://jmlr.csail.mit.edu/papers/volume7/evendar06a/evendar06a.pdf},
volume = {7},
year = {2006}
}
@article{hinz2018speeding,
abstract = {Most modern learning algorithms require the practitioner to manually set the values of many hyperparameters before the learning process can begin. However, with modern algo-rithms the evaluation of a given hyperparameter setting can take a considerable amount of time and the search space is often very high-dimensional. We propose the usage of a genetic algorithm to optimize the hyperparameters specifically for convolutional neural networks (CNNs). Additionally, we suggest using a lower-dimensional representation of the original data to quickly identify promising areas in the hyperparameter space. We compare the re-sults of the genetic algorithm and the hyperparameter optimization on lower-dimensional inputs to random search and the " Tree of Parzen Estimators " (TPE) algorithm. Our experiments show that the genetic algorithm finds hyperparameters that perform similar or better than those found by random search and the TPE algorithm. Additionally, they indicate that it is possible to speed up the optimization process by using lower-dimensional data representations at the beginning while increasing the dimensionality of the input later in the optimization process. This is independent of the underlying optimization procedure and considerably speeds up the hyperparameter optimization process, making the approach promising for future hyperparameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1807.07362},
author = {Hinz, Tobias and Navarro-Guerrero, Nicol{\'{a}}s and Magg, Sven and Wermter, Stefan},
doi = {10.1142/S1469026818500086},
eprint = {1807.07362},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinz et al. - 2018 - Speeding up the hyperparameter optimization of deep convolutional neural networks(2).pdf:pdf},
issn = {14690268},
journal = {International Journal of Computational Intelligence and Applications},
keywords = {convolutional neural networks,evolutionary algorithm,genetic algorithm,hyperparameter importance,hyperparameter optimization},
number = {2},
title = {{Speeding up the hyperparameter optimization of deep convolutional neural networks}},
url = {https://arxiv.org/pdf/1807.07362.pdf},
volume = {17},
year = {2018}
}
@inproceedings{osband2016,
abstract = {Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as ∈-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
archivePrefix = {arXiv},
arxivId = {1602.04621},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and {Van Roy}, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
eprint = {1602.04621},
issn = {10495258},
pages = {4033--4041},
title = {{Deep exploration via bootstrapped DQN}},
year = {2016}
}
@article{zhou2019ablr,
abstract = {To solve a machine learning problem, one typically needs to perform data preprocessing, modeling, and hyperparameter tuning, which is known as model selection and hyperparameter optimization.The goal of automated machine learning (AutoML) is to design methods that can automatically perform model selection and hyperparameter optimization without human interventions for a given dataset. In this paper, we propose a meta-learning method that can search for a high-performance machine learning pipeline from the predefined set of candidate pipelines for supervised classification datasets in an efficient way by leveraging meta-data collected from previous experiments. More specifically, our method combines an adaptive Bayesian regression model with a neural network basis function and the acquisition function from Bayesian optimization. The adaptive Bayesian regression model is able to capture knowledge from previous meta-data and thus make predictions of the performances of machine learning pipelines on a new dataset. The acquisition function is then used to guide the search of possible pipelines based on the predictions.The experiments demonstrate that our approach can quickly identify high-performance pipelines for a range of test datasets and outperforms the baseline methods.},
archivePrefix = {arXiv},
arxivId = {1904.00577},
author = {Zhou, Weilin and Precioso, Frederic},
eprint = {1904.00577},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Precioso - 2019 - Adaptive Bayesian linear regression for automated machine learning.pdf:pdf},
journal = {arXiv preprint arXiv:1904.00577},
title = {{Adaptive Bayesian linear regression for automated machine learning}},
url = {http://arxiv.org/pdf/1904.00577.pdf},
year = {2019}
}
@book{gittins2011,
abstract = {In 1989 the first edition of this book set out Gittins' pioneering index solution to the multi-armed bandit problem and his subsequent investigation of a wide class of sequential resource allocation and stochastic scheduling problems. Since then there has been a remarkable flowering of new insights, generalizations and applications, to which Glazebrook and Weber have made major contributions. This second edition brings the story up to date. There are new chapters on the achievable region approach to stochastic optimization problems, the construction of performance bounds for suboptimal policies, Whittle's restless bandits, and the use of Lagrangian relaxation in the construction and evaluation of index policies. Some of the many varied proofs of the index theorem are discussed along with the insights that they provide. Many contemporary applications are surveyed, and over 150 new references are included. Over the past 40 years the Gittins index has helped theoreticians and practitioners to address a huge variety of problems within chemometrics, economics, engineering, numerical analysis, operational research, probability, statistics and website design. This new edition will be an important resource for others wishing to use this approach.},
author = {Gittins, John and Glazebrook, Kevin and Weber, Richard},
doi = {10.1057/jors.1989.200},
edition = {2nd},
isbn = {9780470670026},
issn = {14769360},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Multi-armed Bandit Allocation Indices}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470980033},
year = {2011}
}
@inproceedings{hoffman2014bayesgap,
abstract = {We address the problem of finding the maximizer of a$\backslash$nnonlinear function that can only be evaluated,$\backslash$nsubject to noise, at a finite number of query$\backslash$nlocations. Further, we will assume that there is a$\backslash$nconstraint on the total number of permitted function$\backslash$nevaluations. We introduce a Bayesian approach for$\backslash$nthis problem and show that it empirically$\backslash$noutperforms both the existing frequentist$\backslash$ncounterpart and other Bayesian optimization methods.$\backslash$nThe Bayesian approach places emphasis on detailed$\backslash$nmodelling, including the modelling of correlations$\backslash$namong the arms. As a result, it can perform well in$\backslash$nsituations where the number of arms is much larger$\backslash$nthan the number of allowed function evaluation,$\backslash$nwhereas the frequentist counterpart is inapplicable.$\backslash$nThis feature enables us to develop and deploy$\backslash$npractical applications, such as automatic machine$\backslash$nlearning toolboxes. The paper presents$\backslash$ncomprehensive comparisons of the proposed approach$\backslash$nwith many Bayesian and bandit optimization$\backslash$ntechniques, the first comparison of many of these$\backslash$nmethods in the literature.},
author = {Hoffman, Matthew W. and Shahriari, Bobak and de Freitas, Nando},
booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Shahriari, de Freitas - 2014 - On correlation and budget constraints in model-based bandit optimization with application to (2).pdf:pdf},
issn = {15337928},
pages = {365--374},
title = {{On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning}},
url = {http://proceedings.mlr.press/v33/hoffman14.pdf},
year = {2014}
}
@inproceedings{khan2017vi,
abstract = {Variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms. Methods specifically designed for conjugate models, even though computationally efficient, find it difficult to deal with non-conjugate terms. On the other hand, stochastic-gradient methods can handle the nonconjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence. In this paper, we propose a new algorithm called Conjugate-computation Variational Inference (CVI) which brings the best of the two worlds together - it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest. We derive this algorithm by using a stochastic mirrordescent method in the mean-parameter space, and then expressing each gradient step as a variational inference in a conjugate model. We demonstrate our algorithm's applicability to a large class of models and establish its convergence. Our experimental results show that our method converges much faster than the methods that ignore the conjugate structure of the model.},
archivePrefix = {arXiv},
arxivId = {1703.04265},
author = {Khan, Mohammad Emtiyaz and Lin, Wu},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1703.04265},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Lin - 2017 - Conjugate-computation variational inference Converting variational inference in non-conjugate models to inferences i.pdf:pdf},
title = {{Conjugate-computation variational inference : Converting variational inference in non-conjugate models to inferences in conjugate models}},
url = {https://arxiv.org/pdf/1703.04265.pdf},
volume = {54},
year = {2017}
}
@article{arora2016embedding,
abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are cre-ated by diverse methods. Many use non-linear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The method-ological novelty is to use the prior to com-pute closed form expressions for word statis-tics. This provides a theoretical justifica-tion for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparame-ter choices. It also helps explain why low-dimensional semantic embeddings contain lin-ear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the gen-erative model assumptions, the most impor-tant of which is that latent word vectors are fairly uniformly dispersed in space.},
archivePrefix = {arXiv},
arxivId = {1502.03520},
author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
doi = {10.1109/TKDE.2018.2807452},
eprint = {1502.03520},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora et al. - 2016 - A latent variable model approach to PMI-based word embeddings.pdf:pdf},
isbn = {9781479936564},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
pages = {385--399},
title = {{A latent variable model approach to PMI-based word embeddings}},
url = {http://aclweb.org/anthology/Q16-1028},
volume = {4},
year = {2016}
}
@article{brown2019,
abstract = {In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.},
author = {Brown, Noam and Sandholm, Tuomas},
doi = {10.1126/science.aay2400},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Sandholm - 2019 - Superhuman AI for multiplayer poker.pdf:pdf},
issn = {0036-8075},
journal = {Science},
title = {{Superhuman AI for multiplayer poker}},
url = {https://www.cs.cmu.edu/{~}noamb/papers/19-Science-Superhuman.pdf},
volume = {eaay2400},
year = {2019}
}
@inproceedings{srinivas2010gpucb,
abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
archivePrefix = {arXiv},
arxivId = {0912.3995},
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
booktitle = {Proceedings of the 27th International conference on Machine Learning (ICML)},
doi = {10.1109/TIT.2011.2182033},
eprint = {0912.3995},
isbn = {9781605589077},
issn = {00189448},
pages = {1015--1022},
title = {{Gaussian process optimization in the bandit setting: No regret and experimental design}},
url = {https://arxiv.org/pdf/0912.3995.pdf},
year = {2010}
}
@inproceedings{zhu2020,
abstract = {The multi-armed bandit (MAB) problem is a classical learning task that exemplifies the exploration-exploitation tradeoff. However, standard formulations do not take into account {\{}$\backslash$em risk{\}}. In online decision making systems, risk is a primary concern. In this regard, the mean-variance risk measure is one of the most common objective functions. Existing algorithms for mean-variance optimization in the context of MAB problems have unrealistic assumptions on the reward distributions. We develop Thompson Sampling-style algorithms for mean-variance MAB and provide comprehensive regret analyses for Gaussian and Bernoulli bandits with fewer assumptions. Our algorithms achieve the best known regret bounds for mean-variance MABs and also attain the information-theoretic bounds in some parameter regimes. Empirical simulations show that our algorithms significantly outperform existing LCB-based algorithms for all risk tolerances.},
archivePrefix = {arXiv},
arxivId = {2002.00232},
author = {Zhu, Qiuyu and Tan, Vincent Y. F.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
eprint = {2002.00232},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Tan - 2020 - Thompson Sampling Algorithms for Mean-Variance Bandits.pdf:pdf},
title = {{Thompson sampling algorithms for mean-variance bandits}},
url = {http://arxiv.org/pdf/2002.00232.pdf},
year = {2020}
}
@article{agrawal1995continuum,
abstract = {In this paper we consider the multiarmed bandit problem where the arms are chosen from a subset of the real line and the mean rewards are assumed to be a continuous function of the arms. The problem with an infinite number of arms is much more difficult than the usual one with a finite number of arms because the built-in learning task is now infinite dimensional. We devise a kernel estimator-based learning scheme for the mean reward as a function of the arms. Using this learning scheme, we construct a class of certainty equivalence control with forcing schemes and derive asymptotic upper bounds on their learning loss. To the best of our knowledge, these bounds are the strongest rates yet available. Moreover, they are stronger than the o(n) required for optimality with respect to the average-cost-per-unit-time criterion.},
author = {Agrawal, Rajeev},
doi = {10.1137/S0363012992237273},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
keywords = {bandit problems,certainty,continuous arms,controlled i.i.d,equivalence with forcing,learning loss,process,stochastic adaptive control},
number = {6},
pages = {1926--1951},
title = {{The continuum-armed bandit problem}},
url = {https://epubs.siam.org/doi/pdf/10.1137/S0363012992237273},
volume = {33},
year = {1995}
}
@inproceedings{redko2017ot,
abstract = {Domain adaptation (DA) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training (source domain) and test (target domain) data are similar but different. This kind of learning paradigm is of vital importance for future advances as it allows a learner to generalize the knowledge across different tasks. Current theoretical results show that the efficiency of DA algorithms depends on their capacity of minimizing the divergence between source and target probability distributions. In this paper, we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory [17] can bring to DA. In particular, we show that the Wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings: (i) classic DA with unsupervised target data (ii) DA combining source and target labeled data, (iii) multiple source DA. Based on the obtained results, we motivate the use of the regularized optimal transport and provide some algorithmic insights for multi-source domain adaptation. We also show when this theoretical analysis can lead to tighter inequalities than those of other existing frameworks. We believe that these results open the door to novel ideas and directions for DA.},
archivePrefix = {arXiv},
arxivId = {1610.04420},
author = {Redko, Ievgen and Habrard, Amaury and Sebban, Marc},
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2017 (ECML-PKDD)},
doi = {10.1007/978-3-319-71246-8_45},
eprint = {1610.04420},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redko, Habrard, Sebban - 2017 - Theoretical analysis of domain adaptation with optimal transport.pdf:pdf},
isbn = {9783319712451},
issn = {16113349},
keywords = {Domain adaptation,Generalization bounds,Optimal transport},
pages = {737--753},
title = {{Theoretical analysis of domain adaptation with optimal transport}},
url = {https://arxiv.org/pdf/1610.04420.pdf},
year = {2017}
}
@inproceedings{parker-holder2020pb2,
author = {Parker-holder, Jack and Nguyen, Vu and Roberts, Stephen},
booktitle = {7th Workshop on Automated Machine Learning at International Conference on Machine Learning (ICML-AutoML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parker-holder, Nguyen, Roberts - 2020 - Provably efficient online hyperparameter optimization with population-based bandits.pdf:pdf},
pages = {1--16},
title = {{Provably efficient online hyperparameter optimization with population-based bandits}},
year = {2020}
}
@inproceedings{jamieson2016hyperband,
abstract = {Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identification problem. Within the multi-armed bandit literature, the cumulative regret objective enjoys algorithms and analyses for both the non-stochastic and stochastic settings while to the best of our knowledge, the best-arm identification framework has only been considered in the stochastic setting. We introduce the non-stochastic setting under this framework, identify a known algorithm that is well-suited for this setting, and analyze its behavior. Next, by leveraging the iterative nature of standard machine learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification, and empirically evaluate our proposed algorithm on this task. Our empirical results show that, by allocating more resources to promising hyperparameter settings, we typically achieve comparable test accuracies an order of magnitude faster than baseline methods.},
archivePrefix = {arXiv},
arxivId = {1502.07943},
author = {Jamieson, Kevin and Talwalkar, Ameet},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1502.07943},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jamieson, Talwalkar - 2016 - Non-stochastic best arm identification and hyperparameter optimization.pdf:pdf},
title = {{Non-stochastic best arm identification and hyperparameter optimization}},
url = {http://arxiv.org/pdf/1502.07943.pdf},
year = {2016}
}
@inproceedings{kaufmann2021rf,
abstract = {Reward-free exploration is a reinforcement learning setting recently studied by Jin et al., who address it by running several algorithms with regret guarantees in parallel. In our work, we instead propose a more adaptive approach for reward-free exploration which directly reduces upper bounds on the maximum MDP estimation error. We show that, interestingly, our reward-free UCRL algorithm can be seen as a variant of an algorithm of Fiechter from 1994, originally proposed for a different objective that we call best-policy identification. We prove that RF-UCRL needs {\$}\backslashmathcal{\{}O{\}}\backslashleft(({\{}SAH{\^{}}4{\}}/{\{}\backslashvarepsilon{\^{}}2{\}})\backslashln(1/\backslashdelta)\backslashright){\$} episodes to output, with probability {\$}1-\backslashdelta{\$}, an {\$}\backslashvarepsilon{\$}-approximation of the optimal policy for any reward function. We empirically compare it to oracle strategies using a generative model.},
archivePrefix = {arXiv},
arxivId = {2006.06294},
author = {Kaufmann, Emilie and M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
booktitle = {Proceedings of the 32nd International Conference on Algorithmic Learning Theory (ALT)},
eprint = {2006.06294},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann et al. - 2020 - Adaptive reward-free exploration.pdf:pdf},
title = {{Adaptive reward-free exploration}},
url = {http://arxiv.org/pdf/2006.06294.pdf},
year = {2021}
}
@article{chowdhury2020kernel,
abstract = {We consider the regret minimization problem in reinforcement learning (RL) in the episodic setting. In many real-world RL environments, the state and action spaces are continuous or very large. Existing approaches establish regret guarantees by either a low-dimensional representation of the stochastic transition model or an approximation of the {\$}Q{\$}-functions. However, the understanding of function approximation schemes for state-value functions largely remains missing. In this paper, we propose an online model-based RL algorithm, namely the CME-RL, that learns representations of transition distributions as embeddings in a reproducing kernel Hilbert space while carefully balancing the exploitation-exploration tradeoff. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order {\$}\backslashtilde{\{}O{\}}\backslashbig(H\backslashgamma{\_}N\backslashsqrt{\{}N{\}}\backslashbig){\$}, where {\$}H{\$} is the episode length, {\$}N{\$} is the total number of time steps and {\$}\backslashgamma{\_}N{\$} is an information theoretic quantity relating the effective dimension of the state-action feature space. Our method bypasses the need for estimating transition probabilities and applies to any domain on which kernels can be defined. It also brings new insights into the general theory of kernel methods for approximate inference and RL regret minimization.},
archivePrefix = {arXiv},
arxivId = {2011.07881},
author = {Chowdhury, Sayak Ray and Oliveira, Rafael},
eprint = {2011.07881},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chowdhury, Oliveira - 2020 - No-regret reinforcement learning with value function approximation A kernel embedding approach(2).pdf:pdf},
journal = {arXiv preprint arXiv:2011.07881},
keywords = {kernel mean embedding,model-based rl,value function approximation},
title = {{No-regret reinforcement learning with value function approximation: A kernel embedding approach}},
url = {http://arxiv.org/pdf/2011.07881.pdf},
year = {2020}
}
@article{zahavy2019deeplinear,
abstract = {We study the neural-linear bandit model for solving sequential decision-making problems with high dimensional side information. Neural-linear bandits leverage the representation power of deep neural networks and combine it with efficient exploration mechanisms, designed for linear contextual bandits, on top of the last hidden layer. Since the representation is being optimized during learning, information regarding exploration with "old" features is lost. Here, we propose the first limited memory neural-linear bandit that is resilient to this phenomenon, which we term catastrophic forgetting. We evaluate our method on a variety of real-world data sets, including regression, classification, and sentiment analysis, and observe that our algorithm is resilient to catastrophic forgetting and achieves superior performance.},
archivePrefix = {arXiv},
arxivId = {1901.08612},
author = {Zahavy, Tom and Mannor, Shie},
eprint = {1901.08612},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zahavy, Mannor - 2019 - Deep neural linear bandits Overcoming catastrophic forgetting through likelihood matching.pdf:pdf},
journal = {arXiv preprint arXiv:1901.08612},
title = {{Deep neural linear bandits: Overcoming catastrophic forgetting through likelihood matching}},
url = {http://arxiv.org/pdf/1901.08612.pdf},
year = {2019}
}
@article{fraccaroli2020,
abstract = {Deep learning techniques play an increasingly important role in industrial and research environments due to their outstanding results. However, the large number of hyper-parameters to be set may lead to errors if they are set manually. The state-of-the-art hyper-parameters tuning methods are grid search, random search, and Bayesian Optimization. The first two methods are expensive because they try, respectively, all possible combinations and random combinations of hyper-parameters. Bayesian Optimization, instead, builds a surrogate model of the objective function, quantifies the uncertainty in the surrogate using Gaussian Process Regression and uses an acquisition function to decide where to sample the new set of hyper-parameters. This work faces the field of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian Optimization applied to Deep Neural Networks. For this goal, we build a new algorithm for evaluating and analyzing the results of the network on the training and validation sets and use a set of tuning rules to add new hyper-parameters and/or to reduce the hyper-parameter search space to select a better combination.},
archivePrefix = {arXiv},
arxivId = {2006.02105},
author = {Fraccaroli, Michele and Lamma, Evelina and Riguzzi, Fabrizio},
eprint = {2006.02105},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fraccaroli, Lamma, Riguzzi - 2020 - Automatic setting of DNN hyper-parameters by mixing Bayesian optimization and tuning rules.pdf:pdf},
journal = {arXiv preprint arXiv:2006.02105},
title = {{Automatic setting of DNN hyper-parameters by mixing Bayesian optimization and tuning rules}},
url = {http://arxiv.org/pdf/2006.02105.pdf},
year = {2020}
}
@inproceedings{finn2017maml,
abstract = {We propose an algorithm for mcta-lcaming that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.03400},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - 2017 - Model-agnostic meta-learning for fast adaptation of deep networks.pdf:pdf},
isbn = {9781510855144},
pages = {1856--1868},
title = {{Model-agnostic meta-learning for fast adaptation of deep networks}},
url = {https://arxiv.org/pdf/1703.03400.pdf},
year = {2017}
}
@inproceedings{kaul2017feature,
abstract = {In recent years, the importance of feature engineering has been confirmed by the exceptional performance of deep learning techniques, that automate this task for some applications. For others, feature engineering requires substantial manual effort in designing and selecting features and is often tedious and non-scalable. We present AutoLearn, a regression-based feature learning algorithm. Being data-driven, it requires no domain knowledge and is hence generic. Such a representation is learnt by mining pairwise feature associations, identifying the linear or non-linear relationship between each pair, applying regression and selecting those relationships that are stable and improve the prediction performance. Our experimental evaluation on 18 UC Irvine and 7 Gene expression datasets, across different domains, provides evidence that the features learnt through our model can improve the overall prediction accuracy by 13.28{\%}, compared to original feature space and 5.87{\%} over other top performing models, across 8 different classifiers without using any domain knowledge.},
author = {Kaul, Ambika and Maheshwary, Saket and Pudi, Vikram},
booktitle = {Proceedings of the 17th IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2017.31},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaul, Maheshwary, Pudi - 2017 - Autolearn - Automated feature generation and selection.pdf:pdf},
isbn = {9781538638347},
issn = {15504786},
keywords = {Classification,Feature Generation,Feature Selection},
pages = {217--226},
title = {{Autolearn - Automated feature generation and selection}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8215494},
year = {2017}
}
@inproceedings{vaswani2017attention,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1706.03762},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
title = {{Attention is all you need}},
url = {http://arxiv.org/pdf/1706.03762.pdf},
year = {2017}
}
@book{sutton1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://www.incompleteideas.net/book/bookdraft2018mar21.pdf},
year = {1998}
}
@inproceedings{odonoghue2020rlpi,
abstract = {Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference' and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, annd clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular ‘RL as inference' approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.},
archivePrefix = {arXiv},
arxivId = {2001.00805},
author = {O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},
booktitle = {Proceedings of the 8th International Conference on Learning Representations (ICLR)},
eprint = {2001.00805},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Donoghue, Osband, Ionescu - 2020 - Making sense of reinforcement learning and probabilistic inference.pdf:pdf},
title = {{Making sense of reinforcement learning and probabilistic inference}},
url = {https://arxiv.org/pdf/2001.00805.pdf},
year = {2020}
}
@inproceedings{skilling2004nested,
abstract = {"The evidence Z is often the single most important number in the [Bayesian] problem and I think every effort should be devoted to calculating it" (MacKay 2003). Nested sampling does this by giving a direct estimate of the density of states. Posterior samples are an optional byproduct.},
archivePrefix = {arXiv},
arxivId = {1306.2144},
author = {Skilling, John},
booktitle = {AIP Conference Proceedings},
doi = {10.1063/1.1835238},
eprint = {1306.2144},
isbn = {0735402175},
issn = {0094243X},
pages = {395--405},
pmid = {25246403},
title = {{Nested sampling}},
url = {http://aip.scitation.org/doi/abs/10.1063/1.1835238},
volume = {735},
year = {2004}
}
@article{knudde2017gpflowopt,
abstract = {A novel Python framework for Bayesian optimization known as GPflowOpt is introduced. The package is based on the popular GPflow library for Gaussian processes, leveraging the benefits of TensorFlow including automatic differentiation, parallelization and GPU computations for Bayesian optimization. Design goals focus on a framework that is easy to extend with custom acquisition functions and models. The framework is thoroughly tested and well documented, and provides scalability. The current released version of GPflowOpt includes some standard single-objective acquisition functions, the state-of-the-art max-value entropy search, as well as a Bayesian multi-objective approach. Finally, it permits easy use of custom modeling strategies implemented in GPflow.},
archivePrefix = {arXiv},
arxivId = {1711.03845},
author = {Knudde, Nicolas and van der Herten, Joachim and Dhaene, Tom and Couckuyt, Ivo},
eprint = {1711.03845},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knudde et al. - 2017 - GPflowOpt A Bayesian optimization library using TensorFlow.pdf:pdf},
isbn = {0000000153225},
journal = {arXiv preprint arXiv:1711.03845},
title = {{GPflowOpt: A Bayesian optimization library using TensorFlow}},
url = {http://arxiv.org/pdf/1711.03845.pdf},
year = {2017}
}
@inproceedings{qin2017ttei,
abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1705.10033},
author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1705.10033},
issn = {10495258},
pages = {5381--5391},
title = {{Improving the expected improvement algorithm}},
url = {http://arxiv.org/pdf/1705.10033.pdf},
year = {2017}
}
@inproceedings{grill2016trail,
abstract = {We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e. the sample complexity. We propose a new algorithm, TrailBlazer, able to handle MDPs with a finite or an infinite number of transitions from state-action to next states. TrailBlazer is an adaptive algorithm that exploits possible structures of the MDP by exploring only a subset of states reachable by following near-optimal policies. We provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states. The algorithm behavior can be considered as an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). Finally, another appealing feature of TrailBlazer is that it is simple to implement and computationally efficient.},
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
issn = {10495258},
pages = {4680--4688},
title = {{Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning}},
url = {https://papers.nips.cc/paper/6253-blazing-the-trails-before-beating-the-path-sample-efficient-monte-carlo-planning.pdf},
year = {2016}
}
@inproceedings{kanter2015synthesis,
author = {Kanter, James M. and Veeramachaneni, Kalyan},
booktitle = {Proceedings of the 2nd IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
doi = {10.1097/AOG.0b013e318195baef},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanter, Veeramachaneni - 2015 - Deep feature synthesis Towards automating data science endeavors.pdf:pdf},
isbn = {9781467382731},
issn = {00297844},
pages = {1--10},
pmid = {19155899},
title = {{Deep feature synthesis: Towards automating data science endeavors}},
url = {https://www.jmaxkanter.com/static/papers/DSAA{\_}DSM{\_}2015.pdf},
year = {2015}
}
@article{achiam2017surprise,
abstract = {Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as {\$}\backslashepsilon{\$}-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the {\$}k{\$}-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.},
archivePrefix = {arXiv},
arxivId = {1703.01732},
author = {Achiam, Joshua and Sastry, Shankar},
eprint = {1703.01732},
journal = {arXiv preprint arXiv:1308.0850},
title = {{Surprise-based intrinsic motivation for deep reinforcement learning}},
url = {http://arxiv.org/pdf/1703.01732.pdf},
year = {2017}
}
@article{Reda2020,
abstract = {Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research.},
author = {R{\'{e}}da, Cl{\'{e}}mence and Kaufmann, Emilie and Delahaye-Duriez, Andr{\'{e}}e},
doi = {10.1016/j.csbj.2019.12.006},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/R{\'{e}}da, Kaufmann, Delahaye-Duriez - 2020 - Machine learning applications in drug development.pdf:pdf},
issn = {20010370},
journal = {Computational and Structural Biotechnology Journal},
keywords = {Adaptive clinical trial,Bayesian optimization,Collaborative filtering,Drug discovery,Drug repurposing,Multi-armed bandit},
pages = {241--252},
title = {{Machine learning applications in drug development}},
url = {https://reader.elsevier.com/reader/sd/pii/S2001037019303988?token=E2EEC41F5137D290418F4D61D466997FB5A349136AE7F44B395994AD78B52E4651DA6A5DE758BBD7B96924F2D34D2E70},
volume = {18},
year = {2020}
}
@inproceedings{ciucanu2019secure,
author = {Ciucanu, Radu and Lafourcade, Pascal and Lombard-platet, Marius and Soare, Marta},
booktitle = {Proceedings of the 15th International Conference on Information Security Practice and Experience (ISPEC)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ciucanu et al. - 2019 - Secure best arm identification in multi-armed bandits.pdf:pdf},
number = {826404},
title = {{Secure best arm identification in multi-armed bandits}},
url = {https://sancy.iut-clermont.uca.fr/{~}lafourcade/PAPERS/PDF/CLLS19.pdf},
year = {2019}
}
@inproceedings{badanidiyuru2014resource,
abstract = {We study contextual bandits with ancillary constraints on resources, which are common in realworld applications such as choosing ads or dynamic pricing of items. We design the first algorithm for solving these problems that improves over a trivial reduction to the non-contextual case. We consider very general settings for both contextual bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties.},
archivePrefix = {arXiv},
arxivId = {1402.6779},
author = {Badanidiyuru, Ashwinkumar and Langford, John and Slivkins, Aleksandrs},
booktitle = {Proceedings of The 27th Conference on Learning Theory (CoLT)},
eprint = {1402.6779},
issn = {15337928},
pages = {1109--1134},
title = {{Resourceful contextual bandits}},
url = {https://proceedings.mlr.press/v35/badanidiyuru14.pdf},
year = {2014}
}
@article{jin2019linear,
abstract = {Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where function approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a "simulator" or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)---a classical algorithm frequently studied in the linear setting---achieves {\$}\backslashtilde{\{}\backslashmathcal{\{}O{\}}{\}}(\backslashsqrt{\{}d{\^{}}3H{\^{}}3T{\}}){\$} regret, where {\$}d{\$} is the ambient dimension of feature space, {\$}H{\$} is the length of each episode, and {\$}T{\$} is the total number of steps. Importantly, such regret is independent of the number of states and actions.},
archivePrefix = {arXiv},
arxivId = {1907.05388},
author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I.},
eprint = {1907.05388},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2019 - Provably efficient reinforcement learning with linear function approximation.pdf:pdf},
journal = {arXiv preprint arXiv:1907.0538},
title = {{Provably efficient reinforcement learning with linear function approximation}},
url = {http://arxiv.org/pdf/1907.05388.pdf},
year = {2019}
}
@article{vaswani2018,
abstract = {We investigate the use of bootstrapping in the bandit setting. We first show that the commonly used non-parametric bootstrapping (NPB) procedure can be provably inefficient and establish a near-linear lower bound on the regret incurred by it under the bandit model with Bernoulli rewards. We show that NPB with an appropriate amount of forced exploration can result in sub-linear albeit sub-optimal regret. As an alternative to NPB, we propose a weighted bootstrapping (WB) procedure. For Bernoulli rewards, WB with multiplicative exponential weights is mathematically equivalent to Thompson sampling (TS) and results in near-optimal regret bounds. Similarly, in the bandit setting with Gaussian rewards, we show that WB with additive Gaussian weights achieves near-optimal regret. Beyond these special cases, we show that WB leads to better empirical performance than TS for several reward distributions bounded on {\$}[0,1]{\$}. For the contextual bandit setting, we give practical guidelines that make bootstrapping simple and efficient to implement and result in good empirical performance on real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1805.09793},
author = {Vaswani, Sharan and Kveton, Branislav and Wen, Zheng and Rao, Anup and Schmidt, Mark and Abbasi-Yadkori, Yasin},
eprint = {1805.09793},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2018 - New insights into bootstrapping for bandits.pdf:pdf},
journal = {arXiv preprint arXiv:1805.09793},
title = {{New insights into bootstrapping for bandits}},
url = {http://arxiv.org/pdf/1805.09793.pdf},
year = {2018}
}
@inproceedings{fellows2018virel,
abstract = {Applying probabilistic models to reinforcement learning (RL) enables the uses of powerful optimisation tools such as variational inference in RL. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, e.g., the lack of mode capturing behaviour in pseudo-likelihood methods, difficulties learning deterministic policies in maximum entropy RL based approaches, and a lack of analysis when function approximators are used. We propose VIREL, a theoretically grounded inference framework for RL that utilises a parametrised action-value function to summarise future dynamics of the underlying MDP, generalising existing approaches. VIREL also benefits from a mode-seeking form of KL divergence, the ability to learn deterministic optimal polices naturally from inference, and the ability to optimise value functions and policies in separate, iterative steps. Applying variational expectation-maximisation to VIREL, we show that the actor-critic algorithm can be reduced to expectation-maximisation, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We derive a family of actor-critic methods from VIREL, including a scheme for adaptive exploration and demonstrate that our algorithms outperform state-of-the-art methods based on soft value functions in several domains.},
archivePrefix = {arXiv},
arxivId = {1811.01132},
author = {Fellows, Matthew and Mahajan, Anuj and Rudner, Tim G.J. and Whiteson, Shimon},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1811.01132},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fellows et al. - 2018 - VIREL A variational inference framework for reinforcement learning(2).pdf:pdf},
title = {{VIREL: A variational inference framework for reinforcement learning}},
url = {https://arxiv.org/pdf/1811.01132.pdf},
year = {2018}
}
@article{rakhlin2016,
abstract = {We revisit the elegant observation of T. Cover '65 which, perhaps, is not as well-known to the broader community as it should be. The first goal of the tutorial is to explain---through the prism of this elementary result---how to solve certain sequence prediction problems by modeling sets of solutions rather than the unknown data-generating mechanism. We extend Cover's observation in several directions and focus on computational aspects of the proposed algorithms. The applicability of the methods is illustrated on several examples, including node classification in a network. The second aim of this tutorial is to demonstrate the following phenomenon: it is possible to predict as well as a combinatorial "benchmark" for which we have a certain multiplicative approximation algorithm, even if the exact computation of the benchmark given all the data is NP-hard. The proposed prediction methods, therefore, circumvent some of the computational difficulties associated with finding the best model given the data. These difficulties arise rather quickly when one attempts to develop a probabilistic model for graph-based or other problems with a combinatorial structure.},
archivePrefix = {arXiv},
arxivId = {1608.09014},
author = {Rakhlin, Alexander and Sridharan, Karthik},
eprint = {1608.09014},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rakhlin, Sridharan - 2016 - A tutorial on online supervised learning with applications to node classification in social networks.pdf:pdf},
journal = {arXiv preprint arXiv:1608.09014},
title = {{A tutorial on online supervised learning with applications to node classification in social networks}},
url = {http://arxiv.org/pdf/1608.09014.pdf},
year = {2016}
}
@inproceedings{even-dar2003confidence,
abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/$\epsilon$2)log(1/$\delta$)) times to find an $\epsilon$-optimal arm with probability of at least 1-$\delta$. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over $\epsilon$-greedy Q-learning.},
author = {Even-dar, Eyal and Mannor, Shie and Mansour, Yishay},
booktitle = {Proceedings of the 20th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-dar, Mannor, Mansour - 2003 - Action elimination and stopping conditions for reinforcement learning.pdf:pdf},
isbn = {1577351894},
issn = {15324435},
pages = {162--169},
title = {{Action elimination and stopping conditions for reinforcement learning}},
url = {https://www.aaai.org/Papers/ICML/2003/ICML03-024.pdf},
year = {2003}
}
@inproceedings{hutter2011smac,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Proceedings of the 5th International Conference on Learning and Intelligent Optimization (LION)},
doi = {10.1007/978-3-642-25566-3_40},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter, Hoos, Leyton-Brown - 2011 - Sequential model-based optimization for general algorithm configuration.pdf:pdf},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
url = {https://www.cs.ubc.ca/{~}hutter/papers/10-TR-SMAC.pdf},
year = {2011}
}
@inproceedings{krizhevsky2012alexnet,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
doi = {10.1201/9781420010749},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
isbn = {9781420010749},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{levy2015embedding,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
pages = {211--225},
pmid = {26099735},
title = {{Improving distributional similarity with lessons learned from word embeddings}},
url = {http://www.aclweb.org/anthology/Q15-1016},
volume = {3},
year = {2015}
}
@article{huang2020boss,
abstract = {The evaluation of hyperparameters, neural architectures, or data augmentation policies becomes a critical model selection problem in advanced deep learning with a large hyperparameter search space. In this paper, we propose an efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the scenario of hyperparameter search evaluation. It evaluates the potential of hyperparameters by the sub-samples of observations and is theoretically proved to be optimal under the criterion of cumulative regret. We further combine SS with Bayesian Optimization and develop a novel hyperparameter optimization algorithm called BOSS. Empirical studies validate our theoretical arguments of SS and demonstrate the superior performance of BOSS on a number of applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).},
archivePrefix = {arXiv},
arxivId = {2007.05670},
author = {Huang, Yimin and Li, Yujun and Li, Zhenguo and Zhang, Zhihua},
eprint = {2007.05670},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2020 - An asymptotically optimal multi-armed bandit algorithm and hyperparameter optimization.pdf:pdf},
journal = {arXiv preprint arXiv:2007.05670},
keywords = {asymptotic optimality,bayesian optimization,hyperparameter optimiza-,multi-armed bandit problems,sub-sampling methods,tion},
pages = {1--27},
title = {{An asymptotically optimal multi-armed bandit algorithm and hyperparameter optimization}},
url = {http://arxiv.org/pdf/2007.05670.pdf},
volume = {1},
year = {2020}
}
@inproceedings{malherbe2017lipo,
abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional Holder like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
archivePrefix = {arXiv},
arxivId = {1703.02628},
author = {Malherbe, C{\'{e}}dric and Vayatis, Nicolas},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.02628},
isbn = {9781510855144},
pages = {3592--3601},
title = {{Global optimization of Lipschitz functions}},
url = {https://arxiv.org/pdf/1703.02628.pdf},
volume = {5},
year = {2017}
}
@inproceedings{audibert2010budget,
abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards. As a by-product of our analysis, we show that identifying the best arm (when it is unique) requires a number of samples of order (up to a log(K) factor) $\Sigma$ i 1/$\Delta$2i, where the sum is on the suboptimal arms and$\Delta$i represents the difference between the mean reward of the best arm and the one of arm i. This generalizes the well-known fact that one needs of order of 1/$\Delta$2 samples to differentiate the means of two distributions with gap $\Delta$.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Audibert, Bubeck - 2010 - Best arm identification in multi-armed bandits.pdf:pdf},
isbn = {9780982252925},
title = {{Best arm identification in multi-armed bandits}},
url = {https://hal-enpc.archives-ouvertes.fr/hal-00654404/document},
year = {2010}
}
@article{elsken2018nas,
abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
archivePrefix = {arXiv},
arxivId = {1808.05377},
author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
eprint = {1808.05377},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elsken, Metzen, Hutter - 2018 - Neural architecture search A survey.pdf:pdf},
title = {{Neural architecture search: A survey}},
url = {http://arxiv.org/pdf/1808.05377.pdf},
year = {2018}
}
@article{kleinberg2013,
abstract = {In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the "Lipschitz MAB problem". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback ("best expert") version of the problem, where after every round the payoffs from all arms are revealed.},
archivePrefix = {arXiv},
arxivId = {1312.1277},
author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
eprint = {1312.1277},
journal = {arXiv preprint arXiv:1312.1277},
title = {{Bandits and experts in metric spaces}},
url = {http://arxiv.org/pdf/1312.1277.pdf},
year = {2013}
}
@inproceedings{abbasi-yadkori2012,
abstract = {We introduce a novel technique, which we call online-to-confidence-set conversion. The technique allows us to construct high-probability confidence sets for linear prediction with correlated inputs given the predictions of any algorithm (e.g., online LASSO, exponentiated gradient algorithm, online least-squares, p-norm algorithm) targeting online learning with linear predictors and the quadratic loss. By construction, the size of the confidence set is directly governed by the regret of the online learning algorithm. Constructing tight confidence sets is interesting on its own, but the new technique is given extra weight by the fact having access tight confidence sets underlies a number of important problems. The advantage of our construction here is that progress in constructing better algorithms for online prediction problems directly translates into tighter confidence sets. In this paper, this is demonstrated in the case of linear stochastic bandits. In particular, we introduce the sparse variant of linear stochastic bandits and show that a recent online algorithm together with our online-to-confidence-set conversion allows one to derive algorithms that can exploit if the reward is a function of a sparse linear combination of the components of the chosen action.},
author = {Abbasi-Yadkori, Yasin and P{\'{a}}l, D{\'{a}}vid and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbasi-Yadkori, P{\'{a}}l, Szepesv{\'{a}}ri - 2012 - Online-to-confidence-set conversions and application to sparse stochastic bandits.pdf:pdf},
issn = {15337928},
keywords = {bandits,linear bandits,online learning,stochastic bandits,theory},
title = {{Online-to-confidence-set conversions and application to sparse stochastic bandits}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.417.4594{\&}rep=rep1{\&}type=pdf},
year = {2012}
}
@inproceedings{kalyanakrishnan2012lucb,
author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalyanakrishnan et al. - 2012 - PAC subset selection in stochastic multi-armed bandits.pdf:pdf},
isbn = {978-1-4503-1285-1},
keywords = {stochastic multi-armed bandits,subset selection},
pages = {655--662},
title = {{PAC subset selection in stochastic multi-armed bandits}},
url = {https://icml.cc/2012/papers/359.pdf},
year = {2012}
}
@inproceedings{amani2019safe,
archivePrefix = {arXiv},
arxivId = {arXiv:1908.05814v1},
author = {Amani, Sanae and Alizadeh, Mahnoosh and Thrampoulidis, Christos and Barbara, Santa},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {arXiv:1908.05814v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amani et al. - 2019 - Linear stochastic bandits under safety constraints.pdf:pdf},
title = {{Linear stochastic bandits under safety constraints}},
url = {https://papers.nips.cc/paper/9124-linear-stochastic-bandits-under-safety-constraints.pdf},
year = {2019}
}
@article{moles2003,
author = {Moles, Carmen G. and Mendes, Pedro and Banga, Julio R.},
doi = {10.1101/gr.1262503.},
journal = {Genome Research},
pages = {2467--2474},
title = {{Parameter estimation in biochemical pathways: A comparison of global optimization methods}},
url = {https://genome.cshlp.org/content/13/11/2467.full.pdf+html},
year = {2003}
}
@inproceedings{auer2000online,
abstract = {We show how a standard tool from statistics - namely confidence bounds - can be used to elegantly deal with situations which exhibit an exploitation/exploration trade-off. Our technique for designing and analyzing algorithms for such situations is very general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We consider two models with such an exploitation/exploration trade-off. For the adversarial bandit problem our new algorithm suffers only O (T1/2) regret over T trials which improves significantly over the previously best O(T2/3) regret. We also extend our results for the adversarial bandit problem to `shifting bandits'. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O (T1/2).},
author = {Auer, Peter},
booktitle = {Proceedings of the 41st Annual Symposium on Foundations of Computer Science (FOCS)},
doi = {10.1109/sfcs.2000.892116},
issn = {02725428},
pages = {270--279},
title = {{Using upper confidence bounds for online learning}},
url = {https://ieeexplore.ieee.org/document/892116},
year = {2000}
}
@article{azuma1967,
author = {Azuma, Kazuoki},
doi = {10.2748/tmj/1178243286},
issn = {00408735},
journal = {Tohoku Mathematical Journal},
number = {3},
pages = {357--367},
title = {{Weighted sums of certain dependent random variables}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.tmj/1178243286},
volume = {19},
year = {1967}
}
@inproceedings{jamieson2014bai,
abstract = {his paper is concerned with identifying the arm with the highest mean in a multi-armed bandit problem using as few independent samples from the arms as possible. While the so-called " best arm problem " dates back to the 1950s, only recently were two qualitatively different algorithms proposed that achieve the optimal sample complexity for the problem. This paper reviews these recent advances and shows that most best-arm algorithms can be described as variants of the two recent optimal algorithms. For each algorithm type we consider a specific instance to analyze both theoretically and empirically thereby exposing the core components of the theoretical analysis of these algorithms and intuition about how the algorithms work in practice. The derived sample complexity bounds are novel, and in certain cases improve upon previous bounds. In addition, we compare a variety of state-of-the-art algorithms empirically through simulations for the best-arm-problem.},
author = {Jamieson, Kevin and Nowak, Robert},
booktitle = {Proceedings of the 48th Annual Conference on Information Sciences and Systems (CISS)},
doi = {10.1109/CISS.2014.6814096},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jamieson, Nowak - 2014 - Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting.pdf:pdf},
pages = {1--6},
title = {{Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6814096},
year = {2014}
}
@inproceedings{xu2018meta,
author = {Xu, Zhongwen and van Hasselt, Hado and Silver, David},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, van Hasselt, Silver - 2018 - Meta-gradient reinforcement learning.pdf:pdf},
title = {{Meta-gradient reinforcement learning}},
url = {https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf},
year = {2018}
}
@article{frazier2018bo,
abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
archivePrefix = {arXiv},
arxivId = {1807.02811},
author = {Frazier, Peter I.},
doi = {10.1109/ICRA.2017.7989111},
eprint = {1807.02811},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frazier - 2018 - A tutorial on Bayesian optimization.pdf:pdf},
isbn = {9781509046331},
issn = {0002-953X},
journal = {arXiv preprint arXiv:1807.02811},
pmid = {9356566},
title = {{A tutorial on Bayesian optimization}},
url = {http://arxiv.org/pdf/1807.02811.pdf},
year = {2018}
}
@article{shahriari2016loop,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
doi = {10.1109/JPROC.2015.2494218},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
pmid = {25246403},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf},
volume = {104},
year = {2016}
}
@inproceedings{karnin2016structure,
abstract = {We consider the problem of finding the best arm in a stochastic Multi-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be easier than discovering the best arm. Our results are focused on the scenario where the failure probability $\delta$ must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification. We demonstrate the effectiveness of our framework by applying it, and matching or improving the state-of-the art results in the problems of: Linear bandits, Dueling bandits with the Condorcet assumption, Copeland dueling bandits, Unimodal bandits and Graphical bandits.},
author = {Karnin, Zohar},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karnin - 2016 - Verification based solution for structured MAB problems.pdf:pdf},
issn = {10495258},
pages = {145--153},
title = {{Verification based solution for structured MAB problems}},
url = {https://papers.nips.cc/paper/6525-verification-based-solution-for-structured-mab-problems.pdf},
year = {2016}
}
@article{menard2019lma,
abstract = {We present a new algorithm based on an gradient ascent for a general Active Exploration bandit problem in the fixed confidence setting. This problem encompasses several well studied problems such that the Best Arm Identification or Thresholding Bandits. It consists of a new sampling rule based on an online lazy mirror ascent. We prove that this algorithm is asymptotically optimal and, most importantly, computationally efficient.},
archivePrefix = {arXiv},
arxivId = {1905.08165},
author = {M{\'{e}}nard, Pierre},
eprint = {1905.08165},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\'{e}}nard - 2019 - Gradient ascent for active exploration in bandit problems.pdf:pdf},
journal = {arXiv preprint arXiv:1905.08165},
title = {{Gradient ascent for active exploration in bandit problems}},
url = {http://arxiv.org/pdf/1905.08165.pdf},
year = {2019}
}
@article{blei2017vi,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
isbn = {1601.00670},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
number = {518},
pages = {859--877},
pmid = {303902},
title = {{Variational inference: A review for statisticians}},
url = {https://arxiv.org/pdf/1601.00670.pdf},
volume = {112},
year = {2017}
}
@article{kaufmann2018mixture,
abstract = {This paper presents new deviation inequalities that are valid uniformly in time under adaptive sampling in a multi-armed bandit model. The deviations are measured using the Kullback-Leibler divergence in a given one-dimensional exponential family, and may take into account several arms at a time. They are obtained by constructing for each arm a mixture martingale based on a hierarchical prior, and by multiplying those martingales. Our deviation inequalities allow us to analyze stopping rules based on generalized likelihood ratios for a large class of sequential identification problems, and to construct tight confidence intervals for some functions of the means of the arms.},
author = {Kaufmann, Emilie and Koolen, Wouter},
doi = {arXiv:1811.11419v1},
journal = {arXiv preprint arXiv:1811.11419},
title = {{Mixture martingales revisited with applications to sequential tests and confidence intervals}},
url = {http://arxiv.org/pdf/1811.11419.pdf},
year = {2018}
}
@article{chaudhuri2019pacbai,
abstract = {We consider the problem of identifying any {\$}k{\$} out of the best {\$}m{\$} arms in an {\$}n{\$}-armed stochastic multi-armed bandit. Framed in the PAC setting, this particular problem generalises both the problem of `best subset selection' and that of selecting `one out of the best m' arms [arcsk 2017]. In applications such as crowd-sourcing and drug-designing, identifying a single good solution is often not sufficient. Moreover, finding the best subset might be hard due to the presence of many indistinguishably close solutions. Our generalisation of identifying exactly {\$}k{\$} arms out of the best {\$}m{\$}, where {\$}1 \backslashleq k \backslashleq m{\$}, serves as a more effective alternative. We present a lower bound on the worst-case sample complexity for general {\$}k{\$}, and a fully sequential PAC algorithm, $\backslash$GLUCB, which is more sample-efficient on easy instances. Also, extending our analysis to infinite-armed bandits, we present a PAC algorithm that is independent of {\$}n{\$}, which identifies an arm from the best {\$}\backslashrho{\$} fraction of arms using at most an additive poly-log number of samples than compared to the lower bound, thereby improving over [arcsk 2017] and [Aziz+AKA:2018]. The problem of identifying {\$}k {\textgreater} 1{\$} distinct arms from the best {\$}\backslashrho{\$} fraction is not always well-defined; for a special class of this problem, we present lower and upper bounds. Finally, through a reduction, we establish a relation between upper bounds for the `one out of the best {\$}\backslashrho{\$}' problem for infinite instances and the `one out of the best {\$}m{\$}' problem for finite instances. We conjecture that it is more efficient to solve `small' finite instances using the latter formulation, rather than going through the former.},
archivePrefix = {arXiv},
arxivId = {1901.08386},
author = {Chaudhuri, Arghya R. and Kalyanakrishnan, Shivaram},
eprint = {1901.08386},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaudhuri, Kalyanakrishnan - 2019 - PAC identification of many good arms in stochastic multi-armed bandits.pdf:pdf},
journal = {arXiv preprint arXiv:1901.08386},
title = {{PAC identification of many good arms in stochastic multi-armed bandits}},
url = {http://arxiv.org/pdf/1901.08386.pdf},
year = {2019}
}
@article{elshawi2019automl,
abstract = {With the continuous and vast increase in the amount of data in our digital world, it has been acknowledged that the number of knowledgeable data scientists can not scale to address these challenges. Thus, there was a crucial need for automating the process of building good machine learning models. In the last few years, several techniques and frameworks have been introduced to tackle the challenge of automating the process of Combined Algorithm Selection and Hyper-parameter tuning (CASH) in the machine learning domain. The main aim of these techniques is to reduce the role of the human in the loop and fill the gap for non-expert machine learning users by playing the role of the domain expert. In this paper, we present a comprehensive survey for the state-of-the-art efforts in tackling the CASH problem. In addition, we highlight the research work of automating the other steps of the full complex machine learning pipeline (AutoML) from data understanding till model deployment. Furthermore, we provide comprehensive coverage for the various tools and frameworks that have been introduced in this domain. Finally, we discuss some of the research directions and open challenges that need to be addressed in order to achieve the vision and goals of the AutoML process.},
archivePrefix = {arXiv},
arxivId = {1906.02287},
author = {Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
eprint = {1906.02287},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elshawi, Maher, Sakr - 2019 - Automated machine learning State-of-the-art and open challenges.pdf:pdf},
journal = {arXiv preprint arXiv:1906.02287},
title = {{Automated machine learning: State-of-the-art and open challenges}},
url = {http://arxiv.org/pdf/1906.02287.pdf},
year = {2019}
}
@inproceedings{shang2020t3c,
abstract = {We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors.},
archivePrefix = {arXiv},
arxivId = {1910.10945},
author = {Shang, Xuedong and de Heide, Rianne and Kaufmann, Emilie and M{\'{e}}nard, Pierre and Valko, Michal},
booktitle = {Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1910.10945},
title = {{Fixed-confidence guarantees for Bayesian best-arm identification}},
url = {http://arxiv.org/pdf/1910.10945.pdf},
year = {2020}
}
@inproceedings{munos2011soo,
abstract = {We consider a global optimization problem of a deterministic function f in a semi-metric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of . We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semi-metric under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.},
author = {Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Munos - 2011 - Optimistic optimization of deterministic functions without the knowledge of its smoothness.pdf:pdf},
isbn = {9781618395993},
pages = {783--791},
title = {{Optimistic optimization of deterministic functions without the knowledge of its smoothness}},
url = {https://papers.nips.cc/paper/4304-optimistic-optimization-of-a-deterministic-function-without-the-knowledge-of-its-smoothness.pdf},
year = {2011}
}
@inproceedings{lu2019glb,
abstract = {In this paper, we study the multi-objective bandits (MOB) problem, where a learner repeatedly selects one arm to play and then receives a reward vector consisting of multiple objectives. MOB has found many real-world applications as varied as online recommendation and network routing. On the other hand, these applications typically contain contextual information that can guide the learning process which, however, is ignored by most of existing work. To utilize this information, we associate each arm with a context vector and assume the reward follows the generalized linear model (GLM). We adopt the notion of Pareto regret to evaluate the learner's performance and develop a novel algorithm for minimizing it. The essential idea is to apply a variant of the online Newton step to estimate model parameters, based on which we utilize the upper confidence bound (UCB) policy to construct an approximation of the Pareto front, and then uniformly at random choose one arm from the approximate Pareto front. Theoretical analysis shows that the proposed algorithm achieves an {\~{O}}(d√T) Pareto regret, where T is the time horizon and d is the dimension of contexts, which matches the optimal result for single objective contextual bandits problem. Numerical experiments demonstrate the effectiveness of our method.},
archivePrefix = {arXiv},
arxivId = {1905.12879},
author = {Lu, Shiyin and Wang, Guanghui and Hu, Yao and Zhang, Lijun},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2019/427},
eprint = {1905.12879},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2019 - Multi-objective generalized linear bandits.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
pages = {3080--3086},
title = {{Multi-objective generalized linear bandits}},
url = {https://arxiv.org/pdf/1905.12879.pdf},
year = {2019}
}
@inproceedings{kalyanakrishnan2010,
abstract = {We consider the general, widely applicable problem of selecting from n real-valued random variables a subset of size m of those with the highest means, based on as few samples as possible. This problem, which we denote Ex-plore-m, is a core aspect in several stochastic optimization algorithms, and applications of simulation and industrial engineering. The theoretical basis for our work is an extension of a previous formulation using multi-armed bandits that is devoted to identifying just the one best of n random variables (Explore-1). In addition to providing PAC bounds for the general case, we tailor our theoretically grounded approach to work efficiently in practice. Empirical comparisons of the resulting sampling algorithm against state-of-the-art subset selection strategies demonstrate significant gains in sample efficiency. Copyright 2010 by the author(s)/owner(s).},
author = {Kalyanakrishnan, Shivaram and Stone, Peter},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML)},
isbn = {9781605589077},
pages = {511--518},
title = {{Efficient selection of multiple bandit arms: Theory and practice}},
url = {https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/ICML10-kalyanakrishnan.pdf},
year = {2010}
}
@inproceedings{ester1996density,
abstract = {Clustering techniques are often used for data exploration. In the literature, there are many examples of applications of different clustering methods. The density-based approaches form a separate group within the clustering techniques since they take into account the density of the data. Using the density of data as a similarity measure is practical in many real situations, because clusters of arbitrary shapes can be handled, what is not possible with convectional clustering methods.{\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
doi = {10.1016/B978-044452701-1.00067-3},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A density-based algorithm for discovering clusters in large spatial databases with noise.pdf:pdf},
isbn = {9780444527011},
keywords = {Color maps,Core plot,Density-based techniques,Inliers,Natural clusters,Outliers,Reachability plot},
pages = {226--231},
title = {{A density-based algorithm for discovering clusters in large spatial databases with noise}},
url = {https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf},
year = {1996}
}
@book{bubeck2015convex,
abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non- Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, minibatches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
archivePrefix = {arXiv},
arxivId = {1405.4980},
author = {Bubeck, S{\'{e}}bastien},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000050},
eprint = {1405.4980},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf:pdf},
issn = {19358245},
pages = {231--357},
title = {{Convex Optimization: Algorithms and Complexity}},
url = {https://arxiv.org/pdf/1405.4980.pdf},
volume = {8},
year = {2015}
}
@book{francois-lavet2018drl,
archivePrefix = {arXiv},
arxivId = {1811.12560v1},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and Brain, Google and Delft, Boston},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000071},
eprint = {1811.12560v1},
pages = {1--136},
publisher = {Mike Casey},
title = {{An Introduction to Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1811.12560.pdf},
volume = {11},
year = {2018}
}
@inproceedings{filippi2010glmucb,
abstract = {We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive finite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difficulty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to significantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach.$\backslash$n},
author = {Filippi, Sarah and Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 23 (NIPS)},
isbn = {9781617823800},
keywords = {generalized linear models,multi-armed bandit,parametric bandits,regret minimization,ucb},
pages = {586--594},
title = {{Parametric bandits: The generalized linear case}},
url = {https://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case.pdf},
year = {2010}
}
@inproceedings{tang2017count,
abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
archivePrefix = {arXiv},
arxivId = {1611.04717},
author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1611.04717},
issn = {10495258},
pages = {2754--2763},
title = {{{\#}Exploration: A study of count-based exploration for deep reinforcement learning}},
url = {https://arxiv.org/pdf/1611.04717.pdf},
volume = {2017-Decem},
year = {2017}
}
@book{brooks2011mcmc,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiaoli},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
pmid = {25246403},
publisher = {CRC Press},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.mcmchandbook.net/},
volume = {20116022},
year = {2011}
}
@article{huang2006multifidelity,
abstract = {When cost per evaluation on a system of interest is high, surrogate systems can provide cheaper but lower-fidelity information. In the proposed extension of the sequential kriging optimization method, surrogate systems are exploited to reduce the total evaluation cost. The method utilizes data on all systems to build a kriging metamodel that provides a global prediction of the objective function and a measure of prediction uncertainty. The location and fidelity level of the next evaluation are selected by maximizing an augmented expected improvement function, which is connected with the evaluation costs. The proposed method was applied to test functions from the literature and a metal-forming process design problem via finite element simulations. The method manifests sensible search patterns, robust performance, and appreciable reduction in total evaluation cost as compared to the original method.},
author = {Huang, Deng and Allen, Theodore T. and Notz, William. I. and Miller, Allen},
doi = {10.1007/s00158-005-0587-0},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2006 - Sequential kriging optimization using multiple-fidelity evaluations.pdf:pdf},
isbn = {1615-147X$\backslash$r1615-1488},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Computer experiments,Efficient global optimization,Kriging,Multiple fidelity,Surrogate systems},
number = {5},
pages = {369--382},
title = {{Sequential kriging optimization using multiple-fidelity evaluations}},
volume = {32},
year = {2006}
}
@article{kolla2016,
abstract = {We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in learning a stochastic multi-armed bandit problem. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbors in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that natural extensions of widely studied single agent learning policies to the network setting need not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies and argue by deriving regret lower bounds that they are liable to suffer a large regret in the networked setting. We also show that the learning performance can be substantially improved if the agents exploit the structure of the network and develop a simple learning algorithm based on dominating sets of the network. Specifically, we first consider a star network, which is a common motif in hierarchical social networks and show analytically that the hub agent can be used as an information sink to expedite learning and improve the overall regret. We also derive network-wide regret bounds for the algorithm applied to general networks. We conduct numerical experiments on a variety of networks to corroborate our analytical results.},
archivePrefix = {arXiv},
arxivId = {1602.08886},
author = {Kolla, Ravi Kumar and Jagannathan, Krishna and Gopalan, Aditya},
doi = {10.1109/TNET.2018.2852361},
eprint = {1602.08886},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kolla, Jagannathan, Gopalan - 2018 - Collaborative learning of stochastic bandits over a social network(2).pdf:pdf},
issn = {10636692},
journal = {IEEE/ACM Transactions on Networking},
keywords = {Online learning,dominating set,multi armed bandits,regret},
month = {feb},
number = {4},
pages = {1782--1795},
title = {{Collaborative learning of stochastic bandits over a social network}},
url = {http://arxiv.org/pdf/1602.08886.pdf},
volume = {26},
year = {2018}
}
@inproceedings{sinclair2020adaptive,
abstract = {We introduce the technique of adaptive discretization to design efficient model-based episodic reinforcement learning algorithms in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective, we provide worst-case regret bounds for our algorithm, which are competitive compared to the state-of-the-art model-based algorithms; moreover, our bounds are obtained via a modular proof technique, which can potentially extend to incorporate additional structure on the problem. From an implementation standpoint, our algorithm has much lower storage and computational requirements, due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed-discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization. 1 MSC Codes 68Q32},
archivePrefix = {arXiv},
arxivId = {2007.00717},
author = {Sinclair, Sean R. and Wang, Tianyu and Jain, Gauri and Banerjee, Siddhartha and Yu, Christina Lee},
booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
eprint = {2007.00717},
issn = {23318422},
title = {{Adaptive discretization for model-based reinforcement learning}},
url = {https://arxiv.org/pdf/2007.00717.pdf},
year = {2020}
}
@article{bubeck2011lipschitz,
abstract = {We consider the setting of stochastic bandit problems with a continuum of arms indexed by [0,1]d . We first point out that the strategies considered so far in the literature only provided theoretical guarantees of the form: given some tuning parameters, the regret is small with respect to a class of environments that depends on these parameters. This is however not the right perspective, as it is the strategy that should adapt to the specific bandit environment at hand, and not the other way round. Put differently, an adaptation issue is raised. We solve it for the special case of environments whose mean-payoff functions are globally Lipschitz. More precisely, we show that the minimax optimal orders of magnitude Ld/(d+2) T(d+1)/(d+2) of the regret bound over T time instances against an environment whose mean-payoff function f is Lipschitz with constant L can be achieved without knowing L or T in advance. This is in contrast to all previously known strategies, which require to some extent the knowledge of L to achieve this performance guarantee. {\textcopyright} 2011 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {1105.5041},
author = {Bubeck, S{\'{e}}bastien and Stoltz, Gilles and Yu, Jia Yuan},
doi = {10.1007/978-3-642-24412-4_14},
eprint = {1105.5041},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck, Stoltz, Yu - 2011 - Lipschitz bandits without the Lipschitz constant(2).pdf:pdf},
isbn = {9783642244117},
issn = {03029743},
journal = {arXiv preprint arXiv:1105.5041},
title = {{Lipschitz bandits without the Lipschitz constant}},
url = {https://arxiv.org/pdf/1105.5041.pdf},
year = {2011}
}
@inproceedings{katariya2016stochastic,
abstract = {We propose stochastic rank-{\$}1{\$} bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The main challenge of the problem is that the individual values of the row and column are unobserved. We assume that these values are stochastic and drawn independently. We propose a computationally-efficient algorithm for solving our problem, which we call Rank1Elim. We derive a {\$}O((K + L) (1 / \backslashDelta) \backslashlog n){\$} upper bound on its {\$}n{\$}-step regret, where {\$}K{\$} is the number of rows, {\$}L{\$} is the number of columns, and {\$}\backslashDelta{\$} is the minimum of the row and column gaps; under the assumption that the mean row and column rewards are bounded away from zero. To the best of our knowledge, we present the first bandit algorithm that finds the maximum entry of a rank-{\$}1{\$} matrix whose regret is linear in {\$}K + L{\$}, {\$}1 / \backslashDelta{\$}, and {\$}\backslashlog n{\$}. We also derive a nearly matching lower bound. Finally, we evaluate Rank1Elim empirically on multiple problems. We observe that it leverages the structure of our problems and can learn near-optimal solutions even if our modeling assumptions are mildly violated.},
author = {Katariya, Sumeet and Kveton, Branislav and Szepesv{\'{a}}ri, Csaba and Vernade, Claire and Wen, Zheng},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katariya et al. - 2017 - Stochastic rank-1 bandits(2).pdf:pdf},
title = {{Stochastic rank-1 bandits}},
url = {https://arxiv.org/pdf/1608.03023.pdf},
year = {2017}
}
@inproceedings{locatelli2017adaptivity,
abstract = {This work addresses various open questions in the theory of active learning for nonparametric classification. Our contributions are both statistical and algorithmic: -We establish new minimax-rates for active learning under common $\backslash$textit{\{}noise conditions{\}}. These rates display interesting transitions -- due to the interaction between noise $\backslash$textit{\{}smoothness and margin{\}} -- not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed. -We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for $\backslash$textit{\{}adaptive confidence sets{\}}, resulting in strictly milder distributional requirements.},
archivePrefix = {arXiv},
arxivId = {1703.05841},
author = {Locatelli, Andrea and Carpentier, Alexandra and Kpotufe, Samory},
booktitle = {Proceedings of the 30th Conference on Learning Theory (CoLT)},
eprint = {1703.05841},
pages = {1383--1416},
title = {{Adaptivity to noise parameters in nonparametric active learning}},
url = {http://proceedings.mlr.press/v65/locatelli andrea17a/locatelli andrea17a.pdf},
year = {2017}
}
@inproceedings{snoek2015,
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa A. and Prabhat and Adams, Ryan P.},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
title = {{Scalable Bayesian optimization using deep neural networks}},
url = {https://arxiv.org/pdf/1502.05700.pdf},
year = {2015}
}
@article{streeter2019earlystopping,
abstract = {We derive an optimal policy for adaptively restarting a randomized algorithm, based on observed features of the run-so-far, so as to minimize the expected time required for the algorithm to successfully terminate. Given a suitable Bayesian prior, this result can be used to select the optimal black-box optimization algorithm from among a large family of algorithms that includes random search, Successive Halving, and Hyperband. On CIFAR-10 and ImageNet hyperparameter tuning problems, the proposed policies offer up to a factor of 13 improvement over random search in terms of expected time to reach a given target accuracy, and up to a factor of 3 improvement over a baseline adaptive policy that terminates a run whenever its accuracy is below-median.},
archivePrefix = {arXiv},
arxivId = {1902.08285},
author = {Streeter, Matthew},
eprint = {1902.08285},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Streeter - 2019 - Bayes optimal early stopping policies for black-box optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1902.08285},
title = {{Bayes optimal early stopping policies for black-box optimization}},
url = {https://arxiv.org/pdf/1902.08285.pdf},
year = {2019}
}
@inproceedings{bubeck2009pure,
abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.},
archivePrefix = {arXiv},
arxivId = {0802.2655},
author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles},
booktitle = {Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT)},
doi = {10.1007/978-3-642-04414-4_7},
eprint = {0802.2655},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck, Munos, Stoltz - 2009 - Pure exploration in multi-armed bandits problems.pdf:pdf},
isbn = {3642044131},
issn = {03029743},
pages = {23--37},
title = {{Pure exploration in multi-armed bandits problems}},
url = {https://arxiv.org/pdf/0802.2655.pdf},
year = {2009}
}
@inproceedings{fiez2019transductive,
abstract = {In this paper we introduce the transductive linear bandit problem: given a set of measurement vectors {\$}\backslashmathcal{\{}X{\}}\backslashsubset \backslashmathbb{\{}R{\}}{\^{}}d{\$}, a set of items {\$}\backslashmathcal{\{}Z{\}}\backslashsubset \backslashmathbb{\{}R{\}}{\^{}}d{\$}, a fixed confidence {\$}\backslashdelta{\$}, and an unknown vector {\$}\backslashtheta{\^{}}{\{}\backslashast{\}}\backslashin \backslashmathbb{\{}R{\}}{\^{}}d{\$}, the goal is to infer {\$}\backslashtext{\{}argmax{\}}{\_}{\{}z\backslashin \backslashmathcal{\{}Z{\}}{\}} z{\^{}}\backslashtop\backslashtheta{\^{}}\backslashast{\$} with probability {\$}1-\backslashdelta{\$} by making as few sequentially chosen noisy measurements of the form {\$}x{\^{}}\backslashtop\backslashtheta{\^{}}{\{}\backslashast{\}}{\$} as possible. When {\$}\backslashmathcal{\{}X{\}}=\backslashmathcal{\{}Z{\}}{\$}, this setting generalizes linear bandits, and when {\$}\backslashmathcal{\{}X{\}}{\$} is the standard basis vectors and {\$}\backslashmathcal{\{}Z{\}}\backslashsubset \backslash{\{}0,1\backslash{\}}{\^{}}d{\$}, combinatorial bandits. Such a transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages {\$}\backslashmathcal{\{}X{\}}{\$} a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages {\$}\backslashmathcal{\{}Z{\}}{\$} that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books {\$}\backslashmathcal{\{}X{\}}{\$} a user is queried about may be restricted to well known best-sellers even though the goal might be to recommend more esoteric titles {\$}\backslashmathcal{\{}Z{\}}{\$}. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we provide the first non-asymptotic algorithm for linear bandits that nearly achieves the information theoretic lower bound.},
archivePrefix = {arXiv},
arxivId = {1906.08399},
author = {Fiez, Tanner and Jain, Lalit and Jamieson, Kevin and Ratliff, Lillian},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1906.08399},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiez et al. - 2019 - Sequential experimental design for transductive linear bandits.pdf:pdf},
title = {{Sequential experimental design for transductive linear bandits}},
url = {http://arxiv.org/pdf/1906.08399.pdf},
year = {2019}
}
@article{baltrusaitis2018multimodal,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Hidden Markov models,Media,Multimedia communication,Multimodal,Speech,Speech recognition,Streaming media,Visualization,introductory,machine learning,survey},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {https://arxiv.org/pdf/1705.09406.pdf},
year = {2018}
}
@article{auer2002linear,
author = {Auer, Peter},
journal = {Journal of Machine Learning Research},
pages = {397--422},
title = {{Using confidence bounds for exploitation-exploration trade-offs}},
url = {https://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf},
volume = {3},
year = {2002}
}
@inproceedings{jamieson2014lilucb,
author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the 27th Annual Conference on Learning Theory (CoLT)},
pages = {423--439},
title = {{lil'UCB: An optimal exploration algorithm for multi-armed bandits}},
url = {https://arxiv.org/pdf/1312.7308.pdf},
year = {2014}
}
@inproceedings{bergstra2011tpe,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - 2011 - Algorithms for hyper-parameter optimization.pdf:pdf},
pages = {2546--2554},
title = {{Algorithms for hyper-parameter optimization}},
url = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2011}
}
@book{pukelsheim2006optimal,
author = {Pukelsheim, Friedrich},
booktitle = {Classics in Applied Mathematics},
doi = {10.1002/0471667196.ess3056.pub2},
pages = {454},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Optimal Design of Experiments}},
url = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898719109.bm},
year = {2006}
}
@article{Authors,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2021 - Guaranteed fixed-confidence best arm identification in multi-armed bandit.pdf:pdf},
number = {Icml},
title = {{Guaranteed fixed-confidence best arm identification in multi-armed bandit}},
year = {2021}
}
@article{sui2020bayesian,
author = {Sui, Guoxin and Yu, Yong},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sui, Yu - 2020 - Bayesian Contextual Bandits for Hyper Parameter Optimization.pdf:pdf},
journal = {IEEE Access},
pages = {42971--42979},
title = {{Bayesian contextual bandits for hyper parameter optimization}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=9017927},
volume = {8},
year = {2020}
}
@inproceedings{li2010contextual,
abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5{\%} click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
archivePrefix = {arXiv},
arxivId = {1003.0146},
author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
booktitle = {Proceedings of the 19th International World Wide Web Conference (WWW)},
doi = {10.1145/1772690.1772758},
eprint = {1003.0146},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2010 - A contextual-bandit approach to personalized news article recommendation.pdf:pdf},
isbn = {9781605587998},
issn = {9781605587998},
keywords = {contextual bandit,exploitation dilemma,exploration,personalization,recommender sys,tems,web service},
organization = {ACM},
pages = {661--670},
publisher = {ACM Press},
title = {{A contextual-bandit approach to personalized news article recommendation}},
url = {https://arxiv.org/pdf/1003.0146.pdf},
year = {2010}
}
@article{honda2015imed,
abstract = {In this paper we consider a stochastic multiarmed bandit problem. It is known in this problem that Deterministic Minimum Empirical Divergence (DMED) policy achieves the asymptotic theoretical bound for the model where each reward distribution is supported in a known bounded interval, say [0; 1]. However, the regret bound of DMED is described in an asymptotic form and the performance in finite time has been unknown. We modify this policy and derive a finite-time regret bound for the new policy, Indexed Minimum Empirical Divergence (IMED), by refining large deviation probabilities to a simple nonasymptotic form. Further, the refined analysis reveals that the finite-time regret bound is valid even in the case that the reward is not bounded from below. Therefore, our finitetime result applies to the case that the minimum reward (that is, the maximum loss) is unknown or unbounded. We also present some simulation results which shows that IMED much improves DMED and performs competitively to other state-of-the-art policies.},
author = {Honda, Junya and Takemura, Akimichi},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Honda, Takemura - 2015 - Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Finite-time regret,Large deviation principle,Stochastic bandit},
pages = {3721--3756},
title = {{Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards}},
url = {https://www.jmlr.org/papers/volume16/honda15a/honda15a.pdf},
volume = {16},
year = {2015}
}
@article{robbins1952,
author = {Robbins, Herbert},
doi = {10.1090/S0002-9904-1952-09620-8},
journal = {Bulletin of the American Mathematics Society},
number = {5},
pages = {527--535},
title = {{Some aspects of the sequential design of experiments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3232{\&}rep=rep1{\&}type=pdf},
volume = {58},
year = {1952}
}
@book{manning1999,
abstract = {"Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf." - Eugene Charniak, Department of Computer Science, Brown University Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications. More on this book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher D. and Sch{\"{u}}tze, Henrich},
doi = {10.1145/601858.601867},
eprint = {arXiv:1011.1669v3},
isbn = {0262133601},
issn = {01635808},
pages = {720},
pmid = {20955506},
publisher = {MIT Press},
title = {{Foundations of Statistical Natural Language Processing}},
url = {https://nlp.stanford.edu/fsnlp/},
year = {1999}
}
@inproceedings{mescheder2017gan,
abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
archivePrefix = {arXiv},
arxivId = {1705.10461},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
doi = {10.1021/jp0466845},
eprint = {1705.10461},
issn = {00104841},
pages = {1825--1835},
title = {{The numerics of GANs}},
url = {http://arxiv.org/pdf/1705.10461.pdf},
year = {2017}
}
@inproceedings{fruit2018efficient,
abstract = {We introduce SCAL, an algorithm designed to perform efficient exploration-exploitation in any unknown weakly-communicating Markov decision process (MDP) for which an upper bound c on the span of the optimal bias function is known. For an MDP with S states, A actions and $\Gamma$ ≤ S possible next states, we prove a regret bound of Oe(c√$\Gamma$SAT), which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter D. In fact, the optimal bias span is finite and often much smaller than D (e.g., D = ∞ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.},
author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
issn = {23318422},
pages = {1578--1586},
title = {{Efficient bias-span-constrained exploration-exploitation in reinforcement learning}},
url = {https://arxiv.org/pdf/1802.04020.pdf},
year = {2018}
}
@inproceedings{korda2013thompson,
abstract = {Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.},
archivePrefix = {arXiv},
arxivId = {1307.3400},
author = {Korda, Nathaniel and Kaufmann, Emilie and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
eprint = {1307.3400},
issn = {10495258},
pages = {1448--1456},
title = {{Thompson sampling for 1-dimensional exponential family bandits}},
url = {http://arxiv.org/pdf/1307.3400.pdf},
year = {2013}
}
@inproceedings{wu2015constrained,
abstract = {We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.},
archivePrefix = {arXiv},
arxivId = {1504.06937},
author = {Wu, Huasen and Srikant, R. and Liu, Xin and Jiang, Chong},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
eprint = {1504.06937},
issn = {10495258},
pages = {433--441},
title = {{Algorithms with logarithmic or sublinear regret for constrained contextual bandits}},
url = {https://arxiv.org/pdf/1504.06937.pdf},
year = {2015}
}
@article{kalai2006fpl,
abstract = {In an online decision problem, one makes a sequence of decisions without knowledge of the future. Each period, one pays a cost based on the decision and observed state. We give a simple approach for doing nearly as well as the best single decision, where the best is chosen with the benefit of hindsight. A natural idea is to follow the leader, i.e. each period choose the decision which has done best so far. We show that by slightly perturbing the totals and then choosing the best decision, the expected performance is nearly as good as the best decision in hindsight. Our approach, which is very much like Hannan's original game-theoretic approach from the 1950s, yields guarantees competitive with the more modern exponential weighting algorithms like Weighted Majority. More importantly, these follow-the-leader style algorithms extend naturally to a large class of structured online problems for which the exponential algorithms are inefficient. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Kalai, Adam and Vempala, Santosh},
doi = {10.1016/j.jcss.2004.10.016},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalai, Vempala - 2005 - Efficient algorithms for online decision problems(2).pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {Decision theory,Hannan's algorithm,Online algorithms,Optimization},
number = {3},
pages = {291--307},
title = {{Efficient algorithms for online decision problems}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2005-Efficient{\_}Algorithms{\_}for{\_}Online{\_}Decision{\_}Problems.pdf},
volume = {71},
year = {2005}
}
@inproceedings{joseph2017better,
abstract = {We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in Joseph et al. [2016], we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.},
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
booktitle = {4th Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph et al. - 2017 - Better fair algorithms for contextual bandits.pdf:pdf},
title = {{Better fair algorithms for contextual bandits}},
url = {https://www.fatml.org/media/documents/better{\_}fair{\_}algorithms{\_}for{\_}contextual{\_}bandits.pdf},
year = {2017}
}
@article{jaksch2010ucrl,
address = {Cambridge, MA, USA},
author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaksch, Ortner, Auer - 2010 - Near-optimal regret bounds for reinforcement learning.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bandits},
mendeley-tags = {bandits},
month = {aug},
pages = {1563--1600},
publisher = {MIT Press},
title = {{Near-optimal regret bounds for reinforcement learning}},
url = {https://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf},
volume = {99},
year = {2010}
}
@inproceedings{azar2017minimax,
abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of Ō(√IISAT+II2 S2A+II√T) where H is the time horizon, S the number of states, A the number of actions and T the number of timesteps. This result improves over the best previous known bound Ō(HS√AT) achieved by the UCRL2 algorithm of Jaksch et al. (2010). The key significance of our new results is that when T ≥ H3S3A and SA ≥ H, it leads to a regret of Ō(√HSAT) that matches the established lower bound of $\Omega$(√HSAT) up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we define Bernstein-based "exploration bonuses" that use the empirical variance of the estimated values at the next states (to improve scaling in H).},
archivePrefix = {arXiv},
arxivId = {1703.05449},
author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.05449},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azar, Osband, Munos - 2017 - Minimax regret bounds for reinforcement learning.pdf:pdf},
isbn = {9781510855144},
pages = {405--433},
title = {{Minimax regret bounds for reinforcement learning}},
url = {https://arxiv.org/pdf/1703.05449.pdf},
year = {2017}
}
@article{goldberg2016nlp,
author = {Goldberg, Yoav},
journal = {Journal of Artificial Intelligence Research},
pages = {345--420},
title = {{A primer on neural network models for natural language processing}},
url = {http://www.jair.org/index.php/jair/article/view/11030},
volume = {57},
year = {2016}
}
@article{Author2020,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - Provable Online Learning and Bandit Algorithms with Neural Networks.pdf:pdf},
title = {{Provable online learning and bandit algorithms with neural networks}},
year = {2020}
}
@inproceedings{agrawal2012analysis,
abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time {\$}T{\$} is {\$}O(\backslashfrac{\{}\backslashln T{\}}{\{}\backslashDelta{\}} + \backslashfrac{\{}1{\}}{\{}\backslashDelta{\^{}}3{\}}){\$}. And, for the {\$}N{\$}-armed bandit problem, the expected regret in time {\$}T{\$} is {\$}O([(\backslashsum{\_}{\{}i=2{\}}{\^{}}N \backslashfrac{\{}1{\}}{\{}\backslashDelta{\_}i{\^{}}2{\}}){\^{}}2] \backslashln T){\$}. Our bounds are optimal but for the dependence on {\$}\backslashDelta{\_}i{\$} and the constant factors in big-Oh.},
archivePrefix = {arXiv},
arxivId = {1111.1797},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the 25th Conference on Learning Theory (CoLT)},
doi = {arXiv:1111.1797},
eprint = {1111.1797},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Goyal - 2012 - Analysis of Thompson sampling for the multi-armed bandit problem.pdf:pdf},
isbn = {0960-3115},
issn = {15337928},
pages = {1--26},
pmid = {618},
title = {{Analysis of Thompson sampling for the multi-armed bandit problem}},
url = {http://arxiv.org/pdf/1111.1797.pdf},
year = {2012}
}
@inproceedings{kakade2002npg,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Alt hough gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal act ion rat her than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9|. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
author = {Kakade, Sham},
booktitle = {Advances in Neural Information Processing Systems 15 (NIPS)},
isbn = {0262042088},
issn = {10495258},
title = {{A natural policy gradient}},
url = {https://papers.nips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf},
year = {2002}
}
@inproceedings{smith2013,
abstract = {Abstract We provide a general technique for making online learning algorithms differentially private, in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a{\{}$\backslash$textbackslash{\}} emph {\{}convex{\}} loss function which is a sum of smaller convex loss terms, one for each data point. We modify the popular{\{}$\backslash$textbackslash{\}} emph {\{}mirror descent{\}} approach, or rather a variant called{\{}$\backslash$textbackslash{\}} emph {\{}follow the approximate leader{\}}. The technique leads to the ...},
author = {Smith, Adam and Thakurta, Abhradeep},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
issn = {10495258},
pages = {2733--2741},
title = {{(Nearly) Optimal algorithms for private online learning in full-information and bandit settings}},
url = {https://papers.nips.cc/paper/5012-nearly-optimal-algorithms-for-private-online-learning-in-full-information-and-bandit-settings.pdf},
year = {2013}
}
@article{dean2019lqr,
abstract = {This paper addresses the optimal control problem known as the linear quadratic regulator in the case when the dynamics are unknown. We propose a multistage procedure, called Coarse-ID control, that estimates a model from a few experimental trials, estimates the error in that model with respect to the truth, and then designs a controller using both the model and uncertainty estimate. Our technique uses contemporary tools from random matrix theory to bound the error in the estimation procedure. We also employ a recently developed approach to control synthesis called System Level Synthesis that enables robust control design by solving a quasi-convex optimization problem. We provide end-to-end bounds on the relative error in control cost that are optimal in the number of parameters and that highlight salient properties of the system to be controlled such as closed-loop sensitivity and optimal control magnitude. We show experimentally that the Coarse-ID approach enables efficient computation of a stabilizing controller in regimes where simple control schemes that do not take the model uncertainty into account fail to stabilize the true system.},
archivePrefix = {arXiv},
arxivId = {1710.01688},
author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
doi = {10.1007/s10208-019-09426-y},
eprint = {1710.01688},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dean et al. - 2019 - On the sample complexity of the linear quadratic regulator.pdf:pdf},
issn = {16153383},
journal = {Foundations of Computational Mathematics},
keywords = {Optimal control,Reinforcement learning,Robust control,Statistical learning theory,System identification,System level synthesis},
title = {{On the sample complexity of the linear quadratic regulator}},
url = {https://arxiv.org/pdf/1710.01688.pdf},
year = {2019}
}
@inproceedings{barret2020ecodqn,
abstract = {We present a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. These results, albeit still quite far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.},
archivePrefix = {arXiv},
arxivId = {1611.09940},
author = {Barret, Thomas D. and Clements, William R. and Foerster, Jakob N. and Lvovsky, Alex I.},
booktitle = {Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {1611.09940},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barret et al. - 2020 - Exploratory combinatorial optimization with reinforcement learning.pdf:pdf},
title = {{Exploratory combinatorial optimization with reinforcement learning}},
url = {https://arxiv.org/pdf/1909.04063.pdf},
year = {2020}
}
@inproceedings{amani2020generalized,
abstract = {The classical multi-armed bandit is a class of sequential decision making problems where selecting actions incurs costs that are sampled independently from an unknown underlying distribution. Bandit algorithms have many applications in safety critical systems, where several constraints must be respected during the run of the algorithm in spite of uncertainty about problem parameters. This paper formulates a generalized linear stochastic multi-armed bandit problem with generalized linear safety constraints that depend on an unknown parameter vector. In this setting, we propose a Safe UCB-GLM algorithm for which we provide general and problem-dependent regret bounds.},
archivePrefix = {arXiv},
arxivId = {2012.00314},
author = {Amani, Sanae and Alizadeh, Mahnoosh and Thrampoulidis, Christos},
booktitle = {Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP40776.2020.9054063},
eprint = {2012.00314},
isbn = {9781509066315},
issn = {15206149},
keywords = {Exploration-Exploitation,Linear Bandit,Multi-armed bandit,Safe learning},
pages = {3562--3566},
title = {{Generalized linear bandits with safety constraints}},
year = {2020}
}
@inproceedings{bubeck2013bounded,
abstract = {We study the stochastic multi-armed bandit problem when one knows the value {\$}\backslashmu{\^{}}{\{}(\backslashstar){\}}{\$} of an optimal arm, as a well as a positive lower bound on the smallest positive gap {\$}\backslashDelta{\$}. We propose a new randomized policy that attains a regret {\{}$\backslash$em uniformly bounded over time{\}} in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows {\$}\backslashDelta{\$}, and bounded regret of order {\$}1/\backslashDelta{\$} is not possible if one only knows {\$}\backslashmu{\^{}}{\{}(\backslashstar){\}}{\$}},
author = {Bubeck, S{\'{e}}bastien and Perchet, Vianney and Rigollet, Philippe},
booktitle = {Proceedings of the 26th Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck, Perchet, Rigollet - 2013 - Bounded regret in stochastic multi-armed bandits(2).pdf:pdf},
title = {{Bounded regret in stochastic multi-armed bandits}},
url = {http://proceedings.mlr.press/v30/Bubeck13.pdf},
year = {2013}
}
@inproceedings{kocsis2006bandit,
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 17th European Conference on Machine Learning (ECML)},
keywords = {bandits},
mendeley-tags = {bandits},
title = {{Bandit-based Monte-Carlo planning}},
url = {http://ggp.stanford.edu/readings/uct.pdf},
year = {2006}
}
@inproceedings{wu2015gaussian,
abstract = {We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i, j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).},
archivePrefix = {arXiv},
arxivId = {1510.08108},
author = {Wu, Yifan and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
eprint = {1510.08108},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Gy{\"{o}}rgy, Szepesv{\'{a}}ri - 2015 - Online learning with Gaussian payoffs and side observations.pdf:pdf},
issn = {10495258},
pages = {1360--1368},
title = {{Online learning with Gaussian payoffs and side observations}},
url = {https://arxiv.org/pdf/1510.08108.pdf},
year = {2015}
}
@inproceedings{sen2018mfdoo,
abstract = {Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is that of hyper-parameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations -- this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion. We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning. We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret. We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.},
author = {Sen, Rajat and Kandasamy, Kirthevasan and Shakkottai, Sanjay},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
issn = {1938-7228},
pages = {4538--4547},
title = {{Multi-fidelity black-box optimization with hierarchical partitions}},
url = {https://www.cs.cmu.edu/{~}kkandasa/pubs/senICML18mfdoo.pdf},
volume = {80},
year = {2018}
}
@book{ziemba2010,
author = {Ziemba, William T. and Vickson, Raymond G.},
booktitle = {The Journal of Risk and Insurance},
issn = {1095-8541},
pmid = {21126524},
title = {{Stochastic Optimization Models in Finance}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1539-6975.2010.01374.x},
year = {2010}
}
@inproceedings{wu2016conservative,
abstract = {We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the design of those algorithms makes them unsuitable under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose natural yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.},
archivePrefix = {arXiv},
arxivId = {1602.04282},
author = {Wu, Yifan and Shariff, Roshan and Lattimore, Tor and Szepesvari, Csaba},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
eprint = {1602.04282},
isbn = {9781510829008},
pages = {1917--1925},
title = {{Conservative bandits}},
year = {2016}
}
@inproceedings{jin2018is,
abstract = {Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [7, 22]. The theoretical question of “whether model-free algorithms can be made sample efficient” is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret {\~{O}}(H3SAT), where S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single H factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes T regret without requiring access to a “simulator.”},
archivePrefix = {arXiv},
arxivId = {1807.03765},
author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1807.03765},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2018 - Is Q-learning provably efficient(2).pdf:pdf},
issn = {10495258},
pages = {4863--4873},
title = {{Is Q-learning provably efficient?}},
url = {https://arxiv.org/pdf/1807.03765.pdf},
year = {2018}
}
@inproceedings{krause11contextual,
author = {Krause, Andreas and Ong, Cheng Soon},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krause, Ong - 2011 - Contextual Gaussian process bandit optimization.pdf:pdf},
pages = {2447--2455},
title = {{Contextual Gaussian process bandit optimization}},
url = {https://papers.nips.cc/paper/4487-contextual-gaussian-process-bandit-optimization.pdf},
year = {2011}
}
@inproceedings{khurana2016feature,
abstract = {Feature Engineering is the task of transforming the feature space in a given learning problem to improve the performance of a trained model. It is a crucial but time-intensive and skillful process, involving a data scientist or a domain expert. It is often the key determinant of the time and cost required to build an effective learner. In this paper, we discuss our system for performing feature engineering in an automated manner using a combination of exploratory and learning techniques. We also mention our larger charter of an automated data science pipeline.},
author = {Khurana, Udayan and Nargesian, Fatemeh and Samulowitz, Horst and Khalil, Elias and Turaga, Deepak},
booktitle = {Workshop on Artificial Intelligence for Data Science at Neural Information Processing Systems (NIPS-AI4DataSci)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khurana et al. - 2016 - Automating feature engineering.pdf:pdf},
title = {{Automating feature engineering}},
url = {http://workshops.inf.ed.ac.uk/nips2016-ai4datasci/papers/NIPS2016-AI4DataSci{\_}paper{\_}13.pdf},
year = {2016}
}
@inproceedings{degenne2020structure,
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the sub-optimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
archivePrefix = {arXiv},
arxivId = {2007.00969},
author = {Degenne, R{\'{e}}my and Shao, Han and Koolen, Wouter M.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
eprint = {2007.00969},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degenne, Shao, Koolen - 2020 - Structure Adaptive Algorithms for Stochastic Bandits.pdf:pdf},
title = {{Structure Adaptive Algorithms for Stochastic Bandits}},
url = {http://arxiv.org/pdf/2007.00969.pdf},
year = {2020}
}
@inproceedings{Saxton2019,
abstract = {Mathematical reasoning-a core ability within human intelligence-presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.01557v1},
author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1904.01557v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxton et al. - 2019 - Analysing mathematical reasoning abilities of neural models.pdf:pdf},
title = {{Analysing mathematical reasoning abilities of neural models}},
url = {https://arxiv.org/pdf/1904.01557.pdf},
year = {2019}
}
@inproceedings{pennington2014glove,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global vectors for word representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global vectors for word representation}},
url = {https://nlp.stanford.edu/pubs/glove.pdf},
year = {2014}
}
@inproceedings{yu2018heavy,
author = {Yu, Xiaotian and Shao, Han and Lyu, Michael R and King, Irwin},
booktitle = {Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2018 - Pure exploration of multi-armed bandits with heavy-tailed payoffs.pdf:pdf},
title = {{Pure exploration of multi-armed bandits with heavy-tailed payoffs}},
url = {http://auai.org/uai2018/proceedings/papers/332.pdf},
year = {2018}
}
@inproceedings{chaudhuri2018infinite,
author = {Chaudhuri, Arghya Roy and Kalyanakrishnan, Shivaram},
booktitle = {Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI)},
title = {{Quantile-regret minimisation in infinitely many-armed bandits}},
url = {https://www.cse.iitb.ac.in/{~}shivaram/papers/rk{\_}uai{\_}2018.pdf},
year = {2018}
}
@inproceedings{skilling2012gmc,
abstract = {We hold this truth to be self-evident, that good principle and good practice go hand in hand. The principles of Bayesian analysis derive from elementary symmetries, and nothing more. In sympathy with those same symmetries, and noting that every invariance broken is generality forgone, we develop the practice of Bayesian computation. This approach leads to nested sampling and Galilean Monte Carlo. Nested sampling is the canonical prior-to-posterior compression algorithm, and Galilean Monte Carlo (GMC) is the canonical multidimensional exploration strategy. Though inspired by high dimension, these general methods apply to problems of all size.},
author = {Skilling, John},
booktitle = {AIP Conference Proceedings},
doi = {10.1063/1.3703630},
isbn = {9780735410398},
issn = {0094243X},
keywords = {Bayesian computation,Galilean Monte Carlo,nested sampling},
pages = {145--156},
title = {{Bayesian computation in big spaces - Nested sampling and Galilean Monte Carlo}},
url = {https://aip.scitation.org/doi/10.1063/1.3703630},
volume = {1443},
year = {2012}
}
@inproceedings{locatelli2016thresholding,
abstract = {We study a specific $\backslash$textit{\{}combinatorial pure exploration stochastic bandit problem{\}} where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and $\backslash$textit{\{}for a fixed time horizon{\}}. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with $\backslash$textit{\{}fixed budget{\}} for which optimal strategies are constructed.},
author = {Locatelli, Andrea and Gutzeit, Maurilio and Carpentier, Alexandra},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Locatelli, Gutzeit, Carpentier - 2016 - An optimal algorithm for the thresholding bandit problem.pdf:pdf},
isbn = {9781510829008},
pages = {2539--2554},
title = {{An optimal algorithm for the thresholding bandit problem}},
url = {http://proceedings.mlr.press/v48/locatelli16.pdf},
year = {2016}
}
@article{battaglia2018gnn,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and graph networks.pdf:pdf},
journal = {arXiv preprint arXiv:1806.01261},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/pdf/1806.01261.pdf},
year = {2018}
}
@article{srivastava2014dropout,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@inproceedings{cherief-abdellatif2019,
abstract = {Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even with model mismatch and adversaries. Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference ? In this paper, we show that this is indeed the case for some variational inference (VI) algorithms. We consider a few existing online, tempered VI algorithms, as well as a new algorithm, and derive their generalization bounds. Our theoretical result relies on the convexity of the variational objective, but we argue that the result should hold more generally and present empirical evidence in support of this. Our work in this paper presents theoretical justifications in favor of online algorithms relying on approximate Bayesian methods.},
archivePrefix = {arXiv},
arxivId = {1904.03920},
author = {Ch{\'{e}}rief-Abdellatif, Badr Eddine and Alquier, Pierre and Khan, Mohammad Emtiyaz},
booktitle = {Proceedings of the 11th Asian Conference on Machine Learning (ACML)},
eprint = {1904.03920},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ch{\'{e}}rief-Abdellatif, Alquier, Khan - 2019 - A generalization bound for online variational inference.pdf:pdf},
keywords = {Bayesian inference,Generalization bounds,Online learning,Variational inference},
pages = {662--677},
title = {{A generalization bound for online variational inference}},
url = {https://proceedings.mlr.press/v101/cherief-abdellatif19a/cherief-abdellatif19a.pdf},
year = {2019}
}
@inproceedings{chen2018autostacker,
abstract = {In this work, an automatic machine learning (AutoML) modeling architecture called Autostacker is introduced. Autostacker combines an innovative hierarchical stacking architecture and an evolutionary algorithm (EA) to perform efficient parameter search without the need for prior domain knowledge about the data or feature preprocessing. Using EA, Autostacker quickly evolves candidate pipelines with high predictive accuracy. These pipelines can be used in their given form, or serve as a starting point for further augmentation and refinement by human experts. Autostacker finds innovative machine learning model combinations and structures, rather than selecting a single model and optimizing its hyperparameters. When its performance on fifteen datasets is compared with that of other AutoML systems, Autostacker produces superior or competitive results in terms of both test accuracy and time cost.},
archivePrefix = {arXiv},
arxivId = {1803.00684},
author = {Chen, Boyuan and Wu, Harvey and Mo, Warren and Chattopadhyay, Ishanu},
booktitle = {Proceedings of the 2018 Genetic and Evolutionary Computation Conference (GECCO)},
doi = {10.1145/3205455.3205586},
eprint = {1803.00684},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2018 - Autostacker A compositional evolutionary learning system.pdf:pdf},
isbn = {9781450356183},
keywords = {AutoML,Evolutionary machine learning,Machine learning},
pages = {402--409},
title = {{Autostacker: A compositional evolutionary learning system}},
url = {https://arxiv.org/pdf/1803.00684.pdf},
year = {2018}
}
@article{marjani2020bpi,
abstract = {We investigate the problem of best-policy identification in discounted Markov Decision Processes (MDPs) when the learner has access to a generative model. The objective is to devise a learning algorithm returning the best policy as early as possible. We first derive a problem-specific lower bound of the sample complexity satisfied by any learning algorithm. This lower bound corresponds to an optimal sample allocation that solves a non-convex program, and hence, is hard to exploit in the design of efficient algorithms. We then provide a simple and tight upper bound of the sample complexity lower bound, whose corresponding nearly-optimal sample allocation becomes explicit. The upper bound depends on specific functionals of the MDP such as the sub-optimality gaps and the variance of the next-state value function, and thus really captures the hardness of the MDP. Finally, we devise KLB-TS (KL Ball Track-and-Stop), an algorithm tracking this nearly-optimal allocation, and provide asymptotic guarantees for its sample complexity (both almost surely and in expectation). The advantages of KLB-TS against state-of-the-art algorithms are discussed and illustrated numerically.},
archivePrefix = {arXiv},
arxivId = {2009.13405},
author = {Marjani, Aymen Al and Proutiere, Alexandre},
eprint = {2009.13405},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marjani, Proutiere - 2020 - Adaptive sampling for best policy identification in Markov decision processes.pdf:pdf},
journal = {arXiv preprint arXiv:2009.13405},
title = {{Adaptive sampling for best policy identification in Markov decision processes}},
url = {http://arxiv.org/pdf/2009.13405.pdf},
year = {2020}
}
@inproceedings{degenne2019game,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.10431v1},
author = {Degenne, R{\'{e}}my and Koolen, Wouter and M{\'{e}}nard, Pierre},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {arXiv:1906.10431v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degenne, Koolen, M{\'{e}}nard - Unknown - Non-asymptotic pure exploration by solving games.pdf:pdf},
title = {{Non-asymptotic pure exploration by solving games}},
url = {http://arxiv.org/pdf/1906.10431.pdf},
year = {2019}
}
@article{kano2019good,
abstract = {We consider a novel stochastic multi-armed bandit problem called good arm identification (GAI), where a good arm is defined as an arm with expected reward greater than or equal to a given threshold. GAI is a pure-exploration problem in which a single agent repeats a process of outputting an arm as soon as it is identified as a good one before confirming the other arms are actually not good. The objective of GAI is to minimize the number of samples for each process. We find that GAI faces a new kind of dilemma, the exploration-exploitation dilemma of confidence, which is different from the best arm identification. As a result, an efficient design of algorithms for GAI is quite different from that for the best arm identification. We derive a lower bound on the sample complexity of GAI that is tight up to the logarithmic factor O(log1$\delta$) for acceptance error rate $\delta$. We also develop an algorithm whose sample complexity almost matches the lower bound. We also confirm experimentally that our proposed algorithm outperforms naive algorithms in synthetic settings based on a conventional bandit problem and clinical trial researches for rheumatoid arthritis.},
archivePrefix = {arXiv},
arxivId = {1710.06360},
author = {Kano, Hideaki and Honda, Junya and Sakamaki, Kentaro and Matsuura, Kentaro and Nakamura, Atsuyoshi and Sugiyama, Masashi},
doi = {10.1007/s10994-019-05784-4},
eprint = {1710.06360},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kano et al. - 2019 - Good arm identification via bandit feedback.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Machine learning,Multi-armed bandits,Reinforcement learning,Thresholding bandits},
number = {5},
pages = {721--745},
title = {{Good arm identification via bandit feedback}},
url = {https://arxiv.org/pdf/1710.06360.pdf},
volume = {108},
year = {2019}
}
@inproceedings{papamakarios2017maf,
abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
archivePrefix = {arXiv},
arxivId = {1705.07057},
author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
doi = {10.1080/0955300001003760},
eprint = {1705.07057},
isbn = {0306-7319},
issn = {10495258},
pages = {2338--2347},
pmid = {11236924},
title = {{Masked autoregressive flow for density estimation}},
url = {http://arxiv.org/pdf/1705.07057.pdf},
year = {2017}
}
@inproceedings{dann2017unifying,
abstract = {Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.},
archivePrefix = {arXiv},
arxivId = {1703.07710},
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1703.07710},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dann, Lattimore, Brunskill - 2017 - Unifying PAC and regret Uniform PAC bounds for episodic reinforcement learning.pdf:pdf},
issn = {10495258},
pages = {5714--5724},
title = {{Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning}},
url = {https://arxiv.org/pdf/1703.07710.pdf},
year = {2017}
}
@inproceedings{li2017hyperband,
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Talwalkar, Ameet and Rostamizadeh, Afshin},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Hyperband Bandit-based configuration evaluation for hyperparameter optimization.pdf:pdf},
title = {{Hyperband: Bandit-based configuration evaluation for hyperparameter optimization}},
url = {https://openreview.net/pdf?id=ry18Ww5ee},
year = {2017}
}
@inproceedings{tossou2017,
abstract = {In this paper, we improve the previously best known regret bound to achieve {\$}\backslashepsilon{\$}-differential privacy in oblivious adversarial bandits from {\$}\backslashmathcal{\{}O{\}}{\{}(T{\^{}}{\{}2/3{\}}/\backslashepsilon){\}}{\$} to {\$}\backslashmathcal{\{}O{\}}{\{}(\backslashsqrt{\{}T{\}} \backslashln T /\backslashepsilon){\}}{\$}. This is achieved by combining a Laplace Mechanism with EXP3. We show that though EXP3 is already differentially private, it leaks a linear amount of information in {\$}T{\$}. However, we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions. This allows us to reach {\$}\backslashmathcal{\{}O{\}}{\{}(\backslashsqrt{\{}\backslashln T{\}}){\}}{\$}-DP, with a regret of {\$}\backslashmathcal{\{}O{\}}{\{}(T{\^{}}{\{}2/3{\}}){\}}{\$} that holds against an adaptive adversary, an improvement from the best known of {\$}\backslashmathcal{\{}O{\}}{\{}(T{\^{}}{\{}3/4{\}}){\}}{\$}. This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis.},
archivePrefix = {arXiv},
arxivId = {1701.04222},
author = {Tossou, Aristide and Dimitrakakis, Christos},
booktitle = {Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {1701.04222},
pages = {2653--2659},
title = {{Achieving privacy in the adversarial multi-armed bandit}},
url = {http://arxiv.org/pdf/1701.04222.pdf},
year = {2017}
}
@inproceedings{zanette2019euler,
abstract = {Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question (Jiang {\&} Agarwal, 2018) about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.},
archivePrefix = {arXiv},
arxivId = {1901.00210},
author = {Zanette, Andrea and Brunskill, Emma},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1901.00210},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zanette, Brunskill - 2019 - Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value funct.pdf:pdf},
isbn = {9781510886988},
pages = {12676--12684},
title = {{Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds}},
url = {https://arxiv.org/pdf/1901.00210.pdf},
year = {2019}
}
@article{jin2018autokeras,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
eprint = {1806.10282},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin, Song, Hu - 2018 - Auto-Keras An efficient neural architecture search system(2).pdf:pdf},
journal = {arXiv preprint arXiv:1806.10282},
keywords = {automated machine learning,automl,bayesian optimization,network morphism,neural architecture search},
title = {{Auto-Keras: An efficient neural architecture search system}},
url = {http://arxiv.org/pdf/1806.10282.pdf},
year = {2018}
}
@inproceedings{tao2020novelty,
abstract = {We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representational space to gauge novelty. We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space for hard exploration tasks with sparse rewards. One key element of our approach is the use of information theoretic principles to shape our representations in a way so that our novelty reward goes beyond pixel similarity. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines.},
archivePrefix = {arXiv},
arxivId = {2009.13579},
author = {Tao, Ruo Yu and Fran{\c{c}}ois-Lavet, Vincent and Pineau, Joelle},
booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
eprint = {2009.13579},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tao, Fran{\c{c}}ois-Lavet, Pineau - 2020 - Novelty search in representational space for sample efficient exploration.pdf:pdf},
title = {{Novelty search in representational space for sample efficient exploration}},
url = {http://arxiv.org/pdf/2009.13579.pdf},
year = {2020}
}
@inproceedings{hsieh2019lifetime,
abstract = {Sequential decision making for lifetime maximization is a critical problem in many real-world applications, such as medical treatment and portfolio selection. In these applications, a "reneging" phenomenon, where participants may disengage from future interactions after observing an unsatisfiable outcome, is rather prevalent. To address the above issue, this paper proposes a model of heteroscedastic linear bandits with reneging, which allows each participant to have a distinct "satisfaction level," with any interaction outcome falling short of that level resulting in that participant reneging. Moreover, it allows the variance of the outcome to be context-dependent. Based on this model, we develop a UCB-type policy, namely HR-UCB, and prove that it achieves {\$}\backslashmathcal{\{}O{\}}\backslashbig(\backslashsqrt{\{}{\{}T{\}}(\backslashlog({\{}T{\}})){\^{}}{\{}3{\}}{\}}\backslashbig){\$} regret. Finally, we validate the performance of HR-UCB via simulations.},
archivePrefix = {arXiv},
arxivId = {1810.12418},
author = {Hsieh, Ping-Chun and Liu, Xi and Bhattacharya, Anirban and Kumar, P. R.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1810.12418},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh et al. - 2019 - Stay with me Lifetime maximization through heteroscedastic linear bandits with reneging.pdf:pdf},
title = {{Stay with me: Lifetime maximization through heteroscedastic linear bandits with reneging}},
url = {http://arxiv.org/pdf/1810.12418.pdf},
year = {2019}
}
@article{bergstra2012random,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Bergstra, James and Bengio, Yoshua},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Bengio - 2012 - Random search for hyper-parameter optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random search for hyper-parameter optimization}},
url = {http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@article{valiant1984pac,
abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learned using it in a reasonable (polynomial) number of steps. Although inherent algorithmic complexity appears to set serious limits to the range of concepts that can be learned, we show that there are some important nontrivial classes of propositional concepts that can be learned in a realistic sense.},
author = {Valiant, Leslie G.},
doi = {10.1145/1968.1972},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valiant - 1984 - A theory of the learnable.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {1134--1142},
title = {{A theory of the learnable}},
url = {http://portal.acm.org/citation.cfm?doid=1968.1972},
volume = {27},
year = {1984}
}
@article{gretton2005kernel,
abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis.},
author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1016/j.cellimm.2009.02.002},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {2075--2129},
title = {{Kernel methods for measuring independence}},
url = {http://www.jmlr.org/papers/volume6/gretton05a/gretton05a.pdf},
volume = {6},
year = {2005}
}
@article{kotowski2019pca,
abstract = {We consider a partial-feedback variant of the well-studied online PCA problem where a learner attempts to predict a sequence of {\$}d{\$}-dimensional vectors in terms of a quadratic loss, while only having limited feedback about the environment's choices. We focus on a natural notion of bandit feedback where the learner only observes the loss associated with its own prediction. Based on the classical observation that this decision-making problem can be lifted to the space of density matrices, we propose an algorithm that is shown to achieve a regret of {\$}O(d{\^{}}{\{}3/2{\}}\backslashsqrt{\{}T{\}}){\$} after {\$}T{\$} rounds in the worst case. We also prove data-dependent bounds that improve on the basic result when the loss matrices of the environment have bounded rank or the loss of the best action is bounded. One version of our algorithm runs in {\$}O(d){\$} time per trial which massively improves over every previously known online PCA method. We complement these results by a lower bound of {\$}\backslashOmega(d\backslashsqrt{\{}T{\}}){\$}.},
archivePrefix = {arXiv},
arxivId = {1902.03035},
author = {Kot{\l}owski, Wojciech and Neu, Gergely},
eprint = {1902.03035},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kot{\l}owski, Neu - 2019 - Bandit principal component analysis.pdf:pdf},
journal = {arXiv preprint arXiv:1902.03035},
keywords = {bandit pca,online linear optimization,online pca,phase retrieval},
title = {{Bandit principal component analysis}},
url = {http://arxiv.org/pdf/1902.03035.pdf},
year = {2019}
}
@article{zhang2019sgp,
abstract = {Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: i) Conventional GP inference scales {\$}O(N{\^{}}{\{}3{\}}){\$} with respect to the number of observations; ii) updating a GP model sequentially is not trivial; and iii) covariance kernels often enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose an online sequential Monte Carlo algorithm to fit mixtures of GPs that capture non-stationary behavior while allowing for fast, distributed inference. By formulating hyperparameter optimization as a multi-armed bandit problem, we accelerate mixing for real time inference. Our approach empirically improves performance over state-of-the-art methods for online GP estimation in the context of prediction for simulated non-stationary data and hospital time series data.},
archivePrefix = {arXiv},
arxivId = {1905.10003},
author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A. and Engelhardt, Barbara E.},
eprint = {1905.10003},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Sequential Gaussian processes for online learning of nonstationary functions.pdf:pdf},
journal = {arXiv preprint arXiv:1905.10003},
title = {{Sequential Gaussian processes for online learning of nonstationary functions}},
url = {http://arxiv.org/pdf/1905.10003.pdf},
year = {2019}
}
@inproceedings{khezeli2020safe,
archivePrefix = {arXiv},
arxivId = {arXiv:1911.09501v1},
author = {Khezeli, Kia and Bitar, Eilyan},
booktitle = {Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {arXiv:1911.09501v1},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khezeli, Bitar - 2020 - Safe linear stochastic bandits.pdf:pdf},
title = {{Safe linear stochastic bandits}},
url = {https://arxiv.org/pdf/1911.09501.pdf},
year = {2020}
}
@article{wang2007,
abstract = {Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner's perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.},
author = {Wang, Gary G. and Shan, Songqing},
doi = {10.1115/1.2429697},
isbn = {0-7918-4255-X},
issn = {10500472},
journal = {Journal of Mechanical Design},
number = {4},
pages = {370},
title = {{Review of metamodeling techniques in support of engineering design optimization}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1449318},
volume = {129},
year = {2007}
}
@article{yang2019laplacian,
abstract = {We study contextual multi-armed bandit problems in the case of multiple users, where we exploit the structure in the user domain to reduce the cumulative regret. Specifically, we model user relation as a graph, and assume that the parameters (preferences) of users form smooth signals on the graph. This leads to a graph Laplacian-regularized estimator, for which we propose a novel bandit algorithm whose performance depends on a notion of local smoothness on the graph. We provide a closed-form solution to the estimator, enabling a theoretical analysis on the convergence property of the estimator as well as single-user upper confidence bound (UCB) and cumulative regret of the proposed bandit algorithm. Furthermore, we show that the regret scales linearly with the local smoothness measure, which approaches zero for densely connected graph. The single-user UCB also allows us to further propose an extension of the bandit algorithm, whose computational complexity scales linearly with the number of users. We support theoretical claims with empirical evidences, and demonstrate the advantage of the proposed algorithm in comparison with state-of-the-art graph-based bandit algorithms on both synthetic and real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1907.05632},
author = {Yang, Kaige and Dong, Xiaowen and Toni, Laura},
eprint = {1907.05632},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Dong, Toni - 2019 - Laplacian-regularized graph bandits Algorithms and theoretical analysis.pdf:pdf},
journal = {arXiv preprint arXiv:1907.05632},
title = {{Laplacian-regularized graph bandits: Algorithms and theoretical analysis}},
url = {http://arxiv.org/pdf/1907.05632.pdf},
year = {2019}
}
@article{ren2017fasterrcnn,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2017 - Faster R-CNN Towards real-time object detection with region proposal networks(2).pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards real-time object detection with region proposal networks}},
url = {https://arxiv.org/pdf/1506.01497.pdf},
volume = {39},
year = {2017}
}
@article{cho2020deepbo,
author = {Cho, Hyunghun and Kim, Yongjin and Lee, Eunjung and Choi, Daeyoung and Lee, Yongjae},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2020 - Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of deep neural networks.pdf:pdf},
journal = {IEEE Access},
title = {{Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of deep neural networks}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=9037259},
volume = {8},
year = {2020}
}
@inproceedings{zoph2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01578v2},
author = {Zoph, Barret and Le, Quoc V},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1611.01578v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Le - 2017 - Neural architecture search with reinforcement learning.pdf:pdf},
title = {{Neural architecture search with reinforcement learning}},
url = {https://arxiv.org/pdf/1611.01578.pdf},
year = {2017}
}
@article{zhao2018offloading,
author = {Zhao, Shangshu and Zhu, Zhaowei and Yang, Fuqian and Luo, Xiliang},
journal = {arXiv preprint arXiv:1806.10547},
title = {{Online optimal task offloading with one-bit feedback}},
url = {https://arxiv.org/pdf/1806.10547.pdf},
year = {2018}
}
@inproceedings{liu2019darts,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1806.09055},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Simonyan, Yang - 2019 - DARTS Differentiable architecture search.pdf:pdf},
title = {{DARTS: Differentiable architecture search}},
url = {http://arxiv.org/pdf/1806.09055.pdf},
year = {2019}
}
@article{guo2018cpc,
abstract = {Unsupervised representation learning has succeeded with excellent results in many applications. It is an especially powerful tool to learn a good representation of environments with partial or noisy observations. In partially observable domains it is important for the representation to encode a belief state, a sufficient statistic of the observations seen so far. In this paper, we investigate whether it is possible to learn such a belief representation using modern neural architectures. Specifically, we focus on one-step frame prediction and two variants of contrastive predictive coding (CPC) as the objective functions to learn the representations. To evaluate these learned representations, we test how well they can predict various pieces of information about the underlying state of the environment, e.g., position of the agent in a 3D maze. We show that all three methods are able to learn belief representations of the environment, they encode not only the state information, but also its uncertainty, a crucial aspect of belief states. We also find that for CPC multi-step predictions and action-conditioning are critical for accurate belief representations in visually complex environments. The ability of neural representations to capture the belief information has the potential to spur new advances for learning and planning in partially observable domains, where leveraging uncertainty is essential for optimal decision making.},
archivePrefix = {arXiv},
arxivId = {1811.06407},
author = {Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo A. and Pohlen, Toby and Munos, R{\'{e}}mi},
eprint = {1811.06407},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2018 - Neural predictive belief representations.pdf:pdf},
journal = {arXiv preprint arXiv:1811.06407},
title = {{Neural predictive belief representations}},
url = {http://arxiv.org/pdf/1811.06407.pdf},
year = {2018}
}
@inproceedings{feraud2016rf,
abstract = {To address the contextual bandit problem, we propose an online random forest algorithm. The analysis of the proposed algorithm is based on the sample complexity needed to find the optimal decision stump. Then, the decision stumps are recursively stacked in a random collection of decision trees, BANDIT FOREST. We show that the proposed algorithm is optimal up to logarithmic factors. The dependence of the sample complexity upon the number of contextual variables is logarithmic. The computational cost of the proposed algorithm with respect to the time horizon is linear. These analytical results allow the proposed algorithm to be efficient in real applications, where the number of events to process is huge, and where we expect that some contextual variables, chosen from a large set, have potentially non-linear dependencies with the rewards. In the experiments done to illustrate the theoretical analysis, BANDIT FOREST obtain promising results in comparison with state-of-the-art algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.06952},
author = {F{\'{e}}raud, Rapha{\"{e}}l and Allesiardo, Robin and Urvoy, Tanguy and Cl{\'{e}}rot, Fabrice},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1504.06952},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/F{\'{e}}raud et al. - 2016 - Random forest for the contextual bandit problem(2).pdf:pdf},
pages = {93--101},
title = {{Random forest for the contextual bandit problem}},
url = {http://proceedings.mlr.press/v51/feraud16.pdf},
volume = {51},
year = {2016}
}
@book{bubeck2012bandits,
author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}}},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/9781601986276},
title = {{Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems}},
url = {https://arxiv.org/pdf/1204.5721.pdf},
year = {2012}
}
@phdthesis{ménard2018thesis,
author = {M{\'{e}}nard, Pierre},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\'{e}}nard - 2018 - Sur la notion d'optimalit{\'{e}} dans les probl{\`{e}}mes de bandit stochastique.pdf:pdf},
school = {Universit{\'{e}} de Toulouse},
title = {{Sur la notion d'optimalit{\'{e}} dans les probl{\`{e}}mes de bandit stochastique}},
url = {http://www.theses.fr/2018TOU30087},
year = {2018}
}
@inproceedings{ashtiani2018,
abstract = {We prove that {\$}\backslashtilde{\{}\backslashTheta{\}}(k d{\^{}}2 / \backslashvarepsilon{\^{}}2){\$} samples are necessary and sufficient for learning a mixture of {\$}k{\$} Gaussians in {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$}, up to error {\$}\backslashvarepsilon{\$} in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that {\$}\backslashtilde{\{}O{\}}(k d / \backslashvarepsilon{\^{}}2){\$} samples suffice, matching a known lower bound. Moreover, these results hold in the agnostic-learning/robust-estimation setting as well, where the target distribution is only approximately a mixture of Gaussians. The upper bound is shown using a novel technique for distribution learning based on a notion of `compression.' Any class of distributions that allows such a compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$} admits a small-sized compression scheme.},
archivePrefix = {arXiv},
arxivId = {1710.05209},
author = {Ashtiani, Hassan and Ben-David, Shai and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas and Plan, Yaniv},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1710.05209},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashtiani et al. - 2018 - Near-optimal sample complexity bounds for robust learning of Gaussians mixtures via sample compression schemes.pdf:pdf},
title = {{Near-optimal sample complexity bounds for robust learning of Gaussians mixtures via sample compression schemes}},
year = {2018}
}
@inproceedings{durand2018contextual,
abstract = {In this work, we present a specific case study where we aim to design effective treatment allocation strategies and validate these using a mouse model of skin cancer. Collecting data for modelling treatments effectiveness on animal models is an expensive and time consuming process. Moreover, acquiring this information during the full range of disease stages is hard to achieve with a conventional random treatment allocation procedure, as poor treatments cause deterioration of subject health. We therefore aim to design an adaptive allocation strategy to improve the efficiency of data collection by allocating more samples for exploring promising treatments. We cast this application as a contextual bandit problem and introduce a simple and practical algorithm for exploration-exploitation in this framework. The work builds on a recent class of approaches for non-contextual bandits that relies on subsampling to compare treatment options using an equivalent amount of information. On the technical side, we extend the subsampling strategy to the case of bandits with context, by applying subsampling within Gaussian Process regression. On the experimental side, preliminary results using 10 mice with skin tumours suggest that the proposed approach extends by more than 50{\%} the subjects life duration compared with baseline strategies: no treatment, random treatment allocation, and constant chemotherapeutic agent. By slowing the tumour growth rate, the adaptive procedure gathers information about treatment effectiveness on a broader range of tumour volumes, which is crucial for eventually deriving sequential pharmacological treatment strategies for cancer.},
author = {Durand, Audrey and Achilleos, Charis and Iacovides, Demetris and Strati, Katerina and Pineau, Joelle},
booktitle = {Proceedings of the 3rd Machine Learning for Health Care Conference (MLHC)},
title = {{Contextual bandits for adapting treatment in a mouse model of de Novo Carcinogenesis}},
url = {https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b737260cd83660fe34ea093/1534292576485/5.pdf},
year = {2018}
}
@inproceedings{mairal2009dictionary,
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2009 - Online dictionary learning for sparse coding.pdf:pdf},
title = {{Online dictionary learning for sparse coding}},
url = {https://arxiv.org/pdf/1811.12359.pdf},
year = {2009}
}
@inproceedings{dong2018,
abstract = {Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases. We establish new bounds that depend instead on a notion of rate-distortion. Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit. We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.},
archivePrefix = {arXiv},
arxivId = {1805.11845},
author = {Dong, Shi and {Van Roy}, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
eprint = {1805.11845},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Van Roy - 2018 - An information-theoretic analysis for Thompson sampling with many actions.pdf:pdf},
issn = {15337928},
pages = {4161--4169},
title = {{An information-theoretic analysis for Thompson sampling with many actions}},
url = {http://arxiv.org/pdf/1805.11845.pdf},
year = {2018}
}
@article{werner2019ranking,
abstract = {Ranking problems define a widely spread class of statistical learning problems with many applications, including fraud detection, document ranking, medicine, credit risk screening, image ranking or media memorability. In this article, we systematically describe different types of non-probabilistic supervised ranking problems, i.e., ranking problems that require the prediction of an order of the response variables, and the corresponding loss functions resp. goodness criteria. We discuss the difficulties when trying to optimize those criteria. As for a detailed and comprehensive overview of existing machine learning techniques to solve such ranking problems, we group the suitable techniques into SVM-, tree-, Boosting and Neural Network-type approaches and recapitulate the corresponding optimization problems in a unified notation. We also discuss to which of the ranking problems the respective algorithms are tailored and identify their strengths and limitations. Computational aspects and open research problems are also considered.},
archivePrefix = {arXiv},
arxivId = {1909.02998},
author = {Werner, Tino},
eprint = {1909.02998},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Werner - 2019 - A review on ranking problems in statistical learning.pdf:pdf},
journal = {arXiv preprint arXiv:1909.02998},
keywords = {1999,2002,2005,boosting,brefeld and scheffer,empirical risk minimization,herbrich et al,including theoretical work as,joachims,ranking problems,structural risk minimiza- rithms,supervised learning,surrogate losses,that use svms,tion,well as learning algo-},
title = {{A review on ranking problems in statistical learning}},
url = {http://arxiv.org/pdf/1909.02998.pdf},
year = {2019}
}
@article{ahipasaoglu2008fw,
abstract = {We show the linear convergence of a simple first-order algorithm for the minimum-volume enclosing ellipsoid problem and its dual, the D-optimal design problem of statistics. Using similar techniques, we show the linear convergence of the Frank-Wolfe algorithm with away steps applied to the simplex, under conditions different from those of Gulat and Marcotte. Computational tests confirm the attractive features of this method.},
author = {Ahipasaoglu, S. Damla and Sun, Peng and Todd, Michael J.},
doi = {10.1080/10556780701589669},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahipasaoglu, Sun, Todd - 2008 - Linear convergence of a modified Frank-Wolfe algorithm for computing minimum-volume enclosing ellipsoids.pdf:pdf},
issn = {10556788},
journal = {Optimization Methods and Software},
keywords = {Frank-Wolfe algorithm,Linear convergence,Minimum-volume ellipsoids,Optimizing on a simplex},
number = {1},
pages = {5--19},
title = {{Linear convergence of a modified Frank-Wolfe algorithm for computing minimum-volume enclosing ellipsoids}},
url = {https://www.tandfonline.com/doi/full/10.1080/10556780701589669},
volume = {23},
year = {2008}
}
@article{mnih2013dqn,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv:1312.5602},
pmid = {25719670},
title = {{Playing Atari with deep reinforcement learning}},
url = {http://arxiv.org/pdf/1312.5602.pdf},
year = {2013}
}
@book{shalev-shwartz2013foundations,
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
doi = {10.1017/CBO9781107298019},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shalev-Shwartz, Ben-David - 2014 - Understanding Machine Learning From Theory to Algorithms.pdf:pdf},
isbn = {9781107298019},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning: From Theory to Algorithms}},
url = {https://www.cs.huji.ac.il/{~}shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf},
year = {2014}
}
@article{kingma2013vae,
author = {Kingma, Diederik and Welling, Max},
journal = {arXiv preprint arXiv:1312.6114},
title = {{Auto-encoding variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf},
year = {2013}
}
@inproceedings{elsken2018simple,
abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6{\%} in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5{\%}.},
archivePrefix = {arXiv},
arxivId = {1711.04528},
author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
booktitle = {Workshop Track of the 6th International Conference on Learning Representations},
eprint = {1711.04528},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elsken, Metzen, Hutter - 2018 - Simple and efficient architecture search for convolutional neural networks.pdf:pdf},
title = {{Simple and efficient architecture search for convolutional neural networks}},
url = {https://arxiv.org/pdf/1711.04528.pdf},
year = {2018}
}
@article{kapoor2018news,
archivePrefix = {arXiv},
arxivId = {1806.09202},
author = {Kapoor, Sayash and Keswani, Vijay and Vishnoi, Nisheeth K. and Celis, L. Elisa},
eprint = {1806.09202},
journal = {arXiv preprint arXiv:1806.09202},
title = {{Balanced news using constrained bandit-based personalization}},
url = {http://arxiv.org/pdf/1806.09202.pdf},
year = {2018}
}
@inproceedings{sui2015safe,
abstract = {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified "safety" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.},
author = {Sui, Yanan and Gotovos, Alkis and Burdick, Joel W. and Krause, Andreas},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
isbn = {9781510810587},
pages = {997--1005},
title = {{Safe exploration for optimization with Gaussian processes}},
url = {https://proceedings.mlr.press/v37/sui15.pdf},
year = {2015}
}
@article{li2018delayed,
abstract = {This paper studies bandit learning problems with delayed feedback, which included multi-armed bandit (MAB) and bandit convex optimization (BCO). Given only function value information (a.k.a. bandit feedback), algorithms for both MAB and BCO typically rely on (possibly randomized) gradient estimators based on function values, and then feed them into well-studied gradient-based algorithms. Different from existing works however, the setting considered here is more challenging, where the bandit feedback is not only delayed but also the presence of its delay is not revealed to the learner. Existing algorithms for delayed MAB and BCO become intractable in this setting. To tackle such challenging settings, DEXP3 and DBGD have been developed for MAB and BCO, respectively. Leveraging a unified analysis framework, it is established that both DEXP3 and DBGD guarantee an {\$}{\{}\backslashcal O{\}}\backslashbig( \backslashsqrt{\{}T+D{\}} \backslashbig){\$} regret over {\$}T{\$} time slots with {\$}D{\$} being the overall delay accumulated over slots. The new regret bounds match those in full information settings.},
archivePrefix = {arXiv},
arxivId = {1807.03205},
author = {Li, Bingcong and Chen, Tianyi and Giannakis, Georgios B.},
eprint = {1807.03205},
journal = {arXiv preprint arXiv:1807.03205},
title = {{Delayed bandit online learning with unknown delays}},
url = {http://arxiv.org/abs/1807.03205},
year = {2018}
}
@inproceedings{dwork2008,
author = {Dwork, Cynthia},
booktitle = {Proceedings of the 5th International Conference on Theory and Applications of Models of Computation (TAMC)},
pages = {1--19},
title = {{Differential privacy: A survey of results}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-3-540-79228-4.pdf},
year = {2008}
}
@inproceedings{ha2019unknown,
abstract = {Applying Bayesian optimization in problems wherein the search space is unknown is challenging. To address this problem, we propose a systematic volume expansion strategy for the Bayesian optimization. We devise a strategy to guarantee that in iterative expansions of the search space, our method can find a point whose function value within epsilon of the objective function maximum. Without the need to specify any parameters, our algorithm automatically triggers a minimal expansion required iteratively. We derive analytic expressions for when to trigger the expansion and by how much to expand. We also provide theoretical analysis to show that our method achieves epsilon-accuracy after a finite number of iterations. We demonstrate our method on both benchmark test functions and machine learning hyper-parameter tuning tasks and demonstrate that our method outperforms baselines.},
archivePrefix = {arXiv},
arxivId = {1910.13092},
author = {Ha, Huong and Rana, Santu and Gupta, Sunil and Nguyen, Thanh and Tran-The, Hung and Venkatesh, Svetha},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1910.13092},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha et al. - 2019 - Bayesian optimization with unknown search space.pdf:pdf},
title = {{Bayesian optimization with unknown search space}},
url = {http://arxiv.org/pdf/1910.13092.pdf},
year = {2019}
}
@inproceedings{li2017ucbglm,
abstract = {Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}dT{\}}){\$} regret over {\$}T{\$} rounds with {\$}d{\$} dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a {\$}\backslashsqrt{\{}d{\}}{\$} factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.},
archivePrefix = {arXiv},
arxivId = {1703.00048},
author = {Li, Lihong and Lu, Yu and Zhou, Dengyong},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.00048},
isbn = {9781510855144},
pages = {2071--2080},
title = {{Provably optimal algorithms for generalized linear contextual bandits}},
url = {http://arxiv.org/pdf/1703.00048.pdf},
year = {2017}
}
@inproceedings{liu2018tranfer,
abstract = {Traditional recommendation systems (RecSys) suffer from two problems: the exploitation-exploration dilemma and the cold-start problem. One solution to solving the exploitation-exploration dilemma is the contextual bandit policy, which adaptively exploits and explores user interests. As a result, the contextual bandit policy achieves increased rewards in the long run. The contextual bandit policy, however, may cause the system to explore more than needed in the cold-start sit-uations, which can lead to worse short-term rewards. Cross-domain RecSys methods adopt transfer learning to leverage prior knowledge in a source RecSys domain to jump start the cold-start target RecSys. To solve the two problems to-gether, in this paper, we propose the first applicable trans-ferable contextual bandit (TCB) policy for the cross-domain recommendation. TCB not only benefits the exploitation but also accelerates the exploration in the target RecSys. TCB's exploration, in turn, helps to learn how to transfer between different domains. TCB is a general algorithm for both ho-mogeneous and heterogeneous domains. We perform both theoretical regret analysis and empirical experiments. The em-pirical results show that TCB outperforms the state-of-the-art algorithms over time.},
author = {Liu, Bo and Wei, Ying and Zhang, Yu and Yan, Zhixian and Yang, Qiang},
booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)},
keywords = {Machine Learning Methods Track},
pages = {3619--3626},
title = {{Transferable contextual bandit for cross-domain recommendation}},
url = {https://www.cse.ust.hk/{~}bliuab/files/aaai{\_}trUCB.pdf},
year = {2018}
}
@article{baltrusaitis2019,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baltrusaitis, Ahuja, Morency - 2019 - Multimodal machine learning A survey and taxonomy.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multimodal,introductory,machine learning,survey},
number = {2},
pages = {423--443},
title = {{Multimodal machine learning: A survey and taxonomy}},
url = {https://arxiv.org/pdf/1705.09406.pdf},
volume = {41},
year = {2019}
}
@inproceedings{kazerouni2017conservative,
abstract = {Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.},
archivePrefix = {arXiv},
arxivId = {1611.06426},
author = {Kazerouni, Abbas and Ghavamzadeh, Mohammad and Abbasi-Yadkori, Yasin and {Van Roy}, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1611.06426},
issn = {10495258},
pages = {3911--3920},
title = {{Conservative contextual linear bandits}},
url = {https://papers.nips.cc/paper/6980-conservative-contextual-linear-bandits.pdf},
year = {2017}
}
@article{ormoneit2002kernelrl,
abstract = {We consider the problem of approximating the cost-to-go functions in reinforcement learning. By mapping the state implicitly into a feature space, we perform a simple algorithm in the feature space, which corresponds to a complex algorithm in the original state space. Two kernel-based reinforcement learning algorithms, the $\epsilon$-insensitive kernel based reinforcement learning ($\epsilon$ KRL) and the least squares kernel based reinforcement learning (LS-KRL) are proposed. An example shows that the proposed methods can deal effectively with the reinforcement learning problem without having to explore many states. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Ormoneit, Dirk and Sen, {\AA}{\'{s}}aunak},
doi = {10.1023/A:1017928328829},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ormoneit, Sen - 2002 - Kernel-based reinforcement learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Kernel smoothing,Kernel-based learning,Lazy learning,Local averaging,Markov decision process,Reinforcement learning},
number = {2-3},
pages = {161--178},
title = {{Kernel-based reinforcement learning}},
volume = {49},
year = {2002}
}
@inproceedings{freeman2016topology,
abstract = {The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important $\backslash$emph{\{}folklore{\}} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. These results are in accordance with empirical practice and recent literature. {\%}Together with {\%}recent results that rigorously establish that no gradient descent can {\%}get stuck on saddle points, we conclude that gradient descent converges {\%}to a global optimum in deep rectified networks. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.},
archivePrefix = {arXiv},
arxivId = {1611.01540},
author = {Freeman, Daniel C. and Bruna, Joan},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
eprint = {1611.01540},
title = {{Topology and geometry of deep rectified network optimization landscapes}},
url = {http://arxiv.org/pdf/1611.01540.pdf},
year = {2017}
}
@inproceedings{dwork2012awareness,
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness. {\textcopyright} 2012 ACM.},
archivePrefix = {arXiv},
arxivId = {1104.3913},
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS)},
doi = {10.1145/2090236.2090255},
eprint = {1104.3913},
isbn = {9781450311151},
pages = {214--226},
title = {{Fairness through awareness}},
url = {https://arxiv.org/pdf/1104.3913.pdf},
year = {2012}
}
@inproceedings{mescheder2018gan,
abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
archivePrefix = {arXiv},
arxivId = {1801.04406},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
doi = {10.1007/s10584-011-0119-4},
eprint = {1801.04406},
isbn = {1938-7228},
issn = {0165-0009},
pmid = {132714},
title = {{Which training methods for GANs do actually converge?}},
url = {http://arxiv.org/pdf/1801.04406.pdf},
year = {2018}
}
@inproceedings{drugan2013,
abstract = {We propose an algorithmic framework for multi-objective multi-armed bandits with multiple rewards. Different partial order relationships from multi-objective optimization can be considered for a set of reward vectors, such as scalarization functions and Pareto search. A scalarization function transforms the multi-objective environment into a single objective environment and are a popular choice in multi-objective reinforcement learning. Scalarization techniques can be straightforwardly implemented into the current multi-armed bandit framework, but the efficiency of these algorithms depends very much on their type, linear or non-linear (e.g. Chebyshev), and their parameters. Using Pareto dominance order relationship allows to explore the multi-objective environment directly, however this can result in large sets of Pareto optimal solutions. In this paper we propose and evaluate the performance of multi-objective MABs using three regret metric criteria. The standard UCB1 is extended to scalarized multi-objective UCB1 and we propose a Pareto UCB1 algorithm. Both algorithms are proven to have a logarithmic upper bound for their expected regret. We also introduce a variant of the scalarized multi-objective UCB1 that removes online inefficient scalarizations in order to improve the algorithm's efficiency. These algorithms are experimentally compared on multi-objective Bernoulli distributions, Pareto UCB1 being the algorithm with the best empirical performance. {\textcopyright} 2013 IEEE.},
author = {Drugan, Madalina M. and Nowe, Ann},
booktitle = {Proceedings of the 2013 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2013.6707036},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drugan, Nowe - 2013 - Designing multi-objective multi-armed bandits algorithms A study.pdf:pdf},
isbn = {9781467361293},
pages = {2358--2365},
title = {{Designing multi-objective multi-armed bandits algorithms: A study}},
url = {http://geza.kzoo.edu/{~}erdi/IJCNN2013/HTMLFiles/PDFs/P343-1547.pdf},
year = {2013}
}
@book{slivkins2019bandits,
abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments. The chapters are as follows: stochastic bandits, lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual Bandits; bandits and games; bandits with knapsacks; bandits and incentives.},
archivePrefix = {arXiv},
arxivId = {1904.07272},
author = {Slivkins, Aleksandrs},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
eprint = {1904.07272},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Slivkins - 2019 - Introduction to Multi-Armed Bandits.pdf:pdf},
title = {{Introduction to Multi-Armed Bandits}},
url = {http://arxiv.org/pdf/1904.07272.pdf},
year = {2019}
}
@inproceedings{kats-samuels2019top,
author = {Katz-Samuels, Julian and Scott, Clayton},
booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AIStats)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz-Samuels, Scott - 2019 - Top feasible arm identification.pdf:pdf},
title = {{Top feasible arm identification}},
url = {http://proceedings.mlr.press/v89/katz-samuels19a/katz-samuels19a.pdf},
year = {2019}
}
@inproceedings{weisz2019caps,
author = {Weisz, Gell{\'{e}}rt and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weisz, Gy{\"{o}}rgy, Szepesv{\'{a}}ri - 2019 - CapsAndRuns An improved method for approximately optimal algorithm configuration.pdf:pdf},
keywords = {algorithm configuration,bernstein race,general purpose solvers},
title = {{CapsAndRuns : An improved method for approximately optimal algorithm configuration}},
url = {http://proceedings.mlr.press/v97/weisz19a/weisz19a.pdf},
year = {2019}
}
@book{murphy2012,
author = {Murphy, Kevin P.},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:pdf},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {https://mitpress.mit.edu/books/machine-learning-1},
year = {2012}
}
@inproceedings{wu2017,
abstract = {Bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledgegradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
archivePrefix = {arXiv},
arxivId = {1703.04389},
author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew Gordon and Frazier, Peter I.},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1703.04389},
issn = {10495258},
pages = {5268--5279},
title = {{Bayesian optimization with gradients}},
url = {https://papers.nips.cc/paper/7111-bayesian-optimization-with-gradients.pdf},
year = {2017}
}
@unpublished{kandasamy2018nas,
abstract = {Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function {\$}f{\$} which is only accessible via point evaluations. It is typically used in settings where {\$}f{\$} is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network $\backslash$emph{\{}architectures{\}}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1802.07191},
author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and P{\'{o}}czos, Barnab{\'{a}}s and Xing, Eric},
doi = {10.4161/cc.10.4.14843},
eprint = {1802.07191},
issn = {1551-4005},
pmid = {21311236},
title = {{Neural architecture search with Bayesian optimisation and optimal transport}},
url = {http://arxiv.org/pdf/1802.07191.pdf},
year = {2018}
}
@unpublished{shang2020dttts,
author = {Shang, Xuedong and Kaufmann, Emilie and Valko, Michal},
title = {{Simple (dynamic) bandit algorithms for hyper-parameter optimization}},
url = {https://xuedong.github.io/static/documents/dttts.pdf},
year = {2020}
}
@article{wang2017,
abstract = {Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.},
archivePrefix = {arXiv},
arxivId = {1706.01445},
author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
eprint = {1706.01445},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Batched large-scale Bayesian optimization in high-dimensional spaces.pdf:pdf},
journal = {arXiv preprint arXiv:1706.01445},
title = {{Batched large-scale Bayesian optimization in high-dimensional spaces}},
url = {http://arxiv.org/pdf/1706.01445.pdf},
year = {2017}
}
@article{radford2015dcgan,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint arXiv:1511.06434},
pmid = {23459267},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
url = {http://arxiv.org/pdf/1511.06434.pdf},
year = {2015}
}
@inproceedings{neyshabur2017generalization,
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1706.08947},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neyshabur et al. - 2017 - Exploring generalization in deep learning.pdf:pdf},
title = {{Exploring generalization in deep learning}},
url = {http://arxiv.org/pdf/1706.08947.pdf},
year = {2017}
}
@article{cappe2013klucb,
abstract = {We consider optimal sequential allocation in the context of the so-called stochastic multi-armed bandit model. We describe a generic index policy, in the sense of Gittins [J. R. Stat. Soc. Ser. B Stat. Methodol. 41 (1979) 148-177], based on upper confidence bounds of the arm payoffs computed using the Kullback-Leibler divergence. We consider two classes of distributions for which instances of this general idea are analyzed: the kl-UCB algorithm is designed for one-parameter exponential families and the empirical KL-UCB algorithm for bounded and finitely supported distributions. Our main contribution is a unified finite-time analysis of the regret of these algorithms that asymptotically matches the lower bounds of Lai and Robbins [Adv. in Appl. Math. 6 (1985) 4-22] and Burnetas and Katehakis [Adv. in Appl. Math. 17 (1996) 122-142], respectively. We also investigate the behavior of these algorithms when used with general bounded rewards, showing in particular that they provide significant improvements over the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.1136v4},
author = {Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Maillard, Odalric Ambrym and Munos, R{\'{e}}mi and Stoltz, Gilles},
doi = {10.1214/13-AOS1119},
eprint = {arXiv:1210.1136v4},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Capp{\'{e}} et al. - 2013 - Kullback–Leibler upper confidence bounds for optimal sequential allocation.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Key words,Kullback–Leibler divergence,Phrases. Multi-armed bandit problems,Sequential testing,Upper confidence bound},
number = {3},
pages = {1516--1541},
title = {{Kullback-Leibler upper confidence bounds for optimal sequential allocation}},
url = {https://arxiv.org/pdf/1210.1136.pdf},
volume = {41},
year = {2013}
}
@article{machado2018count,
abstract = {In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games.},
archivePrefix = {arXiv},
arxivId = {1807.11622},
author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael},
doi = {10.1609/aaai.v34i04.5955},
eprint = {1807.11622},
issn = {2159-5399},
journal = {arXiv preprint arXiv:1807.11622},
title = {{Count-based exploration with the successor representation}},
url = {https://arxiv.org/pdf/1807.11622.pdf},
year = {2018}
}
@article{salomon2011lower,
abstract = {This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistence, and exhibit a generalisation of the logarithmic bound. We also show the non existence of logarithmic bound in the general case of Hannan consistency. To get these results, we study variants of popular Upper Confidence Bounds (ucb) policies. As a by-product, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment.},
archivePrefix = {arXiv},
arxivId = {1112.3827},
author = {Salomon, Antoine and Audibert, Jean-Yves and Alaoui, Issam El},
eprint = {1112.3827},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salomon, Audibert, Alaoui - 2011 - Regret lower bounds and extended upper confidence bounds policies in stochastic multi-armed bandit pr.pdf:pdf},
journal = {arXiv preprint arXiv:1112.3827},
title = {{Regret lower bounds and extended upper confidence bounds policies in stochastic multi-armed bandit problem}},
url = {http://arxiv.org/pdf/1112.3827.pdf},
year = {2011}
}
@article{lepski1992,
abstract = {The paper deals with problems of statistical estimation in the presence of nuisance parameters (adaptive estimation) in the case where the value of the nuisance parameter affects the estimation accuracy. Such problems include: the estimation of functions (and functionals of them) that are adaptive with respect to smoothness classes, in various statistical schemes (Gaussian white noise, nonparametric regression, density estimation), prediction in polynomial regression for an unknown polynomial order, etc.},
author = {Lepski, Oleg V.},
doi = {10.1137/1136085},
issn = {0040-585X},
journal = {Theory of Probability {\&} Its Applications},
number = {4},
pages = {682--697},
title = {{Asymptotically minimax adaptive estimation. I: Upper bounds. optimally adaptive estimates}},
volume = {36},
year = {1992}
}
@phdthesis{wang2016thesis,
author = {Wang, Ziyu},
file = {:home/xuedong/Documents/xuedong/phd/work/readings/Thesis/Ziyu Wang's PhD Thesis.pdf:pdf},
school = {University of Oxford},
title = {{Practical and Theoretical Advances in Bayesian Optimization}},
url = {https://ora.ox.ac.uk/objects/uuid:9612d870-015e-4236-8c8d-0419670172fb/download{\_}file?safe{\_}filename=report.pdf{\&}file{\_}format=application{\%}2Fpdf{\&}type{\_}of{\_}work=Thesis},
year = {2016}
}
@inproceedings{achab2018profit,
abstract = {Originally motivated by default risk management applications, this paper investigates a novel problem, referred to as the profitable bandit problem here. At each step, an agent chooses a subset of the K possible actions. For each action chosen, she then receives the sum of a random number of rewards. Her objective is to maximize her cumulated earnings. We adapt and study three well-known strategies in this purpose, that were proved to be most efficient in other settings: kl-UCB, Bayes-UCB and Thompson Sampling. For each of them, we prove a finite time regret bound which, together with a lower bound we obtain as well, establishes asymptotic optimality. Our goal is also to compare these three strategies from a theoretical and empirical perspective both at the same time. We give simple, self-contained proofs that emphasize their similarities, as well as their differences. While both Bayesian strategies are automatically adapted to the geometry of information, the numerical experiments carried out show a slight advantage for Thompson Sampling in practice.},
archivePrefix = {arXiv},
arxivId = {1805.02908},
author = {Achab, Mastane and Cl{\'{e}}men{\c{c}}on, Stephan and Garivier, Aur{\'{e}}lien},
booktitle = {Proceedings of the 10th Asian Conference on Machine Learning (ACML)},
eprint = {1805.02908},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Achab, Cl{\'{e}}men{\c{c}}on, Garivier - 2018 - Profitable Bandits.pdf:pdf},
title = {{Profitable Bandits}},
url = {http://arxiv.org/pdf/1805.02908.pdf},
year = {2018}
}
@inproceedings{boda2019,
abstract = {While the objective in traditional multi-armed bandit problems is to find the arm with the highest mean, in many settings, finding an arm that best captures information about other arms is of interest. This objective, however, requires learning the underlying correlation structure and not just the means of the arms. Sensors placement for industrial surveillance and cellular network monitoring are a few applications, where the underlying correlation structure plays an important role. Motivated by such applications, we formulate the correlated bandit problem, where the objective is to find the arm with the lowest mean-squared error (MSE) in estimating all the arms. To this end, we derive first an MSE estimator, based on sample variances and covariances, and show that our estimator exponentially concentrates around the true MSE. Under a best-arm identification framework, we propose a successive rejects type algorithm and provide bounds on the probability of error in identifying the best arm. Using minmax theory, we also derive fundamental performance limits for the correlated bandit problem.},
archivePrefix = {arXiv},
arxivId = {1902.02953},
author = {Boda, Vinay Praneeth and Prashanth, L. A.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1902.02953},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boda, Prashanth - 2019 - Correlated bandits or How to minimize mean-squared error online.pdf:pdf},
isbn = {9781510886988},
pages = {1103--1111},
title = {{Correlated bandits or: How to minimize mean-squared error online}},
url = {https://arxiv.org/pdf/1902.02953.pdf},
year = {2019}
}
@inproceedings{pong2019rf,
abstract = {Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.},
archivePrefix = {arXiv},
arxivId = {1903.03698},
author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
eprint = {1903.03698},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pong et al. - 2019 - Skew-Fit State-covering self-supervised reinforcement learning(2).pdf:pdf},
title = {{Skew-Fit: State-covering self-supervised reinforcement learning}},
url = {https://arxiv.org/pdf/1903.03698.pdf},
year = {2019}
}
@article{metz2017thousand,
abstract = {We present TaskSet, a dataset of tasks for use in training and evaluating optimizers. TaskSet is unique in its size and diversity, containing over a thousand tasks ranging from image classification with fully connected or convolutional neural networks , to variational autoencoders, to non-volume preserving flows on a variety of datasets. As an example application of such a dataset we explore meta-learning an ordered list of hyperparameters to try sequentially. By learning this hyperparameter list from data generated using TaskSet we achieve large speedups in sample efficiency over random search. Next we use the diversity of the TaskSet and our method for learning hyperparameter lists to empirically explore the generalization of these lists to new optimization tasks in a variety of settings including ImageNet classification with Resnet50 and LM1B language modeling with transformers. As part of this work we have opensourced code for all tasks, as well as 29 million training curves for these problems and the corresponding hyperparam-eters. 1},
archivePrefix = {arXiv},
arxivId = {2002.11887},
author = {Metz, Luke and Maheswaranathan, Niru and Sun, Ruoxi and {Daniel Freeman}, C and Poole, Ben and Sohl-Dickstein, Jascha},
eprint = {2002.11887},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Metz et al. - 2020 - Using a thousand optimization tasks to learn hyperparameter search strategies.pdf:pdf},
journal = {arXiv preprint arXiv:2002.11887},
title = {{Using a thousand optimization tasks to learn hyperparameter search strategies}},
year = {2020}
}
@inproceedings{lu2019heavy,
author = {Lu, Shiyin and Wang, Guanghui and Hu, Yao and Zhang, Lijun},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2019 - Optimal algorithms for Lipschitz bandits with heavy-tailed rewards.pdf:pdf},
title = {{Optimal algorithms for Lipschitz bandits with heavy-tailed rewards}},
url = {http://proceedings.mlr.press/v97/lu19c/lu19c.pdf},
year = {2019}
}
@article{auer2002exp3,
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auer et al. - 2002 - The nonstochastic multiarmed bandit problem.pdf:pdf},
journal = {SIAM Journal of Computing},
pages = {48--77},
title = {{The nonstochastic multiarmed bandit problem}},
url = {http://rob.schapire.net/papers/AuerCeFrSc01.pdf},
volume = {32},
year = {2002}
}
@inproceedings{joseph2017infinite,
abstract = {We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in Joseph et al. [2016], we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.},
archivePrefix = {arXiv},
arxivId = {1610.09559},
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
booktitle = {4th Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML)},
eprint = {1610.09559},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph et al. - 2017 - Fair algorithms for infinite and contextual bandits.pdf:pdf},
title = {{Fair algorithms for infinite and contextual bandits}},
url = {http://arxiv.org/pdf/1610.09559.pdf},
year = {2017}
}
@book{orabona2019,
abstract = {In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the proofs have been carefully chosen to be as simple and as short as possible.},
archivePrefix = {arXiv},
arxivId = {1912.13213},
author = {Orabona, Francesco},
eprint = {1912.13213},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Orabona - 2019 - A Modern Introduction to Online Learning.pdf:pdf},
title = {{A Modern Introduction to Online Learning}},
url = {http://arxiv.org/pdf/1912.13213.pdf},
year = {2019}
}
@inproceedings{feldman2015,
abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process. When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses. We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
archivePrefix = {arXiv},
arxivId = {1412.3756},
author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
booktitle = {Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
doi = {10.1145/2783258.2783311},
eprint = {1412.3756},
isbn = {9781450336642},
keywords = {Disparate impact,Fairness,Machine learning},
pages = {259--268},
title = {{Certifying and removing disparate impact}},
url = {https://sorelle.friedler.net/papers/kdd{\_}disparate{\_}impact.pdf},
year = {2015}
}
@article{preux2014bandits,
abstract = {We consider function optimization as a sequential decision making problem under the budget constraint. Such constraint limits the number of objective function evaluations allowed during the optimization. We consider an algorithm inspired by a continuous version of a multi-armed bandit problem which attacks this optimization problem by solving the tradeoff between exploration (initial quasi-uniform search of the domain) and exploitation (local optimization around the potentially global maxima). We introduce the so-called Simultaneous Optimistic Optimization (SOO), a deterministic algorithm that works by domain partitioning. The benefit of such an approach are the guarantees on the returned solution and the numerical eficiency of the algorithm. We present this machine learning rooted approach to optimization, and provide the empirical assessment of SOO on the CEC'2014 competition on single objective real-parameter numerical optimization testsuite.},
author = {Preux, Philippe and Munos, R{\'{e}}mi and Valko, Michal},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Preux, Munos, Valko - 2014 - Bandits attack function optimization.pdf:pdf},
journal = {IEEE Congress on Evolutionary Computation},
title = {{Bandits attack function optimization}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=6900558},
year = {2014}
}
@inproceedings{agarwal2017privacy,
abstract = {Abstract: We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal {\$}{\{}\backslashtextbackslash{\}}tilde {\{}{\{}O{\}}({\{}\backslashtextbackslash{\}}sqrt{\}} {\{}{\{}T{\}}){\}} {\$} regret bounds. In the full-information setting, our results demonstrate that {\$}({\{}\backslashtextbackslash{\}}epsilon,{\{}\backslashtextbackslash{\}}delta) {\$}-differential privacy may be ensured for free-in particular, the regret bounds scale as {\$} O ({\{}\backslashtextbackslash{\}}sqrt {\{}{\{}T{\}})+{\{}\backslashtextbackslash{\}}tilde{\}} {\{}{\{}O{\}}{\{}\backslashtextbackslash{\}}big{\}} ({\{}\backslashtextbackslash{\}}frac {\{}1{\}}{\{}{\{}\backslashtextbackslash{\}}epsilon{\}}{\{}\backslashtextbackslash{\}}log{\{}\backslashtextbackslash{\}}frac {\{}1{\}}{\{}{\{}\backslashtextbackslash{\}}delta{\}}{\{}\backslashtextbackslash{\}}big) {\$}. For bandit linear ...},
author = {Agarwal, Naman and Singh, Karan},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
pages = {32--40},
title = {{The price of differential privacy For online learning}},
url = {https://arxiv.org/pdf/1701.07953.pdf},
year = {2017}
}
@inproceedings{baudry2020,
author = {Baudry, Dorian and Kaufmann, Emilie and Maillard, Odalric-Ambrym},
booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baudry, Kaufmann - 2020 - Sub-sampling for efficient non-parametric bandit exploration.pdf:pdf},
title = {{Sub-sampling for efficient non-parametric bandit exploration}},
year = {2020}
}
@article{Liu2019,
abstract = {We propose BMLE, a new family of bandit algorithms, that are formulated in a general way based on the Biased Maximum Likelihood Estimation method originally appearing in the adaptive control literature. We design the cost-bias term to tackle the exploration and exploitation tradeoff for stochastic bandit problems. We provide an explicit closed form expression for the index of an arm for Bernoulli bandits, which is trivial to compute. We also provide a general recipe for extending the BMLE algorithm to other families of reward distributions. We prove that for Bernoulli bandits, the BMLE algorithm achieves a logarithmic finite-time regret bound and hence attains order-optimality. Through extensive simulations, we demonstrate that the proposed algorithms achieve regret performance comparable to the best of several state-of-the-art baseline methods, while having a significant computational advantage in comparison to other best performing methods. The generality of the proposed approach makes it possible to address more complex models, including general adaptive control of Markovian systems.},
archivePrefix = {arXiv},
arxivId = {1907.01287},
author = {Liu, Xi and Hsieh, Ping-Chun and Bhattacharya, Anirban and Kumar, P. R.},
eprint = {1907.01287},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Bandit learning through biased maximum likelihood estimation.pdf:pdf},
journal = {arXiv preprint arXiv:1907.01287},
title = {{Bandit learning through biased maximum likelihood estimation}},
url = {http://arxiv.org/pdf/1907.01287.pdf},
year = {2019}
}
@inproceedings{tossou2016,
abstract = {We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist {\$}(\backslashepsilon, \backslashdelta){\$} differentially private variants of Upper Confidence Bound algorithms which have optimal regret, {\$}O(\backslashepsilon{\^{}}{\{}-1{\}} + \backslashlog T){\$}. This is a significant improvement over previous results, which only achieve poly-log regret {\$}O(\backslashepsilon{\^{}}{\{}-2{\}} \backslashlog{\^{}}{\{}2{\}} T){\$}, because of our use of a novel interval-based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.},
archivePrefix = {arXiv},
arxivId = {1511.08681},
author = {Tossou, Aristide and Dimitrakakis, Christos},
booktitle = {Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)},
eprint = {1511.08681},
isbn = {9781577357605},
pages = {2087--2093},
title = {{Algorithms for differentially private multi-armed bandits}},
url = {http://arxiv.org/pdf/1511.08681.pdf},
year = {2016}
}
@inproceedings{efroni2019tight,
abstract = {State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms typically act by iteratively solving empirical models, i.e., by performing full-planning on Markov Decision Processes (MDPs) built by the gathered experience. In this paper, we focus on model-based RL in the finite-state finite-horizon undiscounted MDP setting and establish that exploring with greedy policies - act by 1-step planning - can achieve tight minimax performance in terms of regret, {\~{O}}(vHSAT). Thus, full-planning in model-based RL can be avoided altogether without any performance degradation, and, by doing so, the computational complexity decreases by a factor of S. The results are based on a novel analysis of real-time dynamic programming, then extended to model-based RL. Specifically, we generalize existing algorithms that perform full-planning to act by 1-step planning. For these generalizations, we prove regret bounds with the same rate as their full-planning counterparts.},
author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
issn = {10495258},
pages = {12224--12234},
title = {{Tight regret bounds for model-based reinforcement learning with greedy policies}},
url = {https://arxiv.org/pdf/1905.11527.pdf},
year = {2019}
}
@inproceedings{mukherjee2017,
abstract = {In this paper we propose the Augmented-UCB (AugUCB) algorithm for a fixed-budget version of the thresholding bandit problem (TBP), where the objective is to identify a set of arms whose quality is above a threshold. A key feature of AugUCB is that it uses both mean and variance estimates to eliminate arms that have been sufficiently explored; to the best of our knowledge this is the first algorithm to employ such an approach for the considered TBP. Theoretically, we obtain an upper bound on the loss (probability of mis-classification) incurred by AugUCB. Although UCBEV in literature provides a better guarantee, it is important to emphasize that UCBEV has access to problem complexity (whose computation requires arms' mean and variances), and hence is not realistic in practice; this is in contrast to AugUCB whose implementation does not require any such complexity inputs. We conduct extensive simulation experiments to validate the performance of AugUCB. Through our simulation work, we establish that AugUCB, owing to its utilization of variance estimates, performs significantly better than the state-of-the-art APT, CSAR and other non variance-based algorithms.},
archivePrefix = {arXiv},
arxivId = {1704.02281},
author = {Mukherjee, Subhojyoti and Purushothama, Naveen Kolar and Sudarsanam, Nandan and Ravindran, Balaraman},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2017/350},
eprint = {1704.02281},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukherjee et al. - 2017 - Thresholding bandits with augmented UCB.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
keywords = {Machine Learning: Learning Theory,Machine Learning: Online Learning,Machine Learning: Reinforcement Learning,Uncertainty in AI: Sequential Decision Making},
pages = {2515--2521},
title = {{Thresholding bandits with augmented UCB}},
url = {https://arxiv.org/pdf/1704.02281.pdf},
year = {2017}
}
@inproceedings{liang2019evolution,
abstract = {Deep neural networks (DNNs) have produced state-of-the-art results in many benchmarks and problem domains. However, the success of DNNs depends on the proper configuration of its architecture and hyperparameters. Such a configuration is difficult and as a result, DNNs are often not used to their full potential. In addition, DNNs in commercial applications often need to satisfy real-world design constraints such as size or number of parameters. To make configuration easier, automatic machine learning (AutoML) systems for deep learning have been developed, focusing mostly on optimization of hyperparameters. This paper takes AutoML a step further. It introduces an evolutionary AutoML framework called LEAF that not only optimizes hyperparameters but also network architectures and the size of the network. LEAF makes use of both state-of-the-art evolutionary algorithms (EAs) and distributed computing frameworks. Experimental results on medical image classification and natural language analysis show that the framework can be used to achieve state-of-the-art performance. In particular, LEAF demonstrates that architecture optimization provides a significant boost over hyperparameter optimization, and that networks can be minimized at the same time with little drop in performance. LEAF therefore forms a foundation for democratizing and improving AI, as well as making AI practical in future applications.},
archivePrefix = {arXiv},
arxivId = {1902.06827},
author = {Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
booktitle = {Proceedings of the 2019 Genetic and Evolutionary Computation Conference (GECCO)},
doi = {10.1145/3321707.3321721},
eprint = {1902.06827},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2019 - Evolutionary neural automl for deep learning.pdf:pdf},
isbn = {9781450361118},
keywords = {Artificial Intelligence,AutoML,Neural Networks/Deep Learning},
pages = {401--409},
title = {{Evolutionary neural AutoML for deep learning}},
url = {https://dl.acm.org/doi/pdf/10.1145/3321707.3321721},
year = {2019}
}
@inproceedings{degroote2018online,
author = {Degroote, Hans and de Causmaecker, Patrick and Bischl, Bernd and Kotthoff, Lars},
booktitle = {Proceedings of the 11th Annual Symposium on Combinatorial Search (SoCS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degroote et al. - 2018 - A regression-based methodology for online algorithm selection.pdf:pdf},
title = {{A regression-based methodology for online algorithm selection}},
year = {2018}
}
@inproceedings{mahajan2019maven,
abstract = {Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].},
archivePrefix = {arXiv},
arxivId = {1910.07483},
author = {Mahajan, Anuj and Rashid, Tabish and Samvelyan, Mikayel and Whiteson, Shimon},
booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
eprint = {1910.07483},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahajan et al. - 2019 - MAVEN Multi-agent variational exploration.pdf:pdf},
title = {{MAVEN: Multi-agent variational exploration}},
url = {https://papers.nips.cc/paper/2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf},
year = {2019}
}
@inproceedings{cauwet2020parallel,
abstract = {Space-filling designs such as scrambled-Hammersley, Latin Hypercube Sampling and Jittered Sampling have been proposed for fully parallel hyperparameter search, and were shown to be more effective than random or grid search. In this paper, we show that these designs only improve over random search by a constant factor. In contrast, we introduce a new approach based on reshaping the search distribution, which leads to substantial gains over random search, both theoretically and empirically. We propose two flavors of reshaping. First, when the distribution of the optimum is some known {\$}P{\_}0{\$}, we propose Recentering, which uses as search distribution a modified version of {\$}P{\_}0{\$} tightened closer to the center of the domain, in a dimension-dependent and budget-dependent manner. Second, we show that in a wide range of experiments with {\$}P{\_}0{\$} unknown, using a proposed Cauchy transformation, which simultaneously has a heavier tail (for unbounded hyperparameters) and is closer to the boundaries (for bounded hyperparameters), leads to improved performances. Besides artificial experiments and simple real world tests on clustering or Salmon mappings, we check our proposed methods on expensive artificial intelligence tasks such as attend/infer/repeat, video next frame segmentation forecasting and progressive generative adversarial networks.},
archivePrefix = {arXiv},
arxivId = {1910.08406},
author = {Cauwet, Marie-Liesse and Couprie, Camille and Dehos, Julien and Luc, Pauline and Rapin, Jeremy and Riviere, Morgane and Teytaud, Fabien and Teytaud, Olivier and Usunier, Nicolas},
booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
eprint = {1910.08406},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cauwet et al. - 2020 - Fully parallel hyperparameter search Reshaped space-filling.pdf:pdf},
title = {{Fully parallel hyperparameter search: Reshaped space-filling}},
url = {http://arxiv.org/pdf/1910.08406.pdf},
year = {2020}
}
@article{bietti2018greedy,
abstract = {Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to compare and empirically optimize contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate and improve several internal components of contextual bandit algorithm design. Overall, this is a thorough study and review of contextual bandit methodology.},
archivePrefix = {arXiv},
arxivId = {1802.04064},
author = {Bietti, Alberto and Agarwal, Alekh and Langford, John},
eprint = {1802.04064},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bietti, Agarwal, Langford - 2018 - A contextual bandit bake-off.pdf:pdf},
journal = {arXiv preprint arXiv:1802.04064},
title = {{A contextual bandit bake-off}},
url = {http://arxiv.org/pdf/1802.04064.pdf},
year = {2018}
}
@article{Lu2019,
abstract = {The dying ReLU refers to the problem when ReLU neurons become inactive and only output 0 for any input. There are many empirical and heuristic explanations of why ReLU neurons die. However, little is known about its theoretical analysis. In this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric probability distributions, which suffers from the dying ReLU. We thus propose a new initialization procedure, namely, a randomized asymmetric initialization. We prove that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are provided to demonstrate the effectiveness of the new initialization procedure.},
archivePrefix = {arXiv},
arxivId = {1903.06733},
author = {Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
eprint = {1903.06733},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2019 - Dying ReLU and initialization Theory and numerical examples.pdf:pdf},
journal = {arXiv preprint arXiv:1903.06733},
keywords = {dying relu,exploding gradient,ized asymmetric initialization,length map,neural network,random-,vanishing},
title = {{Dying ReLU and initialization: Theory and numerical examples}},
url = {http://arxiv.org/pdf/1903.06733.pdf},
year = {2019}
}
@article{lee2020cost,
abstract = {Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.},
archivePrefix = {arXiv},
arxivId = {2003.10870},
author = {Lee, Eric Hans and Perrone, Valerio and Archambeau, Cedric and Seeger, Matthias},
eprint = {2003.10870},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2020 - Cost-aware Bayesian optimization.pdf:pdf},
journal = {arXiv preprint arXiv:2003.10870},
title = {{Cost-aware Bayesian optimization}},
url = {http://arxiv.org/pdf/2003.10870.pdf},
year = {2020}
}
@article{Author2020c,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - Finite Continuum-Armed Bandits.pdf:pdf},
number = {NeurIPS},
title = {{Finite continuum-armed bandits}},
year = {2020}
}
@inproceedings{luc2017semantic,
abstract = {The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.},
archivePrefix = {arXiv},
arxivId = {1703.07684},
author = {Luc, Pauline and Neverova, Natalia and Couprie, Camille and Verbeek, Jakob and Lecun, Yann},
booktitle = {Proceedings of the 16th IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.77},
eprint = {1703.07684},
isbn = {9781538610329},
issn = {15505499},
pages = {648--657},
title = {{Predicting deeper into the future of semantic segmentation}},
url = {https://arxiv.org/pdf/1703.07684.pdf},
year = {2017}
}
@article{bonneel2014sw,
abstract = {This article details two approaches to compute barycenters of measures using 1-D Wasserstein distances along radial projections of the input measures. The first method makes use of the Radon transform of the measures, and the second is the solution of a convex optimization problem over the space of measures. We show several properties of these barycenters and explain their relationship. We show numerical approximation schemes based on a discrete Radon transform and on the resolution of a non-convex optimization problem. We explore the respective merits and drawbacks of each approach on applications to two image processing problems: color transfer and texture mixing.},
author = {Bonneel, Nicolas and Rabin, Julien and Peyr{\'{e}}, Gabriel and Pfister, Hanspeter},
doi = {10.1007/s10851-014-0506-3},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonneel et al. - 2014 - Sliced and Radon Wasserstein barycenters of measures.pdf:pdf},
issn = {15737683},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Barycenter of measures,Optimal transport,Radon transform,Wasserstein distance},
number = {1},
pages = {22--45},
title = {{Sliced and Radon Wasserstein barycenters of measures}},
url = {https://perso.liris.cnrs.fr/nicolas.bonneel/WassersteinSliced-JMIV.pdf},
volume = {51},
year = {2014}
}
@inproceedings{wang2019sat,
abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
archivePrefix = {arXiv},
arxivId = {1905.12149},
author = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1905.12149},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoning using a differentiable satisfiability solver.pdf:pdf},
title = {{SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver}},
url = {http://arxiv.org/pdf/1905.12149.pdf},
year = {2019}
}
@inproceedings{david2014infinite,
author = {David, Yahel and Shimkin, Nahum},
booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2014 (ECML-PKDD)},
doi = {10.1007/978-3-662-44848-9_20},
isbn = {9783662448472},
issn = {16113349},
pages = {307--322},
title = {{Infinitely many-armed bandits with unknown value distribution}},
url = {https://link.springer.com/chapter/10.1007/978-3-662-44848-9{\_}20},
year = {2014}
}
@article{salimans2017es,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
eprint = {1703.03864},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salimans et al. - 2017 - Evolution strategies as a scalable alternative to reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:1703.03864},
title = {{Evolution strategies as a scalable alternative to reinforcement learning}},
url = {http://arxiv.org/pdf/1703.03864.pdf},
year = {2017}
}
@inproceedings{gidel2017frank,
abstract = {We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems. Remarkably, the method only requires access to linear minimization oracles. Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture. We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints.},
archivePrefix = {arXiv},
arxivId = {1610.07797},
author = {Gidel, Gauthier and Jebara, Tony and Lacoste-Julien, Simon},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1610.07797},
title = {{Frank-Wolfe algorithms for saddle point problems}},
url = {https://arxiv.org/pdf/1610.07797.pdf},
year = {2017}
}
@article{Author2020b,
author = {Author, Anonymous and Address, Affiliation},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2020 - CAS Cost-Aware Sampling for Best Arm Identification in Linear Bandits.pdf:pdf},
number = {NeurIPS},
title = {{CAS : Cost-aware sampling for best arm identification in linear bandits}},
year = {2020}
}
@inproceedings{auer1995,
abstract = {In the multi-armed bandit problem, a gambler must decide which arm$\backslash$nof K non-identical slot machines to play in a sequence of trials so as$\backslash$nto maximize his reward. This classical problem has received much$\backslash$nattention because of the simple model it provides of the trade-off$\backslash$nbetween exploration (trying out each arm to find the best one) and$\backslash$nexploitation (playing the arm believed to give the best payoff). Past$\backslash$nsolutions for the bandit problem have almost always relied on$\backslash$nassumptions about the statistics of the slot machines. In this work, we$\backslash$nmake no statistical assumptions whatsoever about the nature of the$\backslash$nprocess generating the payoffs of the slot machines. We give a solution$\backslash$nto the bandit problem in which an adversary, rather than a well-behaved$\backslash$nstochastic process, has complete control over the payoffs. In a sequence$\backslash$nof T plays, we prove that the expected per-round payoff of our algorithm$\backslash$napproaches that of the best arm at the rate O(T-1/3), and we$\backslash$ngive an improved rate of convergence when the best arm has fairly low$\backslash$npayoff. We also consider a setting in which the player has a team of$\backslash$n{\&}ldquo;experts{\&}rdquo; advising him on which arm to play; here, we give a$\backslash$nstrategy that will guarantee expected payoff close to that of the best$\backslash$nexpert. Finally, we apply our result to the problem of learning to play$\backslash$nan unknown repeated matrix game against an all-powerful adversary},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
booktitle = {Proceedings of the 36th IEEE Annual Symposium on Foundations of Computer Science (FOCS)},
doi = {10.1109/SFCS.1995.492488},
isbn = {0-8186-7183-1},
issn = {0272-5428},
pages = {322--331},
title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
url = {http://www.dklevine.com/archive/refs4462.pdf},
year = {1995}
}
@inproceedings{audibert2009moss,
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
author = {Audibert, Jean Yves and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the 22nd Annual Conference on Learning Theory (CoLT)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Audibert, Bubeck - 2009 - Minimax policies for adversarial and stochastic bandits(2).pdf:pdf},
title = {{Minimax policies for adversarial and stochastic bandits}},
url = {https://www.di.ens.fr/willow/pdfscurrent/COLT09a.pdf},
year = {2009}
}
@inproceedings{zaki2019maxoverlap,
abstract = {We give a new algorithm for best arm identification in linearly parameterised bandits in the fixed confidence setting. The algorithm generalises the well-known LUCB algorithm of Kalyanakrishnan et al. (2012) by playing an arm which minimises a suitable notion of geometric overlap of the statistical confidence set for the unknown parameter, and is fully adaptive and computationally efficient as compared to several state-of-the methods. We theoretically analyse the sample complexity of the algorithm for problems with two and three arms, showing optimality in many cases. Numerical results indicate favourable performance over other algorithms with which we compare.},
archivePrefix = {arXiv},
arxivId = {1911.01695v2},
author = {Zaki, Mohammadi and Mohan, Avinash and Gopalan, Aditya},
booktitle = {Workshop on Machine Learning at Neural Information Processing Systems (NeurIPS-CausalML)},
eprint = {1911.01695v2},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Mohan, Gopalan - 2019 - Towards optimal and efficient best arm identification in linear bandits.pdf:pdf},
title = {{Towards optimal and efficient best arm identification in linear bandits}},
url = {https://arxiv.org/pdf/1911.01695.pdf},
year = {2019}
}
@article{floyd1995,
author = {Floyd, Sally and Warmuth, Manfred},
doi = {10.1007/BF00993593},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Floyd, Warmuth - 1995 - Sample compression, learnability, and the Vapnik-Chervonenkis dimension.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {pac-learning,sample compression,vapnik-chervonenkis dimension},
number = {3},
pages = {269--304},
title = {{Sample compression, learnability, and the Vapnik-Chervonenkis dimension}},
volume = {21},
year = {1995}
}
@article{berry1997infinite,
abstract = {We consider a bandit problem consisting of a sequence of {\$}n{\$} choices from an infinite number of Bernoulli arms, with {\$}n /rightarrow /infty{\$}. The objective is to minimize the long-run failure rate. The Bernoulli parameters are independent observations from a distribution {\$}F{\$}. We first assume {\$}F{\$} to be the uniform distribution on (0, 1) and consider various extensions. In the uniform case we show that the best lower bound for the expected failure proportion is between {\$}/sqrt2//sqrtn{\$} and {\$}2//sqrtn{\$} and we exhibit classes of strategies that achieve the latter.},
author = {Berry, Donald A. and Chen, Robert W. and Zame, Alan and Heath, David C. and Shepp, Larry A.},
doi = {10.1214/aos/1069362389},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berry et al. - 1997 - Bandit problems with infinitely many arms.pdf:pdf},
isbn = {00905364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bandit problems,Dynamic allocation of bernoulli processes,Sequential experimentation,Staying with a winner,Switching with a loser},
number = {5},
pages = {2103--2116},
title = {{Bandit problems with infinitely many arms}},
url = {https://faculty.wharton.upenn.edu/wp-content/uploads/2012/04/Bandit-problems-with-infinitely-many-arms.pdf},
volume = {25},
year = {1997}
}
@inproceedings{burda2019curiosity,
abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
archivePrefix = {arXiv},
arxivId = {1808.04355},
author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1808.04355},
title = {{Large-scale study of curiosity-driven learning}},
url = {https://arxiv.org/pdf/1808.04355.pdf},
year = {2019}
}
@article{bastani2017,
abstract = {The contextual bandit literature has traditionally focused on algorithms that address the exploration-exploitation tradeoff. In particular, greedy algorithms that exploit current estimates without any exploration may be sub-optimal in general. However, exploration-free greedy algorithms are desirable in practical settings where exploration may be costly or unethical (e.g., clinical trials). Surprisingly, we find that a simple greedy algorithm can be rate optimal (achieves asymptotically optimal regret) if there is sufficient randomness in the observed contexts (covariates). We prove that this is always the case for a two-armed bandit under a general class of context distributions that satisfy a condition we term covariate diversity. Furthermore, even absent this condition, we show that a greedy algorithm can be rate optimal with positive probability. Thus, standard bandit algorithms may unnecessarily explore. Motivated by these results, we introduce Greedy-First, a new algorithm that uses only observed contexts and rewards to determine whether to follow a greedy algorithm or to explore. We prove that this algorithm is rate optimal without any additional assumptions on the context distribution or the number of arms. Extensive simulations demonstrate that Greedy-First successfully reduces exploration and outperforms existing (exploration-based) contextual bandit algorithms such as Thompson sampling or upper confidence bound (UCB).},
archivePrefix = {arXiv},
arxivId = {1704.09011},
author = {Bastani, Hamsa and Bayati, Mohsen and Khosravi, Khashayar},
eprint = {1704.09011},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Bayati, Khosravi - 2017 - Mostly exploration-free algorithms for contextual bandits(2).pdf:pdf},
journal = {arXiv preprint arXiv:1704.09011},
keywords = {contextual bandit,exploration-exploitation,greedy algorithm,sequential decision-making},
title = {{Mostly exploration-free algorithms for contextual bandits}},
url = {https://arxiv.org/pdf/1704.09011.pdf http://arxiv.org/pdf/1704.09011.pdf},
year = {2017}
}
@inproceedings{Haeffele2017,
abstract = {The past few years have seen a dramatic increase in the performance of recognition systems thanks to the introduc-tion of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key issue is that the neural network training problem is nonconvex, hence optimization algorithms may not return a global minima. This paper provides sufficient conditions to guarantee that local minima are globally optimal and that a local descent strategy can reach a global minima from any initialization. Our conditions require both the network output and the regularization to be positively homogeneous functions of the network parameters, with the regulariza-tion being designed to control the network size. Our re-sults apply to networks with one hidden layer, where size is measured by the number of neurons in the hidden layer, and multiple deep subnetworks connected in parallel, where size is measured by the number of subnetworks.},
author = {Haeffele, Benjamin D. and Vidal, Ren{\'{e}}},
booktitle = {Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.467},
isbn = {9781538604571},
pages = {7331--7339},
title = {{Global optimality in neural network training}},
url = {http://www.vision.jhu.edu/assets/HaeffeleCVPR17.pdf},
year = {2017}
}
@book{shiryaev1996,
author = {Shiryaev, Albert N.},
publisher = {Springer-Verlag New York},
title = {{Probability}},
url = {https://www.springer.com/gb/book/9781475725391},
year = {1996}
}
@article{nguyen2010fdivergence,
abstract = {We develop and analyze {\$}M{\$}-estimation methods for divergence functionals and the likelihood ratios of two probability distributions. Our method is based on a non-asymptotic variational characterization of {\$}f{\$}-divergences, which allows the problem of estimating divergences to be tackled via convex empirical risk optimization. The resulting estimators are simple to implement, requiring only the solution of standard convex programs. We present an analysis of consistency and convergence for these estimators. Given conditions only on the ratios of densities, we show that our estimators can achieve optimal minimax rates for the likelihood ratio and the divergence functionals in certain regimes. We derive an efficient optimization algorithm for computing our estimates, and illustrate their convergence behavior and practical viability by simulations.},
archivePrefix = {arXiv},
arxivId = {0809.0853},
author = {Nguyen, Xuanlong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
eprint = {0809.0853},
isbn = {160560352X},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Convex optimization,Kullback-Leibler (KL) divergence,M-estimation,density ratio estimation,divergence estimation,f-divergence,reproducing kernel Hilbert space (RKHS),surrogate loss functions},
number = {11},
pages = {5847--5861},
title = {{Estimating divergence functionals and the likelihood ratio by convex risk minimization}},
url = {http://dept.stat.lsa.umich.edu/{~}xuanlong/Papers/Nguyen-Wainwright-Jordan-10.pdf},
volume = {56},
year = {2010}
}
@inproceedings{peng2019disentangled,
abstract = {Unsupervised model transfer has the potential to greatly improve the generalizability of deep models to novel domains. Yet the current literature assumes that the separation of target data into distinct domains is known as a priori. In this paper, we propose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this problem, we devise a novel Deep Adversarial Disentangled Autoencoder (DADA) capable of disentangling domain-specific features from class identity. We demonstrate experimentally that when the target domain labels are unknown, DADA leads to state-of-the-art performance on several image classification datasets.},
archivePrefix = {arXiv},
arxivId = {1904.12347},
author = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1904.12347},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2019 - Domain agnostic learning with disentangled representations.pdf:pdf},
title = {{Domain agnostic learning with disentangled representations}},
url = {http://arxiv.org/pdf/1904.12347.pdf},
year = {2019}
}
@book{mankiw2014macro,
author = {Mankiw, Gregory N.},
edition = {7th},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mankiw - 2014 - Principles of Economics - Macroeconomics.pdf:pdf},
isbn = {9781429218870},
publisher = {Cengage Learning},
title = {{Principles of Economics - Macroeconomics}},
year = {2014}
}
@article{busa-fekete2018,
abstract = {In machine learning, the notion of multi-armed bandits refers to a class of online learning problems, in which an agent is supposed to simultaneously explore and exploit a given set of choice alternatives in the course of a sequential decision process. In the standard setting, the agent learns from stochastic feedback in the form of real-valued rewards. In many applications, however, numerical reward signals are not readily available -- instead, only weaker information is provided, in particular relative preferences in the form of qualitative comparisons between pairs of alternatives. This observation has motivated the study of variants of the multi-armed bandit problem, in which more general representations are used both for the type of feedback to learn from and the target of prediction. The aim of this paper is to provide a survey of the state of the art in this field, referred to as preference-based multi-armed bandits or dueling bandits. To this end, we provide an overview of problems that have been considered in the literature as well as methods for tackling them. Our taxonomy is mainly based on the assumptions made by these methods about the data-generating process and, related to this, the properties of the preference-based feedback.},
archivePrefix = {arXiv},
arxivId = {1807.11398},
author = {Busa-Fekete, Robert and H{\"{u}}llermeier, Eyke and Mesaoudi-Paul, Adil El},
eprint = {1807.11398},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Busa-Fekete, H{\"{u}}llermeier, Mesaoudi-Paul - 2018 - Preference-based online learning with dueling bandits A survey.pdf:pdf},
journal = {arXiv preprint arXiv:1807.11398},
keywords = {cumulative regret,exploitation,exploration,multi-armed bandits,online learning,pac learning,preference learning,ranking,sample complexity,selection,top- k},
title = {{Preference-based online learning with dueling bandits: A survey}},
url = {http://arxiv.org/pdf/1807.11398.pdf},
year = {2018}
}
@inproceedings{abbasi-yadkori2011linear,
abstract = {We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer's UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.},
author = {Abbasi-Yadkori, Yasin and P{\'{a}}l, D{\'{a}}vid and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbasi-Yadkori, P{\'{a}}l, Szepesv{\'{a}}ri - 2011 - Improved algorithms for linear stochastic bandits.pdf:pdf},
isbn = {9781618395993},
title = {{Improved algorithms for linear stochastic bandits}},
url = {https://sites.ualberta.ca/{~}szepesva/papers/linear-bandits-NIPS2011.pdf},
year = {2011}
}
@article{halvari2020robustness,
abstract = {Automated machine learning (AutoML) systems aim at finding the best machine learning (ML) pipeline that automatically matches the task and data at hand. We investigate the robustness of machine learning pipelines generated with three AutoML systems, TPOT, H2O, and AutoKeras. In particular, we study the influence of dirty data on the accuracy, and consider how using dirty training data may help to create more robust solutions. Furthermore, we also analyze how the structure of the generated pipelines differs in different cases.},
archivePrefix = {arXiv},
arxivId = {2005.02649},
author = {Halvari, Tuomas and Nurminen, Jukka K. and Mikkonen, Tommi},
eprint = {2005.02649},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Halvari, Nurminen, Mikkonen - 2020 - Testing the robustness of AutoML systems.pdf:pdf},
journal = {arXiv preprint arXiv:2005.02649},
keywords = {automl,machine learning,robustness},
title = {{Testing the robustness of AutoML systems}},
url = {http://arxiv.org/pdf/2005.02649.pdf},
year = {2020}
}
@inproceedings{burda2019rnd,
abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
archivePrefix = {arXiv},
arxivId = {1810.12894},
author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
eprint = {1810.12894},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burda et al. - 2019 - Exploration by random network distillation(2).pdf:pdf},
title = {{Exploration by random network distillation}},
url = {https://arxiv.org/pdf/1810.12894.pdf},
year = {2019}
}
@inproceedings{gerchinovitz2016,
abstract = {We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total lossof the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.},
archivePrefix = {arXiv},
arxivId = {1605.07416},
author = {Gerchinovitz, S{\'{e}}bastien and Lattimore, Tor},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
eprint = {1605.07416},
issn = {10495258},
pages = {1198--1206},
title = {{Refined lower bounds for adversarial bandits}},
url = {http://arxiv.org/pdf/1605.07416.pdf},
year = {2016}
}
@inproceedings{pacchiano2019kernel,
abstract = {We present a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated reproducing kernel Hilbert space) rather than linear functions. We study a version of the exponential weights algorithm and bound its regret in this setting. Under conditions on the eigendecay of the kernel we provide a sharp characterization of the regret for this algorithm. When we have polynomial eigendecay {\$}\backslashmu{\_}j \backslashle \backslashmathcal{\{}O{\}}(j{\^{}}{\{}-\backslashbeta{\}}){\$}, we find that the regret is bounded by {\$}\backslashmathcal{\{}R{\}}{\_}n \backslashle \backslashmathcal{\{}O{\}}(n{\^{}}{\{}\backslashbeta/(2(\backslashbeta-1)){\}}){\$}; while under the assumption of exponential eigendecay {\$}\backslashmu{\_}j \backslashle \backslashmathcal{\{}O{\}}(e{\^{}}{\{}-\backslashbeta j {\}}){\$}, we get an even tighter bound on the regret {\$}\backslashmathcal{\{}R{\}}{\_}n \backslashle \backslashmathcal{\{}O{\}}(n{\^{}}{\{}1/2{\}}\backslashlog(n){\^{}}{\{}1/2{\}}){\$}. We also study the full information setting when the underlying losses are kernel functions and present an adapted exponential weights algorithm and a conditional gradient descent algorithm.},
archivePrefix = {arXiv},
arxivId = {1802.09732},
author = {Pacchiano, Aldo and Chatterji, Niladri S. and Bartlett, Peter L.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1802.09732},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pacchiano, Chatterji, Bartlett - 2019 - Online learning with kernel losses.pdf:pdf},
title = {{Online learning with kernel losses}},
url = {http://arxiv.org/pdf/1802.09732.pdf},
year = {2019}
}
@phdthesis{fruit2019thesis,
author = {Fruit, Ronan},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fruit - 2019 - Exploration-Exploitation Dilemma in Reinforcement Learning under Various Form of Prior nowledge.pdf:pdf},
school = {Universit{\'{e}} de Lille},
title = {{Exploration-Exploitation Dilemma in Reinforcement Learning under Various Form of Prior nowledge}},
year = {2019}
}
