%!TEX root = ../AppendixC.tex
\section{Technical Lemmas}\label{app:lgc.lemmas}

\subsection{Lagrangian lemma}\label{app:lgc.lemmas.lagrange}

\begin{lemma}\label{lemma:lgc.lagrange_alternative}
For $\btheta, \btheta' \in \R^d\,$, $\bomega$ in the interior of the probability simplex $\interior{\Sigma_K}$, $\by\in\R^d\,$, $x\in\R$, we have

\begin{align*}
    \inf_{\btheta':\ \by\transpose\btheta' \geq x} \frac{\normm{\btheta-\btheta'}^2_{\bLambda_{\bomega}}}{2} = 
    \begin{cases}
        \ddfrac{(x - \by\transpose\btheta')^2}{2 \normm{\by}_{\bLambda_{\bomega}^{-1}}^2} &\text{if } x \geq \by\transpose\btheta' \\
        0 &\text{otherwise}
    \end{cases}\,.
\end{align*}
\end{lemma}

\begin{proof}
We consider the Lagrangian of the problem, and we obtain
\begin{align*}
  \inf_{\btheta':\ \by\transpose\btheta' \geq x} \frac{\normm{\btheta-\btheta'}^2_{\bLambda_{\bomega}}}{2}
  &= \sup_{\lambda \geq 0}\inf_{\btheta' \in \R^d} \ddfrac{\normm{\btheta-\btheta'}^2_{\bLambda_{\bomega}}}{2}+ \lambda (x-\by\transpose\btheta')\\
  &=  \sup_{\lambda \geq 0} \lambda (x-\by\transpose\btheta) - \lambda^2 \frac{\normm{\by}^2_{\bLambda_{\bomega}^{-1}}}{2}\\
  &= \begin{cases}
    \ddfrac{(x - \by\transpose\btheta')^2}{2 \normm{\by}_{\bLambda_{\bomega}^{-1}}^2} &\text{if } x \geq \by\transpose\btheta' \\
    0 &\text{otherwise}
    \end{cases}\,,
\end{align*}
where the infimum in the first equality is reached at $\btheta' = \btheta + \lambda \bLambda_{\bomega}^{-1} \by$ and the supremum in the last equality is reached at
\[
    \lambda =
    \begin{cases}
        \ddfrac{(x- \by\transpose\btheta)}{\normm{\by}_{\bLambda_{\bomega}^{-1}}^2} &\text{if } x \geq \by\transpose\btheta \\
        0 &\text{otherwise}
    \end{cases}\,.
\]
\end{proof}

\subsection{Concentration results}\label{app:lgc.lemmas.concentration}

We restate here the Theorem~20.4 (in combination with the Equation~20.10) by \citet{lattimore2018bandits}.
\begin{theorem}
\label{th:confidence_beta}
For all $\lambda >0$ and $\delta \in(0,1)$,
\[
    \PPt{\exists n\in\NN,\, \frac{1}{2}\normm{\hat{\btheta}_n^\lambda-\btheta}^2_{\bLambda_{\bT_n}+\lambda I_d} \geq d_{n,\delta}} \leq \delta\,,
\]
where
\begin{align*}
    d_{n,\delta} &\eqdef  \left( \sqrt{\log\left( \frac{1}{\delta}\right)+\frac{d}{2}\log\left(1+\frac{n L^2}{\lambda d} \right)} +\sqrt{\frac{\lambda}{2}}M\right)^2\\
    &=\log\!\left( \frac{1}{\delta}\right)+\frac{d}{2}\log\left(1+\frac{n L^2}{\lambda d} \right) +  M\sqrt{\lambda}\sqrt{2\log\left( \frac{1}{\delta}\right)+d\log\!\left(1+\frac{n L^2}{\lambda d} \right)}+\frac{\lambda M^2}{2}\,.
%\left(\sqrt{\log\!\left(\frac{1}{\delta}\right) + \frac{d}{2}\log\!\left(1+ \frac{tL^2}{\lambda d}\right)}   +\sqrt{\frac{\lambda}{2}}M\right)^2\,.
\end{align*}
\end{theorem}

\subsection{Other technical lemmas}\label{app:lgc.lemmas.other}

We regroup in this section some other useful technical lemmas.

\begin{lemma}
\label{lem:inq_revert_sqrt}
For all $\alpha,y\geq 0$, if for some $x\geq 0$ if holds $y \geq x-\alpha\sqrt{x}$ then
\[
x \leq y + \alpha \sqrt{y} + \alpha^2\,.
\]
\end{lemma}
\begin{proof}
Just note that for $z=\sqrt{x}$ we have
\[
z^2-\alpha z -y \leq 0\,,
\]
thus
\begin{align*}
  x \leq \frac{1}{4}\left(\alpha +\sqrt{\alpha^2+4y}\right)^2
  \leq y +\frac{\alpha^2}{2}+\frac{\alpha}{2}\sqrt{\alpha^2+4y}
  \leq y +\alpha\sqrt{y}+\alpha^2\,.
\end{align*}
\end{proof}

We then state a result derived from the concavity of $\bLambda\mapsto \log\det(\bLambda)$.
\begin{lemma}
\label{lem:sum_w_norm_a}
Let $(w_t)_{t\geq 1}$ be a sequence in $\Sigma_K$ and $\lambda>0$ then
\[
\sum_{s=1}^t \sum_{\bx\in\cX} w_{\bx}^s \normm{\bx}^2_{W_s +\lambda I_d} \leq d \log\left(1 +\frac{t L^2}{d \lambda}\right)\,.
\]
where $W_t = \sum_{s=1}^t w_s$.
\end{lemma}
\begin{proof}
Define the function $f(W)= \log\det(\bLambda_W+\lambda I_d)$ for any $W\in(\R^+)^K$. It is a concave function since the function
$\bLambda\mapsto \log\det(\bLambda)$ is a concave function over the set of positive definite matrices (see Exercise 21.2 of \citealt{lattimore2018bandits}). And its partial derivative with respect to the coordinate $a$ at $W$ is
\[
\nabla_{\bx} f(W) = \normm{\bx}^2_{(W+\lambda I_d)^{-1}}\,.
\]
Hence using the concavity of $f$ we have
\begin{align*}
  \sum_{\bx\in\cX} w_{\bx}^s \normm{\bx}^2_{(\bLambda_{W_s} +\lambda I_d)^{-1}} = \langle W_s - W_{s-1}, \nabla_{\bx} f(W_s) \rangle \leq f(W_s) - f(W_{s-1})\,.
\end{align*}
Which implies that
\begin{align*}
  \sum_{s=1}^t \sum_{\bx\in\cX} w_{\bx}^s \normm{\bx}^2_{\bLambda_{W_s} +\lambda I_d} \leq f(W_t)-f(W_0) = \log\left(\frac{\det(\bLambda_{W_t} +\lambda I_d) }{\det(\lambda I_d)}\right) \leq d \log\left(1 +\frac{t L^2}{d \lambda}\right)\,,
\end{align*}
where for the last inequality we use the inequality of arithmetic and geometric means in combination with $\Tr(W_t) \leq t L^2$\,.
\end{proof}

A simple consequence of the previous lemma follows.
\begin{lemma} For all $t$,
\label{lem:computation_sum_w_a_N}
\begin{align*}
\sum_{s=1}^t \sum_{\bx\in\cX} \tw_s^{\bx} \normm{\bx}_{(\bLambda_{T_{s-1}}+\lambda I_d)^{-1}}^2 &\leq 2h(t) = 2d_{t,1/t^\alpha} \\
\sum_{s=1}^t \sum_{\bx\in\cX} w_s^{\bx} \normm{\bx}_{(\bLambda_{T_{s-1}}+\lambda I_d)^{-1}}^2 &\leq 2h(t)\,.
\end{align*}
\end{lemma}

\begin{proof}
According to the tracking procedure of~\cite{degenne2020structure}, we know that $T_{s-1}^{\bx} \geq \tW_{s-1}^{\bx} -\log(KA)$. Thus, in combination with the choice of $\lambda$  we can replace counts by weights
\begin{align*}
  \bLambda_{T_{s-1}} + \lambda I_d \geq \bLambda_{\tW^{\bx}_{s}} - \bLambda_{\tw_s^{\bx}} - \log(KA) \bLambda_{\bOne_K} +\lambda I_d \geq  \bLambda_{\tW^{\bx}_{s}} - (\log(K)+1) \bLambda_{\bOne_K} +\lambda I_d \geq \bLambda_{W_{s}}+ \frac{\lambda}{2} I_d\,,
\end{align*}
where $\bOne_K = (1,\ldots,1)\in\R^K$.
Hence we obtain
\[
\normm{\bx}_{(\bLambda_{T_{s-1}}+\lambda I_d)^{-1}}^2 \leq \normm{\bx}_{(\bLambda_{\tW^{\bx}_{s}}+(\lambda/2) I_d)^{-1}}^2\,,
\]
and applying Lemma~\ref{lem:sum_w_norm_a} leads to
\[
\sum_{s=1}^t \sum_{\bx\in\cX} \tw_s^{\bx} \normm{\bx}_{(\bLambda_{T_{s-1}}+\lambda I_d)^{-1}}^2 \leq d \log\!\left(1 +\frac{t L^2}{d \lambda} \right)\leq 2h(t)\,.
\]
The exact same proof holds for $w^{\bx}_s$ instead of $\tw_s^{\bx}$ since thanks to the tracking we have also in this case $T_{s-1}^{\bx} \geq W_{s-1}^{\bx} -\log(K) \geq W_{s-1}^{\bx} -\log(KA)$.

\end{proof}



\iffalse
Let $W_{-1}$ be the negative branch of the Lambert W function and for $x>1$, let $\overline{W}(x) = -W_{-1}(-e^{-x})$. It verifies $\overline{W}(x) \le x + \log(x) + \min\{\frac{1}{2}, \frac{1}{\sqrt{x}}\}$.

Let $\zeta$ be the Riemann function defined by $\zeta(x) = \sum_{n=1}^{+\infty}\frac{1}{n^x}$ .

\begin{theorem}\label{thm:maximal_concentration_inequality}
For all $\lambda>1$ and $\gamma>1$, with probability $1-\delta$, for all $t\in \N$,
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert_{\bLambda_t}^2
&\le \frac{d}{2} \overline{W}\left( 1 + \frac{2}{d}\log \frac{\gamma^{d/2}\zeta(\lambda)^K\prod_{k=1}^K(\log_\gamma(\gamma T_t^k))^\lambda}{\delta} \right)
\end{align*}
\end{theorem}

\todo[inline]{That result would be much better if there was no $K$, but only $d$.

TODO: the proof uses $T_t^k \ge 1$. It should be in the hypotheses.}

%Another concentration result:
%\begin{theorem}
%Let $L = \max_a \Vert a \Vert$ . With probability $1- \delta$, for all $t\in \N$,
%\begin{align*}
%\frac{1}{2}\Vert \theta - \hat{\theta}_t \Vert_{(\bLambda_t^{-1} + I_d/t)^{-1}}^2
%\le \log\frac{1}{\delta} + \frac{d}{2}\log\left( 1 + e\frac{t^2 L^2}{d} \right) + \log(\zeta(\lambda)(1+\log t)^\lambda) \: .
%\end{align*}
%\end{theorem}
%\todo[inline]{TODO proof. Same as the other one, but with $M_0 = \lambda I_d$ and grid over that $\lambda$ to have $\lambda\approx 1/t$}

Another concentration result:
\begin{theorem}
Let $A = \sum_{k=1}^K a_k a_k^\top$. Let $L = \max_a \Vert a \Vert$ . With probability $1- \delta$, for all $t\in \N$,
\begin{align*}
\frac{1}{2}\Vert \theta - \hat{\theta}_t \Vert_{\bLambda_t}^2
\le (1+\lambda)\left(
	\log\frac{1}{\delta}
	+ \frac{d}{2}\log\left( 1 + e\frac{t L^2}{d \lambda_{\min}(A) \lambda} \right)
\right) \: .
\end{align*}
With probability $1- \delta$, for all $t\in \N$,
\begin{align*}
\frac{1}{2}\Vert \theta - \hat{\theta}_t \Vert_{\bLambda_t}^2
\le \frac{d}{2}\overline{W}\left(
	1 + \frac{2}{d}\log\frac{1}{\delta}
	+ \log \frac{t e L^2}{d \lambda_{\min}(A) }
	+ \frac{2}{d}\log(\zeta(\lambda)\log(et)^\lambda)
 \right) \: .
\end{align*}
For all $t\in\N$, with probability $1- \delta$, for all $s\le t$,
\begin{align*}
\frac{1}{2}\Vert \theta - \hat{\theta}_s \Vert_{\bLambda_s}^2
\le \frac{d}{2}\overline{W}\left(
	1 + \frac{2}{d}\log\frac{1}{\delta}
	+ \log \frac{s e L^2}{d \lambda_{\min}(A) }
	+ \frac{2}{d}\log\log(et)
 \right) \: .
\end{align*}
\end{theorem}
\todo[inline]{TODO proof. Take $M_0 = \lambda \lambda_{\min}(A) I_d$ and optimize over that $\lambda$}

We first prove the following Lemma.

\begin{lemma}\label{lem:concentration}
For all stopping times $t$, for all positive definite matrices $M_0$, with probability $1- \delta$,
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \log\frac{1}{\delta} + \frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert_{(\bLambda_t^{-1} + M_0^{-1})^{-1}}^2 + \frac{1}{2} \log \det (I_d + \bLambda_t M_0^{-1})
\end{align*}
As a consequence, using $\Vert \theta - \hat{\theta}_t \Vert_{(\bLambda_t^{-1} + M_0^{-1})^{-1}}^2 \le \Vert \theta - \hat{\theta}_t \Vert_{\bLambda_t}^2 \Vert (I + \bLambda_t^{1/2} M_0^{-1}\bLambda_t^{1/2})^{-1} \Vert$, with probability $1- \delta$,
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \frac{1}{1 - \Vert (I + \bLambda_t^{1/2} M_0^{-1}\bLambda_t^{1/2})^{-1} \Vert}\left( \log\frac{1}{\delta} + \frac{1}{2} \log \det (I_d + \bLambda_t M_0^{-1})\right)
\\
&= \left( 1 + \frac{1}{\lambda_{\min} (\bLambda_t^{1/2} M_0^{-1}\bLambda_t^{1/2})}\right) \left( \log\frac{1}{\delta} + \frac{1}{2} \log \det (I_d + \bLambda_t M_0^{-1})\right)
\end{align*}
\end{lemma}

For a matrix $\bLambda_t$ fixed in advance and $\lambda>0$, we could take $M_0 = \lambda \bLambda_t$ in this lemma to obtain
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le (1+\lambda)\left(\log\frac{1}{\delta} + \frac{d}{2} \log (1 + \frac{1}{\lambda})\right)
\end{align*}
We could then optimize over $\lambda>0$. We are however interested in random $\bLambda_t$.

\begin{proof}[Proof of Lemma~\ref{lem:concentration}]
We first prove that for all distributions $\rho_0$, the following quantity $W_t(\rho_0)$ is such that $e^{W_t(\rho_0)}$ a martingale.
\begin{align*}
W_t(\rho_0) &= \sup_\rho \left( \int \left[y^\top \sum_{s=1}^t X_s a_s - \sum_{s=1}^t \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho(dy) - \KL(\rho,\rho_0) \right)
\end{align*}

Indeed
\begin{align*}
\mathbb{E}[ e^{W_t(\rho_0)}| \mathcal F_{t-1}]
&= \mathbb{E}_{|\mathcal F_{t-1}}\exp \sup_\rho \left( \int \left[y^\top \sum_{s=1}^t X_s a_s - \sum_{s=1}^t \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right]  \rho(dy) - \KL(\rho,\rho_0) \right)
\\
&= \mathbb{E}_{|\mathcal F_{t-1}} \int \exp \left[y^\top \sum_{s=1}^t X_s a_s - \sum_{s=1}^t \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho_0(dy)
\\
&=  \int \mathbb{E}_{|\mathcal F_{t-1}} \exp \left[y^\top \sum_{s=1}^t X_s a_s - \sum_{s=1}^t \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho_0(dy)
\\
&=  \int \exp \left[y^\top \sum_{s=1}^{t-1} X_s a_s - \sum_{s=1}^{t-1} \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right]\mathbb{E}_{|\mathcal F_{t-1}} \exp \left[y^\top  X_t a_t - \log \mathbb{E}_{X\sim \theta^\top a_t+\varepsilon}e^{y^\top X a_t} \right] \rho_0(dy)
\\
&=  \int \exp \left[y^\top \sum_{s=1}^{t-1} X_s a_s - \sum_{s=1}^{t-1} \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho_0(dy)
\\
&=   \exp \sup_\rho \left(\int\left[y^\top \sum_{s=1}^{t-1} X_s a_s - \sum_{s=1}^{t-1} \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho(dy) - \KL(\rho, \rho_0) \right)
\\
&= e^{W_{t-1}(\rho_0)} \: .
\end{align*}

Similar computations show that $\mathbb{E}[e^{W_1(\rho_0)}] = 1$. We apply Doob's inequality to obtain that for all stopping times $t$, with probability $1- \delta$, $e^{W_{t}(\rho_0)} \le \frac{1}{\delta}$, i.e. for all distributions $\rho$,
\begin{align*}
\int \left[y^\top \sum_{s=1}^t X_s a_s - \sum_{s=1}^t \log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} \right] \rho(dy)
\le \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\end{align*}

In our setting, $\log \mathbb{E}_{X\sim \theta^\top a_s+\varepsilon}e^{y^\top X a_s} = \theta^\top a_s y^\top a_s+\frac{1}{2} (y^\top a_s)^2$ . Hence for all stopping times $t$, with probability $1- \delta$, for all distributions $\rho$,
\begin{align*}
\int \left[y^\top \sum_{s=1}^t X_s a_s - \theta^\top \bLambda_t y -\frac{1}{2} y^\top \bLambda_t y \right] \rho(dy)
\le \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\end{align*}
Let $\hat{\theta}_t = \bLambda_t^{-1}\sum_{s=1}^t X_s a_s$.
\begin{align*}
\int \left[y^\top \bLambda_t (\hat{\theta}_t - \theta) - \frac{1}{2} y^\top \bLambda_t y \right] \rho(dy)
&\le \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\\
\Rightarrow \int \left[(z - \theta)^\top \bLambda_t (\hat{\theta}_t - \theta) - \frac{1}{2} (z - \theta)^\top \bLambda_t (z - \theta) \right] \rho(d(z - \theta))
&\le \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\end{align*}
The fact that this is valid for all $\rho$ is equivalent to the following being true for all $\rho$,
\begin{align*}
\int \left[(z - \theta)^\top \bLambda_t (\hat{\theta}_t - \theta) - \frac{1}{2} (z - \theta)^\top \bLambda_t (z - \theta) \right] \rho(dz)
&\le \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\\
\Leftrightarrow
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \int  \frac{1}{2} \Vert y - \hat{\theta}_t \Vert_{\bLambda_t}^2  \rho(dy)  + \log\frac{1}{\delta} + \KL(\rho,\rho_0)
\end{align*}

For a given $\rho_0$, the minimum of the right hand side is
\begin{align*}
\inf_\rho \int \frac{1}{2} \Vert y - \hat{\theta}_t \Vert_{\bLambda_t}^2  \rho(dy) + \KL(\rho,\rho_0)
&= - \sup_\rho \left(\mathbb{E}_{y\sim\rho}[\frac{1}{2} \Vert y - \hat{\theta}_t \Vert_{\bLambda_t}^2] - \KL(\rho, \rho_0)\right)
\\
&= - \log \mathbb{E}_{y \sim \rho_0} e^{- \frac{1}{2} \Vert y - \hat{\theta}_t \Vert_{\bLambda_t}^2}
\end{align*}

We have proved that for all stopping times $t$, with probability $1- \delta$,
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \log\frac{1}{\delta} - \log \mathbb{E}_{y \sim \rho_0} e^{- \frac{1}{2} \Vert y - \hat{\theta}_t \Vert_{\bLambda_t}^2}
\end{align*}

For $\rho_0 = \mathcal N(\theta, M_0^{-1})$ , this is
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \log\frac{1}{\delta} + \frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert_{(\bLambda_t^{-1} + M_0^{-1})^{-1}}^2 + \frac{1}{2} \log \det (I_d + \bLambda_t M_0^{-1})
\end{align*}

\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:maximal_concentration_inequality}]
Let $\gamma > 1$, $\lambda > 1$. Let $\zeta$ be the Riemann function defined by $\zeta(x) = \sum_{n=1}^{+\infty}\frac{1}{n^x}$ . For $i\in \N$, let $w_i = \frac{1}{\zeta(\lambda)(1+i)^\lambda}$. For a tuple $I=(i_1,\ldots,i_K)\in \N^K$, let $W_I = \prod_{k=1}^K w_{i_k}$. Note that $\sum_{I \in \N^K} W_I = 1$ .

Let $M_{0,I} = \lambda_I \sum_{k=1}^K \gamma^i a_k a_k^\top$ where $\lambda_I > 0$ will be defined later.

With probability $1- \delta W_I$,
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \log\frac{1}{W_I \delta} + \frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert_{(\bLambda_t^{-1} + M_{0,I}^{-1})^{-1}}^2 + \frac{1}{2} \log \det (I_d + \bLambda_t M_{0,I}^{-1})
\end{align*}

With probability $1- \delta$ this is true for all $I \in \N^K$. In particular, it is true for $I=(i_1,\ldots,i_K)$ such that for all $k \in [K]$, $\gamma^{i_k} \le T_t^k < \gamma^{i_k+1}$ . For that $I$, we get that $\lambda_I \bLambda_t \frac{1}{\gamma} \preceq M_{0,I} \preceq \lambda_I \bLambda_t$ (where $A\preceq B$ means that $B-A$ is positive semi-definite) and
\begin{align*}
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le \log\frac{\zeta(\lambda)^K\prod_{k=1}^K(1+i_k)^\lambda}{\delta} + \frac{1}{2} \frac{\lambda_I}{1+\lambda_I} \Vert \theta - \hat{\theta}_t \Vert_{\bLambda_t}^2 + \frac{d}{2} \log (\gamma(1 + \frac{1}{\lambda_I}))
\\
\Rightarrow
\frac{1}{2} \Vert \theta - \hat{\theta}_t \Vert^2_{\bLambda_t}
&\le (1 + \lambda_I)\left(\log\frac{\zeta(\lambda)^K\prod_{k=1}^K(1+i_k)^\lambda}{\delta} + \frac{d}{2} \log (\gamma(1 + \frac{1}{\lambda_I})) \right)
\end{align*}

The minimal value of the r.h.s. over $\lambda_I$ is
\begin{align*}
\frac{d}{2} \overline{W}\left( 1 + \frac{2}{d}\log \frac{\gamma^{d/2}\zeta(\lambda)^K\prod_{k=1}^K(1+i_k)^\lambda}{\delta} \right)
\end{align*}
We make that choice for $\lambda_I$ and remark that $(1+i_k)^\lambda \le (1+\log_\gamma T_t^k)^\lambda$ to get the result of the theorem.


\end{proof}

\begin{lemma}
For $a>1$, the minimal value over $\lambda>0$ of $f(\lambda) = (1+\lambda)(a+\log(1+1/\lambda))$ is $\overline{W}(1+a)$.
\end{lemma}
\begin{proof}
Solve the equation $f'(\lambda)=0$ and remark that the point obtained is a minimum.
\end{proof}

\fi
