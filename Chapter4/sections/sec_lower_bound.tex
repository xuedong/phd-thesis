%!TEX root = ../Chapter4.tex
\section{Fixed-Confidence Optimality and Complexities}\label{sec:lgc.complexity}

%As first stated by~\cite{soare2014linear}, we only consider a finite number of arms and we restrict to bandits  with Gaussian rewards and conjugate (Gaussian) priors in this work. 
Our primary goal is to propose a BAI strategy that outputs quickly and reliably a final guess of the best arm. Formally, given a risk level $\delta$, we want to show that
\[
  \PP{\bx_{J_{\tau_\delta}}\neq \bx^\star} \leq \delta,
\]
while minimizing the expected number of samples $\EE{\tau_\delta}$ that is required. To achieve this objective, we first investigate the lower bound of the sample complexity.

\subsection{Lower bound}\label{sec:lgc.complexity.lb}

In this section we extend the lower bound of \citet{garivier2016tracknstop}, to hold for \emph{pure exploration in finitely-armed linear bandit} problems.

\paragraph{Alternative.}
Recall the notion of \gls{alternative set} which is already defined in Section~\ref{sec:mab.performance.sample} for \gls{bai}. For the general pure-exploration problem, the definition is given below.

\begin{definition}[alternative set (pure exploration)]
\begin{leftbar}[defnbar]
For any answer $i\in\cI$ we define the \emph{alternative set} of arm $i$, denoted by $\neg i$ the set of parameters where the answer $i$ is not correct, i.e.
\[
    \neg i \eqdef \{\btheta\in\Theta:\ i\neq\Istar(\btheta)\}\,.
\]
\end{leftbar}
\end{definition}

%We also define, for any $w\in(\R^+)^A$, the design matrix \[\bLambda_{\bomega} \eqdef \sum_{a\in \cX} w^a a a^\top\,.\] Further, we define $\normm{x}_V\eqdef \sqrt{x^\top Vx}$ for $x\in\R^d$ and a symmetric positive matrix $V\in\R^{d\times d}$. Note that it is a norm only if $V$ is positive definite. We also denote by $\Sigma_K$ the probability simplex of dimension $K-1$ for all $K\ge 2$.

\paragraph{Lower bound.} 
A general result on the (non-asymptotic) sample-complexity lower bound in the fixed-confidence regime~\citep{garivier2016tracknstop}, which we reviewed in Section~\ref{sec:mab.performance.sample}, states that for any $\delta$-correct strategy, we have
\begin{align}\label{eq:lgc.lb}
    \EE{\tau_\delta} \geq T^\star(\btheta)\log(\frac{1}{3\delta}),
\end{align}
for a given parameter $\btheta$ and a given confidence level $\delta$. And the characteristic time $T^\star(\btheta)$ is written as
\begin{align}\label{eq:lgc.characteristic2}
    \Tstar(\btheta)^{-1} \eqdef \max_{\bomega \in \Sigma_K} \inf_{\btheta'\in \neg\Istar(\btheta)} \frac{1}{2}\normm{\btheta - \btheta'}_{\bLambda_{\bomega}}^2\,,
\end{align}
and it can be further particularized into~\eqref{eq:lgc.characteristic1} defined in Proposition~\ref{prop:lgc.lb} for linear bandits.

\begin{proposition}\label{prop:lgc.lb}
\begin{leftbar}[propositionbar]
In the linear case, the quantity $T^\star(\btheta)$ is written as
\begin{align}\label{eq:lgc.characteristic1}
    T^\star(\btheta) \eqdef \inf_{\bomega\in\Sigma_K}\max_{\bx\neq \bx^\star} \frac{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}{\left(\bx\transpose\btheta-(\bx^\star)\transpose\btheta\right)^2}\,.
\end{align}
\end{leftbar}
\end{proposition}

%In fact using the Sion's minimax theorem we can rewrite the characteristic times in various ways which will be useful to prove the lower bound of Theorem~\ref{th:lb_genral}.

\begin{proof}
Using the alternative set of $\Istar(\btheta)$, and by~\eqref{eq:lgc.characteristic2}, we obtain
\begin{align*}
    T^\star(\btheta)^{-1} &= \max_{\bomega \in \Sigma_K} \inf_{\btheta'\in \neg\Istar(\btheta)} \frac{1}{2}\normm{\btheta - \btheta'}_{\bLambda_{\bomega}}^2\\
    &= \max_{\bomega\in\Sigma_K}\inf_{\btheta'\in\neg\Istar(\btheta)}\sum_{i=1}^K \omega_i d(\mu_i;\mu_i')\\
    &= \max_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\inf_{\bx\transpose\btheta'>(\bx^\star)\transpose\btheta'} \frac{\normm{\btheta-\btheta'}_{\bLambda_{\bomega}}^2}{2}.
\end{align*}
Then we introduce the Lagrangian with $\eta$ as the Lagrange multiplier, and it then becomes
\begin{align*}
    T^\star(\btheta)^{-1} &= \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\inf_{\btheta'}\sup_{\eta>0} \frac{\normm{\btheta-\btheta'}_{\bLambda_{\bomega}}^2}{2} - \eta(\bx-\bx^\star)\transpose\btheta',
\end{align*}
and the inner expression attains its minimum when it comes
\[
    \bLambda_{\bomega}(\btheta-\btheta') = \eta(\bx^\star-\bx),
\]
which implies
\begin{align*}
    T^\star(\btheta)^{-1} &=
    \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\sup_{\eta>0} \eta(\bx^\star-\bx)\transpose\btheta - \frac{\eta^2\normm{\bx-\bx^\star}_{\bLambda_{\bomega}^{-1}}}{2\sigma^2}\\
    &= \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\frac{\left(\bx\transpose\btheta-(\bx^\star)\transpose\btheta\right)^2}{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}\,.
\end{align*}
\end{proof}

\paragraph{Asymptotic optimality.}
We can define the asymptotic optimality upon $T^\star(\btheta)$. 

\begin{definition}\label{def:lgc.optimality}
\begin{leftbar}[defnbar]
A BAI strategy is called optimal in the fixed-confidence setting if it satisfies
\[
    \limsup_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \leq T^\star(\btheta)\,.
\]
\end{leftbar}
\end{definition}

\begin{remark}
\begin{leftbar}[remarkbar]
Using the same lower-bound techniques, one can also prove that under any $\delta$-correct strategy satisfying $T_{n,I^\star}/n \rightarrow \beta$ for a given $\beta$,
\[
    \liminf_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \geq T^\star_\beta(\btheta)\,,
\]
where $T^\star_\beta(\btheta)$ is defined in the same way as $T^\star(\btheta)$, but restricted to the constraint $\omega_{I^\star}=\beta$,
\[
    T^\star_{\beta}(\btheta) \eqdef \inf_{\bomega\in\Sigma_K,\omega_{I^\star}=\beta}\max_{\bx\neq \bx^\star} \frac{2\sigma^2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}{\left(\bx\transpose\btheta-(\bx^\star)\transpose\btheta\right)^2}\,.
\]

% And a relaxed notion of optimality that depends on $\beta$ can be also defined. A BAI strategy is called \emph{$\beta$-optimal} if it satisfies 
% \[
%     \frac{T_{n,I^\star}}{n}\rightarrow \beta \quad \text{and} \quad \limsup_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \leq T^\star_\beta(\bmu).
% \]

Essentially, we can recover the $\beta$-optimality defined in Chapter~\ref{CHAP:T3C}.
\end{leftbar}
\end{remark}

\subsection{Best-arm identification complexity}\label{sec:lgc.complexity.complexity}

The inverse of the characteristic time of Proposition~\ref{prop:lgc.lb} can also be written as
\[
    \Tstar(\btheta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\bx\neq \bx^\star} \frac{\left(\bx\transpose\btheta-(\bx^\star)\transpose\btheta\right)^2}{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}
\]
for BAI.

Since the characteristic time involves many problem dependent quantities that are unknown to the learner, previous work target loose problem-independent upper bounds on the characteristic time. \citet{soare2014linear} (see also \citealt{tao2018alba,fiez2019transductive}) introduce the G-complexity (denoted by $\gopt$) which coincides with the G-optimal design of experimental design theory (see \citealt{pukelsheim2006optimal}) and the $\xyopt$-complexity (denoted by $\xyopt$) inspired by the transductive experimental design theory  \citep{yu2006active},
\begin{align*}
    \gopt &=\min_{\bomega\in\Sigma_K} \max_{\bx\in\cX} \normm{\bx}_{\bLambda_{\bomega}^{-1}}^2\,,\\
    \xyopt &=\min_{\bomega\in\Sigma_K} \max_{\by\in\cY_{\texttt{dir}}} \normm{\by}_{\bLambda_{\bomega}^{-1}}^2\,,
\end{align*}
where $\cY_{\texttt{dir}}$ is the set of directions induced by $\cX$:
\[
\cY_{\texttt{dir}}\eqdef\{\bx-\bx':\ (\bx,\bx')\in\cX\times\cX\}\,.
\]

For the G-optimal complexity we seek for a proportion of pulls $\bomega$ that explores \emph{uniformly} the means of the arms, since the statistical uncertainty for estimating $\bx\transpose\btheta$ scales roughly with $\normm{\bx}_{\bLambda_{\bomega}^{-1}}$. In the $\xyopt$-complexity we try to estimate \emph{uniformly} all the \emph{directions} $\bx-\bx'$, while a potentially more plausible quantity to maximize would be the characteristic time itself. For the latter, we try to estimate all the \emph{directions} $\bx^\star - \bx$ scaled by the squared gaps $(\bx^\star - \bx)\transpose\btheta$. 

The fact that previous works maximize over loose upper bounds on the characteristic time is potentially the main reason that they cannot achieve optimality. We can see later (in Section~\ref{sec:lgc.game}) that directly maximizing the weighted gaps would indeed lead to an asymptotically optimal algorithm.

Note that the characteristic time can also be seen as a particular optimal transductive design. Indeed for 
\[
    \cY^\star \eqdef \left\{ \frac{\bx^\star(\btheta)- \bx}{\left|(\bx^\star(\btheta)- \bx)\transpose\btheta\right|}: \bx\in\cX/\big\{\bx^\star(\btheta)\big\}  \right\}\,,
\]
it holds
\[
    \Tstar(\btheta) = 2 \cX\cY^\star(\btheta) \eqdef 2 \min_{\bomega\in\Sigma_K} \max_{\by\in\cY^\star(\btheta)} \normm{\by}_{\bLambda_{\bomega}^{-1}}^2\,.
\]

Besides, we have the following ordering on the complexities
\begin{align}\label{eq:lgc.complexities}
    \Tstar(\btheta) \leq 2 \frac{\xyopt}{\DeltaMin(\btheta)^2}\leq 8 \frac{\gopt}{\DeltaMin(\btheta)^2} = \frac{8d}{\DeltaMin(\btheta)^2}\,,
\end{align}
where $\DeltaMin = \min_{\bx\neq\bx^\star(\btheta)}(\bx^\star(\btheta)- \bx)\transpose\btheta$ and the last equality follows from the Kiefer-Wolfowitz equivalence theorem~\citep{kiefer1959}. 

%Conversely the $\gopt$-complexity and the $\xyopt$-complexity are linked to an \emph{other} pure exploration problem, the thresholding bandits (see Appendix~\ref{app:threshold_bandits}).

\begin{remark}
\begin{leftbar}[remarkbar]
In order to compute all these complexities, it is sufficient to solve the following generic optimal transductive design problem: for $\cY$ a finite set of elements in $\R^d$,
\[
\xyopt=\min_{\bomega\in\Sigma_K}\max_{\by\in\cY}\normm{\by}^2_{\bLambda_{\bomega}^{-1}}\,.
\]
When $\cY=\cX$ we can use an algorithm inspired by Frank-Wolfe \citep{frank1956algorithm} which possesses convergence guarantees~\citep{atwood1969optimal,ahipasaoglu2008fw}. But in the general case, up to our knowledge, there is no algorithm with the same kind of guarantees. Previous works used an heuristic based on a straightforward adaptation of the aforementioned algorithm for general sets $\cY$ but it seems to not converge on particular instances (see Section~\ref{sec:lgc.experiments.complexity}). We instead propose in the same section an algorithm based on saddle-point Frank-Wolfe algorithm that seems to converge on the different instances we tested.
\end{leftbar}
\end{remark}

\paragraph{Empirical evaluation.}

We use the following problem instance to illustrate how various complexities differ in practice. In this instance, contexts are the canonical basis  $\bx_1 = \be_1, \bx_2 = \be_2, \cdots, \bx_d = \be_d$, plus an additional disturbing context 
\[
    \bx_{d+1} = (\cos(\alpha), \sin(\alpha), 0, \ldots, 0)\transpose\,,
\] 
and a true regression parameter which is proportional to $\be_1$: $\btheta^\star = c\be_1$. This instance is frequently used in the literature of linear bandits BAI (see e.g.~\citealt{soare2014linear,xu2018linear}) and is considered as a hard instance to test the performance of linear BAI algorithms. In this problem, the best arm is always $\be_1$, but when the angle $\alpha$ is small, the disturbing context is hard to discriminate from $\be_1$. In this section, we set $d=2, c=2, \delta=0.01$ and $\alpha=0.1$.

In Table~\ref{table:optimal_weights} we compare the different complexities previously mentioned: the characteristic time $\Tstar(\btheta)$ and its associated optimal weights $\bomega^\star_{\cX\cY^\star(\btheta)}$, the $\xyopt$-complexity and its associated optimal design $\bomega^\star_{\xyopt}$, the G-optimal complexity $\gopt$ and its associated optimal design $\bomega^\star_{\cX\cX}$. For each weight vector 
\[
    \bomega \in\left\{\bomega^\star_{\cX\cY^\star(\btheta)},\bomega_{\xyopt}, \bomega_{\gopt}\right\}\,,
\]
we also provide the lower bound $T_{\bomega}$ given by~\eqref{eq:lgc.lb}, i.e.
\[
    T_{\bomega} = \max_{\bx\neq \bx^\star(\btheta)} \frac{\left((\bx^\star(\btheta)-\bx)\transpose\btheta\right)^2}{2\normm{\bx^\star(\btheta)-\bx}_{\bLambda_{\bomega}^{-1}}^2} \log(1/\delta)\,.
\]
In particular we notice that targeting the proportions of pulls $\bomega_{\xyopt}, \bomega_{\gopt}$ leads to a much larger lower bound than the one obtained with the optimal weights.
\begin{table}[t!]
\centering
%\def\arraystretch{1.2}
\begin{tabular}{|c|c|c|c|}
 \hline
   & $\bomega^\star_{\cX\cY^\star}$ & $\bomega^\star_{\xyopt}$  & $\bomega^\star_{\gopt}$   \\
 \hline
 \textbf{$\bx_1$} & $0.047599$ & $0.499983$ & $0.499983$ \\
 \hline
 \textbf{$\bx_2$} & $0.952354$ & $0.499983$ & $0.499983$ \\
 \hline
 \textbf{$\bx_3$} & $0.000047$ & $0.000033$ & $0.000033$ \\
 \hline
 \textbf{$T_{\bomega}$} & $369$ & $2882$ & $2882$ \\
 \hline
   & $\Tstar(\theta)$ & $2\xyopt/\DeltaMin^2$ & $8\gopt/\DeltaMin^2$\\
 \hline
  \textbf{Complexity} & $0.124607$ & $32.0469$ & $64.0939$ \\
 \hline
\end{tabular}
\caption{Optimal weights for various complexities with $\DeltaMin= 0.0049958$.}
\label{table:optimal_weights}
\end{table}

% TODO:
% \begin{itemize}
%   \item compare the three complexity on the problem $\theta = [1,0], a_1 = [1,0 ], a_2 =[1,\epsilon], a_3 = [0,1]$
%   \item compare also on the same problem
%   \[
%   \inf_{\lambda\in \neg \istar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\,,
%   \]
%   for $w = w^\star(\theta), w^G, w^{\cX\cY}$.
% \end{itemize}

\subsection{Computation of different complexities}\label{sec:lgc.experiments.complexity}

As mentioned in Section~\ref{sec:lgc.complexity.lb}, computing the solution to a specified optimization problem is required in many existing linear BAI algorithms. We survey some methods that can potentially be useful to handle that issue.

We recall that the three notions of complexity $\gopt, \xyopt, \cX\cY^\star(\btheta)$ can be written in a unified form,
\begin{align}\label{eq:complexity_general}
    \cX\cY = \min_{\bomega\in\Sigma_K} \max_{\by\in\cY}\normm{\by}_{\bLambda_{\bomega}^{-1}}^2,
\end{align}
where $\cY$ is the transductive set, i.e. a finite set of elements in $\R^d$. Transductive sets corresponding to different complexity types mentioned in this paper can be found in Table~\ref{tab:transductive_sets}.

\begin{table}[ht]
    \centering
	\begin{tabular}{@{}lll@{}}
		\toprule
		\thead{Allocation type} & \thead{Arm set} & \thead{Transductive set} \\ \midrule
		(1) $\gopt$-allocation & $\cX$ & $\cX$\\
		(2) $\xyopt$-allocation & $\cX$ & $\cY_{\texttt{dir}} = \{\bx-\bx':\ (\bx,\bx')\in\cX\times\cX\}$ \\
		(3) $\cX\cY^\star(\btheta)$-allocation & $\cX$ & $\cY^\star(\btheta) = \left\{ (\bx^\star(\btheta)- \bx)/\left|(\bx^\star(\btheta)-\bx)\transpose\btheta\right|: \bx\in\cX/\big\{\bx^\star(\btheta)\big\}  \right\}$ \\
		\bottomrule
	\end{tabular}
	\caption{Some examples of different transductive sets.}
	\label{tab:transductive_sets}
\end{table}

\paragraph{Frank-Wolfe.} We can use a Frank-Wolfe heuristic to compute the optimizer of~\eqref{eq:complexity_general} shown in Algorithm~\ref{alg:fw_aa}. This heuristic is used for example by~\citet{fiez2019transductive}. Note that it has been proved to have a linear convergence guarantee when $\cY = \cX$~\citep{ahipasaoglu2008fw}. It is not clear, however, that the same guarantee holds for other transductive sets.

A simple sanity check to test whether a solver works smoothly is  to solve $\cX\cY^\star(\btheta)$ for classical multi-armed bandits (i.e. when $\cX= \{\be_1,\be_2,\ldots,\be_d\}$), for which a solver with guarantee exists (see \citealt{garivier2018explore}). In particular we found instances where Algorithm~\ref{alg:fw_aa} does not converge toward the optimal weights, for example: $\cX= \{\be_1,\be_2,\be_3\}, \theta = (0.9, 0.5, 0.5)$.

% \begin{algorithm}[ht]
%    \caption{Frank-Wolfe heuristic for computing $\gopt$-design}
%    \label{alg:fw_aa}
% \begin{algorithmic}
%    \State {\bfseries Input:} arm set $\cX\subset\R^d$, transductive set $\cY\subset\R^d$, maximum iterations $n$
%    \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), V \leftarrow{} I_d, t \leftarrow{} 0$
%    \While{$t<n$}
%         \State $i_{t}\in\argmin_{i\in\{1,\ldots,A\}}\max_{b\in\cY}\normm{b}^2_{(V+a_i a_i^\top)^{-1}}$
%         \State $V \leftarrow{} V + a_{i_{t}}a_{i_{t}}^\top$
%         \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{i_{t}}$
%         \State $t \leftarrow{} t+1$
%    \EndWhile
%    \RETURN $w$
% \end{algorithmic}
% \end{algorithm}


\begin{algorithm}[ht]
\centering
\caption{Frank-Wolfe heuristic for computing $\gopt$-design}
\label{alg:fw_aa}
\begin{algorithmic}
   \State {\bfseries Input:} arm set $\cX\subset\R^d$, transductive set $\cY\subset\R^d$, maximum iterations $N$
   \State  {\bfseries Initialize:} $\bomega \leftarrow{} (1, 1, \ldots, 1)\in\R^A, \Lambda \leftarrow{} I_d, t \leftarrow{} 0$
   \While{$n<N$}
        \State $\tbx\in\argmin_{\bx\in\cX}\max_{\by\in\cY}\langle \bx,\by\rangle_{\Lambda^{-1}}^2$%\normm{b}^2_{(V+a_i a_i^\top)^{-1}}$
        \State $\Lambda \leftarrow{} \Lambda + \tbx \tbx\transpose$
        \State $\bomega \leftarrow{} \frac{n}{n+1}\bomega+\frac{1}{n+1}\be_{\tbx}$
        \State $n \leftarrow{} n+1$
   \EndWhile
   \State {\bfseries Return} $\bomega$
\end{algorithmic}
\end{algorithm}

We propose a variant of the previous heuristic that takes into account a count for each element in the transductive set $\cY$. The pseudo-code of our method is displayed in Algorithm~\ref{alg:fw_ab}. $N\in\NN^{|\cY|}$ denotes the vector of counts for all $b\in\cY$. Sanity check on various MAB instances shows the correctness of our heuristic, its convergence guarantee remains for the future work.

% \begin{algorithm}[ht]
%    \caption{Saddle Frank-Wolfe heuristic for computing generic $\cX\cY$-design}
%    \label{alg:fw_ab}
% \begin{algorithmic}
%    \State {\bfseries Input:} arm set $\cX\subset\R^d$, transductive set $\cY\subset\R^d$, maximum iterations $n$
%    \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), N \leftarrow{} (1, 1, \ldots, 1), V \leftarrow{} I_d, t \leftarrow{} 0$
%    \While{$t<n$}
%         \State $i_{t}\in\argmin_{i\in\{1,\ldots,A\}}\sum_{j=1}^B(-2N[j]\langle a_i,V b_j\rangle^2)$
%         \State $j_{t}\in\argmax_{j\in\{1,\ldots,B\}}\normm{b_{j}}^2_{V^{-1}}$
%         \State $V \leftarrow{} V + a_{i_{t}}a_{i_{t}}^\top$
%         \State $N[j_t] \leftarrow{} N[j_t] + 1$
%         \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{i_{t}}$
%         \State $t \leftarrow{} t+1$
%    \EndWhile
%    \RETURN $w$
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[ht]
\centering
\caption{Saddle Frank-Wolfe heuristic for computing generic $\cX\cY$-design}
\label{alg:fw_ab}
\begin{algorithmic}
   \State {\bfseries Input:} arm set $\cX\subset\R^d$, transductive set $\cY\subset\R^d$, maximum iterations $N$
   \State  {\bfseries Initialize:} $\bomega \leftarrow{} (1, 1, \ldots, 1)\in\R^d, \tLambda \leftarrow{} I_d, \Lambda \leftarrow{} I_d, n \leftarrow{} 0$
   \While{$n<N$}
        \State $\tbx\in\argmax_{\bx\in\cX} \norm{\bx}_{\Lambda^{-1} \tLambda \Lambda^{-1} }^2$
        \State $\tby\in\argmax_{\by\in\cY} \normm{\by}^2_{\Lambda^{-1}}$
        \State $\Lambda \leftarrow{} \Lambda + \tbx\tbx\transpose$
		\State $\tLambda \leftarrow{} \tLambda + \tby\tby\transpose$
        \State $\bomega \leftarrow{} \frac{n}{n+1}\bomega+\frac{1}{n+1}\be_{\tbx}$
        \State $n \leftarrow{} n+1$
   \EndWhile
   \State {\bfseries Return} $\bomega$
\end{algorithmic}
\end{algorithm}

\paragraph{Entropic mirror descent.}
An entropic mirror descent alternative is used by~\citet{tao2018alba} to compute $\gopt$. The entropic mirror descent approach requires the knowldge of the Lipschitz constant of $\log\det \bLambda_{\bomega}$. Unfortunately, that Lipschitzness property does not seem to hold. \citet{lu2018convex} propose a solution to overcome the Lipschitz issue, but only for $\gopt$-design. Whether it still works for general $\cX\cY$-design remains an open question.

%\begin{algorithm}[ht]
%   \caption{Entropic mirror descent heuristic for computing $\gopt$-design}
%   \label{alg:algoMD}
%\begin{algorithmic}
%   \State {\bfseries Input:} arm set $\cX\subset\R^d$, transductive set $\cY\subset\R^d$, tolerance constant $\epsilon$, Lipschitz constant $L$
%   \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), t \leftarrow{} 1$
%   \While{$|\max_{a\in\cX}a^\top \bLambda_{\bomega}^{-1} a - d| \geq \epsilon$}
%        \State $\gamma \leftarrow{} \frac{2\sqrt{A}}{L} \frac{1}{\sqrt{t}}$
%        \State Compute gradient $\nabla^i \leftarrow{} \Tr(\bLambda_{\bomega}^{-1}a_ia_i^\top)$
%        \State $w^{a_i} \leftarrow{} \frac{w^{a_i}\exp(\gamma \nabla^i)}{\sum_{i=1}^A w^{a_i}\exp(\gamma \nabla^i)}$
%        \State $t \leftarrow{} t+1$
%   \EndWhile
%   \RETURN $w$
%\end{algorithmic}
%\end{algorithm}

\subsection{Some extensions}\label{sec:lgc.complexity.examples}

In this section, we present two relevant extensions of linear bandits BAI. We introduce the settings from a pure-exploration point of view.

% \paragraph{Best response for BAI.}\label{sec:lgc.formulation.examples.general}
% For BAI the goal is to identify the arm with the largest mean. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\  |\argmax_{a\in\cX} \langle\theta,a\rangle|>1\}$, the set of possible answers is $\cI = \cX$ and the correct answer is given by $\istar(\theta)=\astar(\theta)\eqdef \argmax_{a\in\cX} \langle\theta,a\rangle$.

% % \begin{lemma}
% % \label{lem:complexity_bai}
% % For all $\theta\in \cM$,
% % \[
% % \Tstar(\theta)^{-1} = \max_{w\in\Sigma_A} \min_{a\neq \astar(\theta)} \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% % \]
% % and
% % \[
% % \Tstar(\theta) = \min_{w\in\Sigma_A} \max_{a\neq \astar(\theta)} \frac{2\normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}{\big\langle \theta, \astar(\theta)-a\big\rangle^2}\,.
% % \]
% % \end{lemma}
% % \begin{proof}
% % Recall that the characteristic time is given by
% % \[
% % \Tstar(\theta)^{-1} = \max_{w \in \Delta_A} \inf_{\lambda\in \neg \astar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\,.
% % \]
% % We just express the set $\neg \astar(\theta)$ as a union of convex sets, and then compute the infimum for each one of them. Using Lemma~\ref{lem:lagrange_alternative}, it yields
% % \begin{align*}
% %   \Tstar(\theta)^{-1} &= \max_{w \in \Delta_A} \min_{a \neq \astar(\theta)} \inf_{\lambda: \langle\lambda,a\rangle > \langle\lambda,\astar(\theta)\rangle} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\\
% %   &= \max_{w \in \Delta_A} \min_{a \neq \astar(\theta)} \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,.
% % \end{align*}
% % The formula for $\Tstar(\theta)$ is then straightforward given the one for  $\Tstar(\theta)^{-1}$.

% % \end{proof}

% % In fact the characteristic time is just a particular case of the optimal transductive design. Indeed if we set
% % \[
% % \cYstar(\theta) \eqdef \left\{ \frac{1}{\left|\big\langle \theta, \astar(\theta)-a\big\rangle\right|}\big(\astar(\theta)- a\big): a\in\cX/\big\{\astar(\theta)\big\}  \right\}\,,
% % \]
% % then we have $\Tstar(\theta) = 2 \cX\cYstar(\theta)$ where
% % \[
% % \cX\cYstar(\theta) \eqdef  \min_{w\in\Sigma_A} \max_{b\in \cYstar} \normm{b}_{\bLambda_{\bomega}^{-1}}^2\,.
% % \]

% There is an explicit formula for the best response in BAI. Indeed if we inspect the proof of Proposition~\ref{prop:lgc.lb} we have
% \[
% \inf_{\lambda\in \neg \astar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2} = \min_{a \neq \astar(\theta)} \normm{\theta-\lambda^\star_a}_{\bLambda_{\bomega}^{-1}}\,.
% \]
% where $\lambda^\star_a$ is defined in Lemma~\ref{lem:best_response_BAI}.

% \begin{lemma}
% \label{lem:best_response_BAI}
% For $\theta \in\R^d\,$, $w$ in the interior of the probability simplex $\interior{\Sigma_A}$, we have
% \[
% \min_{\substack{\langle \lambda,a-\astar(\theta)\rangle\geq 0}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% \]
% and $\lambda^\star_a$ defined below attains the infimum of the left hand term above
% \[
% \lambda^\star_a = \theta - \frac{\max\left(\langle \theta, \astar(\theta)-a\rangle,0\right)}{\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}} \bLambda_{\bomega}^{-1}(a^\star - a)\,.
% \]
% \end{lemma}
% \begin{proof}
% See proof of Lemma~\ref{lemma:lgc.lagrange_alternative}.
% \end{proof}

\paragraph{Bounded BAI.}\label{sec:lgc.formulation.examples.bounded}

One straightforward extension is to consider the \emph{bounded} BAI. In this case, the set of parameters is 
\[
    \Theta \eqdef \{\btheta \in \R^d:\ |\argmax_{\bx\in\cX} \bx\transpose\btheta|=1 \text{ and } \normm{\btheta}\leq M\}
\]
for some $M>0$. The set of possible answers is $\cI = \cX$ and the correct answer is given by 
\[
    \Istar(\theta) = \bx^\star(\theta) \eqdef \argmax_{\bx\in\cX} \bx\transpose\btheta\,.
\]

This additional assumption reduces the characteristic time to
\[
    \Tstar(\theta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\bx\neq \bx^\star(\btheta)} \inf_{\substack{(\bx-\bx^\star)\transpose\btheta'>0\\ \normm{\btheta'}\leq M}} \normm{\btheta -\btheta'}_{\bLambda_{\bomega}}^2 \,.
\]

% But the best response is less trivial to compute, in particular there is no closed formula for $\lambda^\star_a$ as in BAI, see Lemma~\ref{lem:lagrange_bounded_BAI}.
% \begin{lemma}
%   \label{lem:lagrange_bounded_BAI}
% For $\theta, \lambda \in \R^d\,$, $w$ in the interior of the probability simplex $\interior{\Sigma_A}$,
% \begin{align}\label{eq:bounded_bai}
% \min_{\substack{\langle \lambda,a-\astar(\theta)\rangle\geq 0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0} \frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar(\theta)-a)\rangle,0\right)^2 }{2\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}}- \frac{\gamma}{2}\left(\normm{\theta}^2-M^2\right)\,.
% \end{align}
% And if $\gamma$ attains the supremum in the right hand term of~\eqref{eq:bounded_bai}, then
% \[
% \lambda = \theta - \frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar(\theta)-a)\rangle,0\right)}{\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}} (\bLambda_{\bomega}+\gamma I_d)^{-1}(a^\star - a)\,,
% \]
% attains the minimum of the left hand term of~\eqref{eq:bounded_bai}.
% \end{lemma}
% \begin{proof}
% We set $\astar(\theta) = \astar$, and introduce the Lagrangian
% \[
%  \inf_{\substack{\langle \lambda,a-\astar \rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0, \alpha\geq 0} \inf_{\substack{\langle \lambda,a-\astar\rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 +\alpha \langle \theta, \astar-a\rangle + \frac{\gamma}{2}\left(\normm{\lambda}^2-M^2 \right)\,.
% \]
% The infimum above is attained for
% \[
% \lambda = \theta - \alpha (\bLambda_{\bomega} + \gamma I_d)^{-1}(\astar-a)\,.
% \]
% Thus the Lagrangian reduces to
% \[
% \inf_{\substack{\langle \lambda,a-\astar \rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0, \alpha\geq 0}
% -\frac{\alpha^2}{2} \normm{\astar-a}^2_{\bLambda_{\bomega}+\gamma I_d} + \alpha \langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1}\bLambda_{\bomega} (\astar-a)\rangle +\frac{\gamma}{2}\left(\normm{\theta}^2-M^2 \right)\,.
% \]
% The supremum in $\alpha$ is reached for
% \[
% \alpha =\frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar-a)\rangle,0\right)}{\normm{\astar-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}}\,.
% \]
% Using this particular $\alpha$ in the definition of $\lambda$ and in the Lagrangian allows us to conclude.
% \end{proof}

\paragraph{Transductive BAI.}\label{sec:lgc.formulation.examples.transductive}
Another very closely-related setting is the transductive BAI~\citep{fiez2019transductive} where the learner wants to find the best arm of a different set $\cY$ that the one they are allowed to pull. Precisely the set of parameters is 
\[
    \Theta \eqdef \R^d/\{\btheta\in\R^d: |\argmax_{\by\in\cY} \by\transpose\btheta|>1\}\,.
\]
The set of possible answers is $\cI = \cY$ and the correct answer is given by 
\[
    \Istar(\btheta) = \by^\star(\btheta) \eqdef \argmax_{\by\in\cY} \by\transpose\btheta\,.
\]

The characteristic time in this case is
\[
    \Tstar(\btheta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\by\neq \by^\star(\btheta)} \frac{(\by\transpose\btheta-(\by^\star)\transpose\btheta)^2}{2\normm{\by^\star - \by}^2_{\bLambda_{\bomega}^{-1}}}\,.
\]
Note that the dependency on the arm set $\cX$ here only appears through the matrix $\bLambda_{\bomega}$.


% \subsection{Threshold bandits}
% \label{app:threshold_bandits}
% In this example the goal is to identify the set of arms whose mean is above a threshold $\iota\in \R$ known by the agent. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\ \exists a\in \cX,\, \langle \theta,a\rangle = \iota\}$, the set of possible answers is $\cI = \cP(\cX)$, the power set of the set of arms and the correct answer is given by $\istar(\theta)=\{a\in\cX:\ \langle \theta,a\rangle \geq \iota\}$.
% We can also express in this example the characteristic time in a more explicit way.
% \begin{lemma}
% \label{lem:complexity_threshold_bandits}
% For all $\theta\in \cM$,
% \[
% \Tstar(\theta)^{-1} =  \max_{w\in\Sigma_A} \min_{a\in\cX} \frac{\big(\iota -\langle \theta,a\rangle\big)^2}{2 \normm{a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% \]
% and $\Tstar(\theta)= 2\cX\cX(\iota)$, where we define $\cX(\iota)\eqdef \{ |\iota- \langle\theta,a\rangle|^{-1} a:\ a\in\cX\}$ and
% \[
% \cX\cX(\iota) \eqdef  \min_{w\in\Sigma_A} \max_{a\in\cX(\iota)}\normm{a}_{\bLambda_{\bomega}^{-1}}^2\,.
% \]
% \end{lemma}
% \begin{proof}
% We proceed as the proof of Lemma~\ref{lem:complexity_bai}. We have, using Lemma~\ref{lem:lagrange_alternative},
% \begin{align*}
%   \Tstar(\theta)^{-1} &= \max_{w \in \Delta_A} \min_{a\in\cX} \inf_{\lambda:\ \text{sign}(\iota-\langle\lambda,a\rangle)\langle\lambda,a\rangle > \iota} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\\
%   &= \max_{w \in \Delta_A} \min_{a\in\cX}  \frac{\big(\iota -\langle \theta,a\rangle\big)^2}{2 \normm{a}_{\bLambda_{\bomega}^{-1}}^2}\,.
% \end{align*}
% \end{proof}
% Note that we recover in this example a weighted version of the G-complexity ($\gopt$-complexity) defined in Section~\ref{sec:lower_bound}. In particular if $\theta=0$ and $\iota=1$ then
% \[
% \Tstar(\theta) =2\gopt = 2d\,.
% \]
% That makes sense since in this case, one shall estimate \emph{uniformly} the mean of each arms.


% \subsubsection{Transductive threshold bandits}
% \label{app:transductive_threshold_bandits}
% We can generalize the previous example to any set of arms. Indeed if we fix a finite set of vector $\cY\in\R^d$ the goal is then to identify all the elements $b$ of this set such that $\langle \theta, b \rangle \geq \iota$ for a known threshold $\tau \in \R$. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\ \exists b\in \cY,\, \langle \theta,b\rangle = \iota\}$, the set of possible answers is $\cI = \cP(\cY)$ and the correct answer is given by
% $\istar(\theta)=\{b\in\cY:\ \langle \theta,b\rangle \geq \iota\}$. The characteristic time makes appear, unsurprisingly, in this case, the transductive optimal design \citep{yu2006active}.
% \begin{lemma} For all $\theta \in\cM$,
%   \label{lem:complexity_transductive_threshold_bandits}
%   \[
%   \Tstar(\theta)^{-1} =  \max_{w\in\Sigma_A} \min_{b\in\cY} \frac{\big(\iota -\langle \theta,b\rangle\big)^2}{2 \normm{b}_{\bLambda_{\bomega}^{-1}}^2}\,,
%   \]
%   and $\Tstar(\theta)= 2\cX\cY(\iota)$, where we defined $\cY(\iota)\eqdef \{ |\iota- \langle\theta,b\rangle|^{-1} b:\ b\in\cY\}$ and
%   \[
%   \cX\cY(\iota) \eqdef  \min_{w\in\Sigma_A} \max_{b\in\cY(\iota)}\normm{b}_{\bLambda_{\bomega}^{-1}}^2\,.
%   \]
% \end{lemma}
% \begin{proof}
%   Simple adaptation of the proof of Lemma~\ref{lem:complexity_threshold_bandits}.
% \end{proof}
% Again, in particular, if $\theta=0$ and $\tau=1$ we recover the complexity of the optimal transductive design
% \[
% \Tstar(\theta)^{-1} = 2 \cX\cY\,.
% \]


