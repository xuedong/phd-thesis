%!TEX root = ../Chapter3.tex
\section{Fixed-Confidence Optimality}\label{sec:lgc.complexity}

%As first stated by~\cite{soare2014linear}, we only consider a finite number of arms and we restrict to bandits  with Gaussian rewards and conjugate (Gaussian) priors in this work. 
Just remember that our primary goal is to propose a BAI strategy that outputs quickly and reliably a final guess of the best arm. Formally, given a risk level $\delta$, we want to show that
\[
  \PP{\bx_{J_{\tau_\delta}}\neq \bx^\star} \leq \delta,
\]
while minimizing the expected number of samples $\EE{\tau_\delta}$ that is required. To achieve this objective, we first investigate the lower bound of the sample complexity.

\subsection{Lower bound}\label{sec:lgc.complexity.lb}

In this section we extend the lower bound of \citet{garivier2016tracknstop}, to hold for \emph{pure exploration in finitely-armed linear bandit} problems.

\paragraph{Alternative.}
We first define the notion of \gls{alternative set} in a pure-exploration game. For any answer $i\in\cI$ we define the \emph{alternative set} of arm $i$, denoted by $\neg i$ the set of parameters where the answer $i$ is not correct, i.e.
\[
    \neg i \eqdef \{\btheta\in\Theta:\ i\neq\Istar(\btheta)\}\,.
\]

%We also define, for any $w\in(\R^+)^A$, the design matrix \[\bLambda_{\bomega} \eqdef \sum_{a\in \cA} w^a a a^\top\,.\] Further, we define $\normm{x}_V\eqdef \sqrt{x^\top Vx}$ for $x\in\R^d$ and a symmetric positive matrix $V\in\R^{d\times d}$. Note that it is a norm only if $V$ is positive definite. We also denote by $\Sigma_K$ the probability simplex of dimension $K-1$ for all $K\ge 2$.

\paragraph{Lower bound.} 
A general result on the (non-asymptotic) sample-complexity lower bound in the fixed-confidence regime~\citep{garivier2016tracknstop}, which we reviewed in Section~\ref{sec:mab.performance.sample}, states that for any $\delta$-correct strategy, we have
\[
    \EE{\tau_\delta} \geq T^\star(\btheta)\log(\frac{1}{3\delta}),
\]
for a given parameter $\btheta$ and a given confidence level $\delta$. And the characteristic time $T^\star(\btheta)$ is written as
\begin{align}\label{eq:lgc.characteristic2}
    \Tstar(\btheta)^{-1} \eqdef \max_{\bomega \in \Sigma_K} \inf_{\btheta'\in \neg\Istar(\btheta)} \frac{1}{2}\normm{\btheta - \btheta'}_{\bLambda_{\bomega}}^2\,,
\end{align}
and it can be further particularized into~\eqref{eq:lgc.characteristic1} defined in Proposition~\ref{prop:lgc.lb} for linear bandits.

\begin{proposition}\label{prop:lgc.lb}
\begin{leftbar}[propositionbar]
In the linear case, the quantity $T^\star(\btheta)$ is written as
\begin{align}\label{eq:lgc.characteristic1}
    T^\star(\btheta) \eqdef \inf_{\bomega\in\Sigma_K}\max_{\bx\neq \bx^\star} \frac{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}{(\bx\transpose\btheta-(\bx^\star)\transpose\btheta)^2}\,.
\end{align}
\end{leftbar}
\end{proposition}

\begin{remark}
\begin{leftbar}[remarkbar]
    As noted in the seminal work of \citet{chernoff1959}, the complexity $\Tstar(\btheta)^{-1}$ is the value of a fictitious zero-sum game between the agent choosing an optimal proportion of allocation of pulls $\bomega$ and a second player, the nature, that tries to fool the agent by choosing the most confusing alternative $\lambda$ leading to an incorrect answer.
\end{leftbar}
\end{remark}

%In fact using the Sion's minimax theorem we can rewrite the characteristic times in various ways which will be useful to prove the lower bound of Theorem~\ref{th:lb_genral}.

\begin{proof}
Using the alternative set of $\Istar(\btheta)$, and by~\eqref{eq:lgc.characteristic2}, we obtain
\begin{align*}
    T^\star(\btheta)^{-1} &= \max_{\bomega \in \Sigma_K} \inf_{\btheta'\in \neg\Istar(\btheta)} \frac{1}{2}\normm{\btheta - \btheta'}_{\bLambda_{\bomega}}^2\\
    &= \max_{\bomega\in\Sigma_K}\inf_{\btheta'\in\neg\Istar(\btheta)}\sum_{i=1}^K \omega_i d(\mu_i;\mu_i')\\
    &= \max_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\inf_{\bx\transpose\btheta'>(\bx^\star)\transpose\btheta'} \frac{\normm{\btheta-\btheta'}_{\bLambda_{\bomega}}^2}{2}.
\end{align*}
Then we introduce the Lagrangian with $\eta$ as the Lagrange multiplier, and it then becomes
\begin{align*}
    T^\star(\btheta)^{-1} &= \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\inf_{\btheta'}\sup_{\eta>0} \frac{\normm{\btheta-\btheta'}_{\bLambda_{\bomega}}^2}{2} - \eta(\bx-\bx^\star)\transpose\btheta',
\end{align*}
and the inner expression attains its minimum when it comes
\[
    \bLambda_{\bomega}(\btheta-\btheta') = \eta(\bx^\star-\bx),
\]
which implies
\begin{align*}
    T^\star(\btheta)^{-1} &=
    \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\sup_{\eta>0} \eta(\bx^\star-\bx)\transpose\btheta - \frac{\eta^2\normm{\bx-\bx^\star}_{\bLambda_{\bomega}^{-1}}}{2\sigma^2}\\
    &= \sup_{\bomega\in\Sigma_K}\min_{\bx\neq\bx^\star}\frac{(\bx\transpose\btheta-(\bx^\star)\transpose\btheta)^2}{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}\,.
\end{align*}
\end{proof}

\paragraph{Asymptotic optimality.}
We can define the asymptotic optimality upon $T^\star_\beta(\btheta)$. 

\begin{definition}\label{def:lgc.optimality}
\begin{leftbar}[defnbar]
A BAI strategy is called optimal in the fixed-confidence setting if it satisfies
\[
    \limsup_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \leq T^\star(\btheta)\,.
\]
\end{leftbar}
\end{definition}

\begin{remark}
\begin{leftbar}[remarkbar]
Using the same lower-bound techniques, one can also prove that under any $\delta$-correct strategy satisfying $T_{n,I^\star}/n \rightarrow \beta$ for a given $\beta$,
\[
    \liminf_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \geq T^\star_\beta(\btheta)\,,
\]
where $T^\star_\beta(\btheta)$ is defined in the same way as $T^\star(\btheta)$, but restricted to the constraint $\omega_{I^\star}=\beta$,
\[
    T^\star_{\beta}(\btheta) \eqdef \inf_{\bomega\in\Sigma_K,\omega_{I^\star}=\beta}\max_{\bx\neq \bx^\star} \frac{2\sigma^2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}{(\bx\transpose\btheta-(\bx^\star)\transpose\btheta)^2}\,.
\]

% And a relaxed notion of optimality that depends on $\beta$ can be also defined. A BAI strategy is called \emph{$\beta$-optimal} if it satisfies 
% \[
%     \frac{T_{n,I^\star}}{n}\rightarrow \beta \quad \text{and} \quad \limsup_{\delta \rightarrow 0}\frac{\EE{\tau_\delta}}{\ln(1/\delta)} \leq T^\star_\beta(\bmu).
% \]

Essentially, we can recover the $\beta$-optimality defined in Chapter~\ref{CHAP:T3C}.
\end{leftbar}
\end{remark}

\subsection{Best-arm identification complexity}\label{sec:lgc.complexity.complexity}

The inverse of the characteristic time of Proposition~\ref{prop:lgc.lb} can also be written as
\[
    \Tstar(\btheta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\bx\neq \bx^\star} \frac{(\bx\transpose\btheta-(\bx^\star)\transpose\btheta)^2}{2\normm{\bx^\star - \bx}^2_{\bLambda_{\bomega}^{-1}}}
\]
for BAI.

Since the characteristic time involves many problem dependent quantities that are unknown to the learner, previous work target loose problem-independent upper bounds on the characteristic time. \citet{soare2014linear} (see also \citealt{tao2018alba,fiez2019transductive}) introduce the G-complexity (denoted by $\gopt$) which coincides with the G-optimal design of experimental design theory (see \citealt{pukelsheim2006optimal}) and the $\xyopt$-complexity (denoted by $\xyopt$) inspired by the transductive experimental design theory  \citep{yu2006active},
\begin{align*}
    \gopt &=\min_{\bomega\in\Sigma_K} \max_{\bx\in\cX} \normm{\bx}_{\bLambda_{\bomega}^{-1}}^2\,,\\
    \xyopt &=\min_{\bomega\in\Sigma_K} \max_{\by\in\cY_{\texttt{dir}}} \normm{\by}_{\bLambda_{\bomega}^{-1}}^2\,,
\end{align*}
where $\cY_{\texttt{dir}}$ is the set of directions induced by $\cX$:
\[
\cY_{\texttt{dir}}\eqdef\{\bx-\bx':\ (\bx,\bx')\in\cX\times\cX\}\,.
\]

For the G-optimal complexity we seek for a proportion of pulls $\bomega$ that explores \emph{uniformly} the means of the arms, since the statistical uncertainty for estimating $\bx\transpose\btheta$ scales roughly with $\normm{\bx}_{\bLambda_{\bomega}^{-1}}$. In the $\xyopt$-complexity we try to estimate \emph{uniformly} all the \emph{directions} $\bx-\bx'$, while a potentially more plausible quantity to maximize would be the characteristic time itself. For the latter, we try to estimate all the \emph{directions} $\bx^\star - \bx$ scaled by the squared gaps $(\bx^\star - \bx)\transpose\btheta$. 

The fact that previous works maximize over loose upper bounds on the characteristic time is potentially the main reason that they cannot achieve optimality. We can see later (in Section~\ref{sec:lgc.game}) that directly maximizing the weighted gaps would indeed lead to an asymptotically optimal algorithm.

Note that the characteristic time can also be seen as a particular optimal transductive design. Indeed for 
\[
    \cY^\star \eqdef \left\{ \frac{\bx^\star(\btheta)- \bx}{\left|(\bx^\star(\btheta)- \bx)\transpose\btheta\right|}: \bx\in\cX/\big\{\bx^\star(\btheta)\big\}  \right\}\,,
\]
it holds
\[
    \Tstar(\btheta) = 2 \cX\cY^\star(\btheta) \eqdef 2 \min_{\bomega\in\Sigma_K} \max_{\by\in\cY^\star(\btheta)} \normm{\by}_{\bLambda_{\bomega}^{-1}}^2\,.
\]

Besides, we have the following ordering on the complexities
\begin{align}\label{eq:lgc.complexities}
    \Tstar(\btheta) \leq 2 \frac{\xyopt}{\DeltaMin(\btheta)^2}\leq 8 \frac{\gopt}{\DeltaMin(\btheta)^2} = \frac{8d}{\DeltaMin(\btheta)^2}\,,
\end{align}
where $\DeltaMin = \min_{\bx\neq\bx^\star(\btheta)}(\bx^\star(\btheta)- \bx)\transpose\btheta$ and the last equality follows from the Kiefer-Wolfowitz equivalence theorem~\citep{kiefer1959}. 

%Conversely the $\gopt$-complexity and the $\xyopt$-complexity are linked to an \emph{other} pure exploration problem, the thresholding bandits (see Appendix~\ref{app:threshold_bandits}).

\begin{remark}
In order to compute all these complexities, it is sufficient to solve the following generic optimal transductive design problem: for $\cY$ a finite set of elements in $\R^d$,
\[
\xyopt=\min_{\bomega\in\Sigma_K}\max_{\by\in\cY}\normm{\by}^2_{\bLambda_{\bomega}^{-1}}\,.
\]
When $\cY=\cX$ we can use an algorithm inspired by Frank-Wolfe \citep{frank1956algorithm} which possesses convergence guarantees~\citep{atwood1969optimal,ahipasaoglu2008fw}. But in the general case, up to our knowledge, there is no algorithm with the same kind of guarantees. Previous works used an heuristic based on a straightforward adaptation of the aforementioned algorithm for general sets $\cY$ but it seems to not converge on particular instances, see Section~\ref{sec:lgc.experiments.implem}. We instead propose in the same section an algorithm based on Saddle point Frank-Wolfe algorithm that seems to converge on the different instances we tested.
\end{remark}

% TODO:
% \begin{itemize}
%   \item compare the three complexity on the problem $\theta = [1,0], a_1 = [1,0 ], a_2 =[1,\epsilon], a_3 = [0,1]$
%   \item compare also on the same problem
%   \[
%   \inf_{\lambda\in \neg \istar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\,,
%   \]
%   for $w = w^\star(\theta), w^G, w^{\cX\cY}$.
% \end{itemize}

\subsection{Some extensions}\label{sec:lgc.complexity.examples}

In this section, we present two relevant extensions of linear bandits BAI. We introduce the settings from a pure-exploration point of view.

% \paragraph{Best response for BAI.}\label{sec:lgc.formulation.examples.general}
% For BAI the goal is to identify the arm with the largest mean. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\  |\argmax_{a\in\cA} \langle\theta,a\rangle|>1\}$, the set of possible answers is $\cI = \cA$ and the correct answer is given by $\istar(\theta)=\astar(\theta)\eqdef \argmax_{a\in\cA} \langle\theta,a\rangle$.

% % \begin{lemma}
% % \label{lem:complexity_bai}
% % For all $\theta\in \cM$,
% % \[
% % \Tstar(\theta)^{-1} = \max_{w\in\Sigma_A} \min_{a\neq \astar(\theta)} \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% % \]
% % and
% % \[
% % \Tstar(\theta) = \min_{w\in\Sigma_A} \max_{a\neq \astar(\theta)} \frac{2\normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}{\big\langle \theta, \astar(\theta)-a\big\rangle^2}\,.
% % \]
% % \end{lemma}
% % \begin{proof}
% % Recall that the characteristic time is given by
% % \[
% % \Tstar(\theta)^{-1} = \max_{w \in \Delta_A} \inf_{\lambda\in \neg \astar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\,.
% % \]
% % We just express the set $\neg \astar(\theta)$ as a union of convex sets, and then compute the infimum for each one of them. Using Lemma~\ref{lem:lagrange_alternative}, it yields
% % \begin{align*}
% %   \Tstar(\theta)^{-1} &= \max_{w \in \Delta_A} \min_{a \neq \astar(\theta)} \inf_{\lambda: \langle\lambda,a\rangle > \langle\lambda,\astar(\theta)\rangle} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\\
% %   &= \max_{w \in \Delta_A} \min_{a \neq \astar(\theta)} \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,.
% % \end{align*}
% % The formula for $\Tstar(\theta)$ is then straightforward given the one for  $\Tstar(\theta)^{-1}$.

% % \end{proof}

% % In fact the characteristic time is just a particular case of the optimal transductive design. Indeed if we set
% % \[
% % \cBstar(\theta) \eqdef \left\{ \frac{1}{\left|\big\langle \theta, \astar(\theta)-a\big\rangle\right|}\big(\astar(\theta)- a\big): a\in\cA/\big\{\astar(\theta)\big\}  \right\}\,,
% % \]
% % then we have $\Tstar(\theta) = 2 \cA\cBstar(\theta)$ where
% % \[
% % \cA\cBstar(\theta) \eqdef  \min_{w\in\Sigma_A} \max_{b\in \cBstar} \normm{b}_{\bLambda_{\bomega}^{-1}}^2\,.
% % \]

% There is an explicit formula for the best response in BAI. Indeed if we inspect the proof of Proposition~\ref{prop:lgc.lb} we have
% \[
% \inf_{\lambda\in \neg \astar(\theta)} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2} = \min_{a \neq \astar(\theta)} \normm{\theta-\lambda^\star_a}_{\bLambda_{\bomega}^{-1}}\,.
% \]
% where $\lambda^\star_a$ is defined in Lemma~\ref{lem:best_response_BAI}.

% \begin{lemma}
% \label{lem:best_response_BAI}
% For $\theta \in\R^d\,$, $w$ in the interior of the probability simplex $\interior{\Sigma_A}$, we have
% \[
% \min_{\substack{\langle \lambda,a-\astar(\theta)\rangle\geq 0}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \frac{\big\langle \theta, \astar(\theta)-a\big\rangle^2}{2 \normm{\astar(\theta)-a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% \]
% and $\lambda^\star_a$ defined below attains the infimum of the left hand term above
% \[
% \lambda^\star_a = \theta - \frac{\max\left(\langle \theta, \astar(\theta)-a\rangle,0\right)}{\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}} \bLambda_{\bomega}^{-1}(a^\star - a)\,.
% \]
% \end{lemma}
% \begin{proof}
% See proof of Lemma~\ref{lemma:lgc.lagrange_alternative}.
% \end{proof}

\paragraph{Bounded BAI.}\label{sec:lgc.formulation.examples.bounded}

One straightforward extension is to consider the \emph{bounded} BAI. In this case, the set of parameters is 
\[
    \Theta \eqdef \{\btheta \in \R^d:\ |\argmax_{\bx\in\cX} \bx\transpose\btheta|=1 \text{ and } \normm{\btheta}\leq M\}
\]
for some $M>0$. The set of possible answers is $\cI = \cX$ and the correct answer is given by 
\[
    \Istar(\theta) = \bx^\star(\theta) \eqdef \argmax_{\bx\in\cX} \bx\transpose\btheta\,.
\]

This additional assumption reduces the characteristic time to
\[
    \Tstar(\theta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\bx\neq \bx^\star(\btheta)} \inf_{\substack{(\bx-\bx^\star)\transpose\btheta'>0\\ \normm{\btheta'}\leq M}} \normm{\btheta -\btheta'}_{\bLambda_{\bomega}}^2 \,.
\]

% But the best response is less trivial to compute, in particular there is no closed formula for $\lambda^\star_a$ as in BAI, see Lemma~\ref{lem:lagrange_bounded_BAI}.
% \begin{lemma}
%   \label{lem:lagrange_bounded_BAI}
% For $\theta, \lambda \in \R^d\,$, $w$ in the interior of the probability simplex $\interior{\Sigma_A}$,
% \begin{align}\label{eq:bounded_bai}
% \min_{\substack{\langle \lambda,a-\astar(\theta)\rangle\geq 0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0} \frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar(\theta)-a)\rangle,0\right)^2 }{2\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}}- \frac{\gamma}{2}\left(\normm{\theta}^2-M^2\right)\,.
% \end{align}
% And if $\gamma$ attains the supremum in the right hand term of~\eqref{eq:bounded_bai}, then
% \[
% \lambda = \theta - \frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar(\theta)-a)\rangle,0\right)}{\normm{\astar(\theta)-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}} (\bLambda_{\bomega}+\gamma I_d)^{-1}(a^\star - a)\,,
% \]
% attains the minimum of the left hand term of~\eqref{eq:bounded_bai}.
% \end{lemma}
% \begin{proof}
% We set $\astar(\theta) = \astar$, and introduce the Lagrangian
% \[
%  \inf_{\substack{\langle \lambda,a-\astar \rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0, \alpha\geq 0} \inf_{\substack{\langle \lambda,a-\astar\rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 +\alpha \langle \theta, \astar-a\rangle + \frac{\gamma}{2}\left(\normm{\lambda}^2-M^2 \right)\,.
% \]
% The infimum above is attained for
% \[
% \lambda = \theta - \alpha (\bLambda_{\bomega} + \gamma I_d)^{-1}(\astar-a)\,.
% \]
% Thus the Lagrangian reduces to
% \[
% \inf_{\substack{\langle \lambda,a-\astar \rangle>0\\ \normm{\lambda}\leq M}} \normm{\theta -\lambda }_{\bLambda_{\bomega}}^2 = \sup_{\gamma\geq 0, \alpha\geq 0}
% -\frac{\alpha^2}{2} \normm{\astar-a}^2_{\bLambda_{\bomega}+\gamma I_d} + \alpha \langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1}\bLambda_{\bomega} (\astar-a)\rangle +\frac{\gamma}{2}\left(\normm{\theta}^2-M^2 \right)\,.
% \]
% The supremum in $\alpha$ is reached for
% \[
% \alpha =\frac{\max\left(\langle \theta, (\bLambda_{\bomega}+\gamma I_d)^{-1} \bLambda_{\bomega} (\astar-a)\rangle,0\right)}{\normm{\astar-a}^2_{(\bLambda_{\bomega}+\gamma I_d)^{-1}}}\,.
% \]
% Using this particular $\alpha$ in the definition of $\lambda$ and in the Lagrangian allows us to conclude.
% \end{proof}

\paragraph{Transductive BAI.}\label{sec:lgc.formulation.examples.transductive}
Another very closely-related setting is the transductive BAI~\citep{fiez2019transductive} where the learner wants to find the best arm of a different set $\cY$ that the one they are allowed to pull. Precisely the set of parameters is 
\[
    \Theta \eqdef \R^d/\{\btheta\in\R^d: |\argmax_{\by\in\cY} \by\transpose\btheta|>1\}\,.
\]
The set of possible answers is $\cI = \cY$ and the correct answer is given by 
\[
    \Istar(\btheta) = \by^\star(\btheta) \eqdef \argmax_{\by\in\cY} \by\transpose\btheta\,.
\]

The characteristic time in this case is
\[
    \Tstar(\btheta)^{-1} = \max_{\bomega\in\Sigma_K} \min_{\by\neq \by^\star(\btheta)} \frac{(\by\transpose\btheta-(\by^\star)\transpose\btheta)^2}{2\normm{\by^\star - \by}^2_{\bLambda_{\bomega}^{-1}}}\,.
\]
Note that the dependency on the arm set $\cA$ here only appears through the matrix $\bLambda_{\bomega}$.


% \subsection{Threshold bandits}
% \label{app:threshold_bandits}
% In this example the goal is to identify the set of arms whose mean is above a threshold $\iota\in \R$ known by the agent. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\ \exists a\in \cA,\, \langle \theta,a\rangle = \iota\}$, the set of possible answers is $\cI = \cP(\cA)$, the power set of the set of arms and the correct answer is given by $\istar(\theta)=\{a\in\cA:\ \langle \theta,a\rangle \geq \iota\}$.
% We can also express in this example the characteristic time in a more explicit way.
% \begin{lemma}
% \label{lem:complexity_threshold_bandits}
% For all $\theta\in \cM$,
% \[
% \Tstar(\theta)^{-1} =  \max_{w\in\Sigma_A} \min_{a\in\cA} \frac{\big(\iota -\langle \theta,a\rangle\big)^2}{2 \normm{a}_{\bLambda_{\bomega}^{-1}}^2}\,,
% \]
% and $\Tstar(\theta)= 2\cA\cA(\iota)$, where we define $\cA(\iota)\eqdef \{ |\iota- \langle\theta,a\rangle|^{-1} a:\ a\in\cA\}$ and
% \[
% \cA\cA(\iota) \eqdef  \min_{w\in\Sigma_A} \max_{a\in\cA(\iota)}\normm{a}_{\bLambda_{\bomega}^{-1}}^2\,.
% \]
% \end{lemma}
% \begin{proof}
% We proceed as the proof of Lemma~\ref{lem:complexity_bai}. We have, using Lemma~\ref{lem:lagrange_alternative},
% \begin{align*}
%   \Tstar(\theta)^{-1} &= \max_{w \in \Delta_A} \min_{a\in\cA} \inf_{\lambda:\ \text{sign}(\iota-\langle\lambda,a\rangle)\langle\lambda,a\rangle > \iota} \frac{\normm{\theta - \lambda}_{\bLambda_{\bomega}}^2}{2}\\
%   &= \max_{w \in \Delta_A} \min_{a\in\cA}  \frac{\big(\iota -\langle \theta,a\rangle\big)^2}{2 \normm{a}_{\bLambda_{\bomega}^{-1}}^2}\,.
% \end{align*}
% \end{proof}
% Note that we recover in this example a weighted version of the G-complexity ($\gopt$-complexity) defined in Section~\ref{sec:lower_bound}. In particular if $\theta=0$ and $\iota=1$ then
% \[
% \Tstar(\theta) =2\gopt = 2d\,.
% \]
% That makes sense since in this case, one shall estimate \emph{uniformly} the mean of each arms.


% \subsubsection{Transductive threshold bandits}
% \label{app:transductive_threshold_bandits}
% We can generalize the previous example to any set of arms. Indeed if we fix a finite set of vector $\cB\in\R^d$ the goal is then to identify all the elements $b$ of this set such that $\langle \theta, b \rangle \geq \iota$ for a known threshold $\tau \in \R$. Thus, the set of parameters is $\cM=\cR^d/\{\theta\in\R^d:\ \exists b\in \cB,\, \langle \theta,b\rangle = \iota\}$, the set of possible answers is $\cI = \cP(\cB)$ and the correct answer is given by
% $\istar(\theta)=\{b\in\cB:\ \langle \theta,b\rangle \geq \iota\}$. The characteristic time makes appear, unsurprisingly, in this case, the transductive optimal design \citep{yu2006active}.
% \begin{lemma} For all $\theta \in\cM$,
%   \label{lem:complexity_transductive_threshold_bandits}
%   \[
%   \Tstar(\theta)^{-1} =  \max_{w\in\Sigma_A} \min_{b\in\cB} \frac{\big(\iota -\langle \theta,b\rangle\big)^2}{2 \normm{b}_{\bLambda_{\bomega}^{-1}}^2}\,,
%   \]
%   and $\Tstar(\theta)= 2\cA\cB(\iota)$, where we defined $\cB(\iota)\eqdef \{ |\iota- \langle\theta,b\rangle|^{-1} b:\ b\in\cB\}$ and
%   \[
%   \cA\cB(\iota) \eqdef  \min_{w\in\Sigma_A} \max_{b\in\cB(\iota)}\normm{b}_{\bLambda_{\bomega}^{-1}}^2\,.
%   \]
% \end{lemma}
% \begin{proof}
%   Simple adaptation of the proof of Lemma~\ref{lem:complexity_threshold_bandits}.
% \end{proof}
% Again, in particular, if $\theta=0$ and $\tau=1$ we recover the complexity of the optimal transductive design
% \[
% \Tstar(\theta)^{-1} = 2 \cA\cB\,.
% \]


