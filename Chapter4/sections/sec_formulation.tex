%!TEX root = ../Chapter4.tex
\section{Problem Setting and Assumptions}\label{sec:lgc.formulation}

In this section, we recall the problem setting of linear bandits as well as linear bandits BAI. In particular, we specify the assumptions used in this chapter.

\paragraph{Linear bandits.}
We consider a finitely-armed linear bandit problem, where the collection of arms $\cX\eqdef\{\bx_1,\cdots,\bx_K\}\subset \R^d$ is given with $|\cX|=K$, and spans $\R^d$. When there is no ambiguity, we can also use the index $i\in[K]$ to represent arm $\bx_i$. The (unknown) mean of each arm is given by
\[
    \mu_i = \bx_i\transpose\btheta\,.
\]

\begin{assumption}\label{ass:lgc.bounded_arm}
\begin{leftbar}[assumptionbar]
    We assume that $\exists L>0$, $\forall \bx\in\cX, \normm{\bx}\leq L$, where $\normm{\bx}$ denotes the Euclidean norm of the vector $\bx$.
\end{leftbar}
\end{assumption}

The learning protocol (see also Definition~\ref{def:mab.linear_bai}) goes as follows: for each round $n$, the learner chooses an arm $\hbx_n\in\cX$ and observes a noisy sample
\[
    r_n = \hbx_n\transpose\btheta +\epsilon_n\,,
\]
where $\epsilon_n$ is the noise and $\btheta$ is the true regression parameter (unknown to the learner).
\begin{assumption}\label{ass:lgc.noise}
\begin{leftbar}[assumptionbar]
    We assume that $\epsilon_n \sim \cN(0,\sigma^2)$ is conditionally independent from the past\,.
\end{leftbar}
\end{assumption}
For the sake of simplicity, we set $\sigma^2 = 1$ in the rest of this paper.

\paragraph{Best-arm identification for linear bandits.}
We assume that $\btheta$ belongs to some parameter set $\Theta\subset\R^d$ known to the learner. Recall that in a pure exploration game, given a parameter $\btheta$, the learner aims to find the correct answer $\Istar(\btheta)\in\cI$ by interacting with the finite-armed linear bandit environment parameterized by $\btheta$ (see also Section~\ref{sec:mab.extensions.pure}).

In particular, we are interested in BAI for which the objective is to identify the arm with the largest mean. That is, the correct answer given $\btheta$ is given by 
\[
\Istar(\btheta) = \bx^\star(\btheta) \eqdef \argmax_{\bx\in\cX} \bx\transpose\btheta
\]
for $\btheta\in\Theta = \R^d$ and the set of possible correct answers is $\cI = \cX$. When clear from the context, we can simply denote $\bx^\star(\btheta)$ by $\bx$.

\paragraph{Algorithm.}
Let $\cF_{n}=\sigma (\hbx_1,r_1,\cdots, \hbx_n,r_n)$ be the information available to the learner after $n$ round. We restate the definition of a BAI algorithm under the fixed-confidence setting, which is given by three components: (1) a \emph{sampling rule} $(\hbx_n)_{n\geq 1}$, where $\hbx_n\in\cX$ is $\cF_{n-1}$-measurable, (2) a \emph{stopping rule} $\tau_\delta$, a stopping time for the filtration $(\cF_n)_{n\geq 1}$, and (3) a \emph{decision rule} $J_\tau\in \cX$ which is $\cF_{\tau_\delta}$-measurable.
%Non-deterministic algorithms could also be considered by allowing the rules to depend on additional internal randomization. The algorithms we present are deterministic.

\paragraph{$\delta$-correctness and fixed-confidence objective.}
As already stated several times in the previous chapters, we say that an algorithm is $\delta$-correct if it predicts the correct best arm with probability at least $1-\delta$, precisely if $\PPt{\big(\bx_{J_\tau} \neq \Istar(\btheta)\big) \leq \delta}$ and $\tau_\delta < +\infty$ almost surely for all $\btheta \in\Theta$. Our goal is to find a $\delta$-correct algorithm that minimizes the \emph{sample complexity}, that is,  $\E_\btheta[\tau_\delta]$ the expected number of sample needed to predict an answer.

\paragraph{Linear estimator.}
A crucial step in linear bandits is to estimate the regression parameter $\btheta$. Let $\bX_n=(\hat{\bx}_1,\ldots,\hat{\bx}_n)$ be a sequence of sampled arms, and $\br_n=(r_1,\ldots,r_n)$ be the corresponding observations. To estimate $\btheta^\star$ based on the adaptive sequence of observations $\br_n$, one may use the \gls{regularized least-square estimation}
\begin{align}\label{eq:update_mean}
    \hat{\btheta}_n^{\lambda} = (\lambda \1_d + \bA_{\bX_n})^{-1}\bb_{\bX_n}\,,
\end{align}
where $\bA_{\bX_n}$ and $\bb_{\bX_n}$ are the design matrix and the response vector respectively given by
\[
    \bA_{\bX_n} \eqdef \sum_{t=1}^n \hat{\bx}_t\hat{\bx}_t\transpose, \quad \bb_{\bX_n} \eqdef \sum_{t=1}^n \hat{\bx}_t r_t\,,
\]
and $\lambda\in\R$ is the regularization parameter. When clear from the context, we can simply denote $\bA_{\bX_n}$ by $\bA_n$ and $\bb_{\bX_n}$ by $\bb_n$.

\paragraph{Useful notation.}
The fixed-confidence optimality, as proved by~\cite{garivier2016tracknstop,russo2016ttts}, is related to the \emph{proportion vector} of pulls of each arm that we denote by $\bomega = (\omega_1,\ldots,\omega_K)$, where $\bomega\in\Sigma_K$. Given a vector of proportions $\bomega$, we can define a counterpart of the design matrix 
\[
    \bLambda_{\bomega} \eqdef \sum_{i=1}^K \omega_i\bx_i\bx_i\transpose.
\]

It is easy to switch between the design matrix and the proportion vector. Indeed, given a sequence of sampled arms $\bX_n$, the corresponding proportion vector can be written as
\[
    \forall i\in [K], \quad \omega_{n+1,i} = \frac{T_{n+1,i}}{n},
\]
where recall that $T_{n,i} \eqdef \sum_{t=1}^{n-1} \1\{\hat{\bx_t} = i\}$ is the number of pulls of arm $i$ before round $n$. Therefore, the corresponding design matrix can be written as $\bA_{\bX_n}=n\bLambda_{\bomega_{\bX_n}}$.

Another important notation that we employ ceaselessly is the Mahalanobis norm which is defined, given a positive semi-definite matrix $\bA\in\R^{d\times d}$, by
\[
    \forall \bx\in\R^d, \quad \normm{\bx}_{\bA} = \sqrt{\bx\transpose\bA\bx}.
\]
