%!TEX root = ../Chapter4.tex
\section{Introduction}\label{sec:lgc.intro}

%Multi-armed bandits (MAB) probe fundamental \emph{exploration-exploitation} trade-offs in sequential decision learning. We study the pure exploration framework, from among different MAB models, which is subject to the maximization of information gain after an exploration phase. 

Following the previous chapter, we study a natural extension of the vanilla BAI problem in this chapter, namely linear bandits BAI. As already stated in Chapter~\ref{CHAP:MAB}, linear bandits BAI studies the case where noisy linear payoffs depending on some \emph{unknown} regression parameter $\btheta$ are assumed.

Bandits with linear payoffs (or more generally with contextual payoffs) are of great interest in many real-world applications. Typically, we can think of advertisement display optimization where an e-content provider seeks to identify the best-performing advertisement display design. Other relevant applications include recommender systems, path routing, power grid cost minimization, etc. It is arguable whether we need regret minimization or best-arm identification for those situations: a reasonable guess is that it is often subject to the real business needs.

Linear bandits were first investigated by~\citet{auer2002linear} in the stochastic setting for regret minimization and later considered for BAI problems in the fixed-confidence setting by~\citet{soare2014linear}. In this chapter, we again focus on the fixed-confidence setting (see Definition~\ref{def:mab.bai_confidence}). A quick reminder that for fixed-confidence BAI, we search for algorithms that are able to output the correct best arm with high confidence using as few samples as possible.
%Inspired by~\citet{degenne2019game}, we treat the problem as a \emph{two-player zero-sum} game between the agent and the nature (in a sense described in Section~\ref{sec:lgc.lower_bound}), 

%In this chapter, we consider a general pure-exploration setting (see Appendix~\ref{app:lgc.examples} for details). Nevertheless, for the sake of simplicity, in the main text we primarily focus on BAI.  For stochastic bandits, BAI has been studied within two major theoretical frameworks. The first one,  \emph{fixed-budget} BAI, aims at minimizing the probability of misidentifying the optimal arm within a given number of pulls~\citep{audibert2010budget}. In this work, we consider another setting, \emph{fixed-confidence} BAI, introduced by~\citet{even-dar2003confidence}. Its goal is to ensure that the algorithm returns a wrong arm with probability less than a given risk level, while using a small total number of samples before making the decision. %Note that these two frameworks are very different in general and do not share transferable regret bounds (see~\citealt{carpentier2016budget} for an additional discussion).
%Existing fixed-confidence algorithms mostly depend on the risk parameter $\delta \in (0,1)$ \todo{what does that mean?}.

Recall that vanilla fixed-confidence BAI problems are often treated by arm eliminations such as \SE~\citep{karnin2013sha}, or by confidence-based methods such as \UGapE~\citep{gabillon2012ugape}. Those algorithms have naturally been extended to the linear setting (we survey existing methods for linear bandits BAI in Section~\ref{sec:lgc.related_work}). Before the work presented in this chapter, BAI for linear bandits has been previously studied by~\cite{soare2014linear,tao2018alba,xu2018linear,zaki2019maxoverlap,fiez2019transductive,kazerouni2019glb}. They all consider the fixed-confidence setting.

Beside studying fixed-confidence sample complexity,~\citet{garivier2016tracknstop} and some subsequent works~\citep{qin2017ttei,shang2020t3c} investigate a general criterion of judging the optimality of a BAI sampling rule: Algorithms that achieve the minimal sample complexity when $\delta$ tends to zero are called asymptotically optimal as elaborated on in Chapter~\ref{CHAP:T3C}. Previous work do not seem to satisfy this (asymptotic) optimality rule.

Since then, \citet{menard2019lma} and~\citet{degenne2019game} further study the problem from a game theoretical point of view, and extend the asymptotic optimality to the general pure exploration for structured bandits. Note that a naive adaptation of the algorithm proposed by~\cite{degenne2019game} may not work smoothly in the linear setting. Algorithms that benefit better from the linear structure are needed.

The primary goal of this chapter is thus to investigate what is the key element that impacts the optimality of an algorithm and how to design (asymptotically) optimal algorithms for linear bandits BAI. A first set of candidates is the Bayesian algorithms presented in Chapter~\ref{CHAP:T3C}: can they be extended in order to achieve optimality in the linear case? Other potential options are considered as well, in particular methods that approach the lower bound step by step (e.g. inspired by~\citealt{garivier2016tracknstop} or by~\cite{degenne2019game}).

\paragraph{Contributions.}
\textbf{1)}
We provide new insights on the complexity of BAI for linear bandits. In particular, we relate the asymptotic complexity of the BAI problem and other measures of complexity inspired by optimal design theory, which were used in prior work.
\textbf{2)}
We propose extensions of the Bayesian algorithms studied in Chapter~\ref{CHAP:T3C} to the linear setting, and provide empirical evidence that they are not asymptotically optimal.
\textbf{3)}
We develop a saddle-point approach to the lower bound optimization problem, which also guides the design of an algorithm \LG{} for linear bandits BAI in the fixed-confidence regime. Its sample complexity is asymptotically optimal and its empirical performance is competitive with the best existing algorithms.

%--- new insight on the complexity
%
%--- presenting a saddle point of view
%
%--- insight leads into convexication
%
%--- main result + 2 algos +  2 theorems proving that they are asymptotically optimal
