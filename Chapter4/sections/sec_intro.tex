% !TEX root = ../Chapter4.tex
\section{Introduction}\label{sec:gpo.intro}

\textit{Global optimization} (GO)
has applications in several domains including hyper-parameter tuning \citep{jamieson2016hyperband, li2017hyperband,samothrakis2013}.  GO usually consists of a data-driven optimization process over an expensive-to-evaluate function. It is also known as \emph{black-box optimization} since the inner behavior of a function is often unknown.

In GO, we optimize an unknown and costly-to-evaluate function $f: \mathcal{X} \rightarrow \mathbb{R}$ based on~$n$ noisy evaluations, that can be sequentially selected. This setting is a generalization of \textit{multi-armed bandits}, where the arm space $\mathcal{X}$ is some measurable space (\citealt{bubeck2011pure}). Each \emph{arm} $x \in \mathcal{X}$ gets its mean reward $f(x)$ through the reward function $f$, which is  the function to be optimized. At each round $t$, the learner chooses an arm $x_t \in \mathcal{X}$ and receives a reward~$r_t$. We study the noisy setting in which the obtained reward is a noisy evaluation of $f$: $r_t \triangleq f(x_t) + \epsilon_t$, where $\epsilon_t$ is a bounded noise.

Treating the setting without any further assumption would be a \emph{mission impossible}. However, the setting gets easier if we assume a global smoothness of the reward function~\citep{agrawal1995continuum,kleinberg2004nearly,kleinberg2008multi,cope2009,auer2007improved,slivkins2011taxonomy,kleinberg2013}. A weaker condition is some \emph{local} smoothness where only neighborhoods around the maximum are required to be smooth.  In fact, local smoothness is sufficient for achieving near-optimality~\citep{valko2013stosoo,azar2014online,grill2015poo,bull2015adaptive}.
We base our work on optimistic tree-based optimization algorithms \citep{munos2011soo,valko2013stosoo,preux2014bandits,azar2014online} that approach the problem with a hierarchical partitioning of the arm space and take the \textit{optimistic principle}. This idea comes from \textit{planning} in Markov decision processes~\citep{kocsis2006bandit,munos2014,grill2016trail}.

Our work is motivated by the \textbf{p}arallel \textbf{o}ptimistic \textbf{o}ptimization (\POO) approach proposed by \citet{grill2015poo},  that \emph{adapts to the smoothness} without the knowledge of it. \POO is a \textit{meta-algorithm} which can be used on top of any hierarchical optimization algorithm that \emph{knows the smoothness}, that we call a subroutine. Not only does \POO require only the mildest local regularity conditions, but it also gets rid of the unnecessary metric assumption that is often required. Local smoothness naturally covers a larger class of functions than global smoothness, yet still assures that the function does not decrease too fast around the maximum. We highlight that the analysis of \POO is modular: Assuming the subroutine has a regret of order $R_n$ \emph{under a local smoothness assumption with respect to a fixed partitioning} (\citealt{grill2015poo}, Assumption~\ref{ass1}, formally introduced in Section~\ref{sec:gpo.pre}), \POO run with such subroutine has a regret bounded by $R_n \sqrt{\log n}$. \POO was originally analyzed using \HOO as a subroutine. However, unlike what \citet{grill2015poo} hypothesize, it is non-trivial to provide a regret bound for \HOO under Assumption~\ref{ass1}. We elaborate on that in Section~\ref{sec:gpo.hct}. In order to validate \POO,  there needs to exist a subroutine with a  regret guarantee that is provable under Assumption~\ref{ass1}. This is what we deliver.

In particular, we prove that \HCT-$\operatorname{iid}$\footnote{Denoted by \HCT in the rest of the paper since we do not consider the correlated feedback setting.} of~\cite{azar2014online} satisfies the required regret guarantee, and is, therefore, a desirable subroutine to be plugged in \POO. Similar to \HOO, \HCT is a hierarchical optimization algorithm based on confidence intervals. However, unlike \HOO, these confidence intervals are obtained by repeatedly sampling a representative point of each cell in the partitioning before splitting the cell. This yields partition trees that have a \emph{controlled depth}, which are easier to analyze under a local smoothness assumption with respect to the partitioning. Whether \HOO has similar regret guarantees under the desired local metricless assumption remains an open question.

\POO requires the subroutine to have a \emph{cumulative} regret guarantee. In this paper, we also provide a more general wrapper for algorithms that only have guarantee for their simple regret, called \GPO (for \textbf{g}eneral \textbf{p}arallel \textbf{o}ptimization). We show that with a cross-validation scheme instead of the original recommendation strategy, any hierarchical bandit algorithm with simple regret guarantee can be plugged into \GPO with only a tiny increase in the resulting simple regret.

\paragraph{Outline.}
We first formulate the sequential optimization problem and introduce some preliminary notions and assumptions in Section~\ref{sec:gpo.pre}. Our main result is presented in Section~\ref{sec:gpo.hct}, where we provide a regret upper bound for \HCT under local smoothness with respect to the partitioning. In Section~\ref{sec:gpo.pct}, we present the instantiation of \POO studied in the paper that we call \PCT, in which the underlying subroutine \HOO is replaced by \HCT. We show that \PCT enjoys the same regret bound as \HCT up to a $\sqrt{\log n}$ factor. The general wrapper and its simple regret analysis are presented in Section~\ref{sec:gpo.gpo}. We conclude by some numerical simulations in Section~\ref{sec:gpo.experiments}.

%\todomi{I  suggest carefully and consistently applying editorial comments of Michael Littman: \url{http://cs.brown.edu/~mlittman/etc/style.html}.For example, you are using  citations as nouns.}
