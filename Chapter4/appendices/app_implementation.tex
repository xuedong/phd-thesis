%!TEX root = ../lin_bandit_explo.tex
\section{Implementation Details}\label{app:lgc.implem}

In this section, we provide some further experiment details. We also share a few insights over different aspects of implementations of different pure exploration linear bandits algorithms. In particular, we propose a new Frank-Wolfe-typed heuristic to solve generic $\cA\cB$-design.

\subsection{Experimental setting}

\paragraph{More details for algorithm implementations.} We give more clarifications on each individual algorithm implemented.

\begin{itemize}
	\item For our algorithms \LG and \LGC, we implemented the version with the boundedness assumption.
	\item For \LGapE We implemented the greedy version, that is, pull the arm 
	\[
	    \argmin_{a\in\cA} \normm{a_{i_t}-a_{j_t}}_{(V_{N_t}+aa^\top)^{-1}}^2
	\]
	with $i_t = i^\star(\hat\theta_t)$ and $j_t = \argmax_{j\neq i_t}\langle\hat\theta_t,a_{j}-a^\star(\hat\theta_t)\rangle + \normm{a^\star(\hat\theta_t) - a_{j_t}}_{V_{N_t}^{-1}} \sqrt{2\beta(t,\delta)}$. Note that this version does not have a theoretical guarantee in the general case. However, as we stated in Section~\ref{sec:lgc.related_work}, the \GLUCB proposed by~\citet{zaki2019maxoverlap} is equivalent to this greedy version of \LGapE, and they provided an analysis for the 2-arm and 3-arm case. \LGapE is designed for $\epsilon$-best-arm identification, we set $\epsilon=0$ in our experiments to make sure that it outputs the optimal one.
	\item For \XYS, we implemented the greedy incremental version for both $\gopt$-allocation and $\xyopt$-allocation, that allows us to avoid the optimal design-computing step. To implement the non-greedy version, readers are invited to look at next Appendix~\ref{app:lgc.implem_complexity} where we discuss in detail the computation of $\cA\cB$-optimal design.
	\item For \XYA, it requires a hyper-parameter that characterizes the length of each phase. We set that hyper-parameter to $0.1$ as done by~\citet{soare2014linear}.
\end{itemize}

\paragraph{Technical details.} All the algorithms and experiments are implemented in \lstinline{Julia 1.3.1}, and plots are generated using the \lstinline{StatsPlots.jl} package. Other external dependencies are: \lstinline{JLD2.jl, Distributed.jl, IterTools.jl, CPUTime.jl, LaTeXStrings.jl}.

\paragraph{For reproducibility.} To rerun our code, your need to have Julia installed, then unzip \lstinline{code.zip} and do the following in your terminal.

\begin{lstlisting}
  $ cd PATH/TO/THE/FOLDER/code/linear
  $ julia
  julia> include("experiment_bai1.jl") # reproduce Fig.1
  julia> include("viz_bai1.jl") # visualization
  julia> include("experiment_bai2.jl") # reproduce Fig.2
  julia> include("viz_bai2.jl") # visualization
\end{lstlisting}

%\subsubsection{Arm generation.} In Section~\ref{sec:experiments}, we need to generate arms uniformlly from a unit sphere of arbitrary dimension. This can be done using Algorithm~\ref{alg:generation}, it can be trivially extended to higher dimension.
%
%\begin{algorithm}[ht]
%   \caption{Generating arms from the 2D unit sphere (Box-Mueller)}
%   \label{alg:generation}
%\begin{algorithmic}
%        \State $u \sim \cN(0,1)$
%        \State $v \sim \cN(0,1)$
%        \State $(x, y) = (u, v)/\normm{(x, y)}_2$
%        \RETURN $x, y$
%\end{algorithmic}
%\end{algorithm}

\subsection{Computation of different complexities}\label{app:lgc.implem_complexity}

As mentioned in Section~\ref{sec:lgc.lower_bound}, computing the solution to a specified optimization problem is required in many existing linear BAI algorithms. We survey some methods that can potentially be useful to handle that issue.

We recall that the three notions of complexity $\gopt, \xyopt, \cA\cBstar(\theta)$ can be written in a unified form,
\begin{align}\label{eq:complexity_general}
    \cA\cB = \min_{w\in\Sigma_A} \max_{b\in\cB}\normm{b}_{V_w^{-1}}^2,
\end{align}
where $\cB$ is the transductive set, i.e. a finite set of elements in $\R^d$. Transductive sets corresponding to different complexity types mentioned in this paper can be found in Table~\ref{tab:transductive_sets}.

\begin{table}[ht]
    \centering
	\begin{tabular}{@{}lll@{}}
		\toprule
		\thead{Allocation type} & \thead{Arm set} & \thead{Transductive set} \\ \midrule
		(1) $\gopt$-allocation & $\cA$ & $\cA$\\
		(2) $\xyopt$-allocation & $\cA$ & $\cB_{\texttt{dir}} = \{a-a':\ (a,a')\in\cA\times\cA\}$ \\
		(3) $\cA\cBstar(\theta)$-allocation & $\cA$ & $\cB^\star(\theta) = \left\{ (\astar(\theta)- a)/\left|\big\langle \theta, \astar(\theta)-a\big\rangle\right|: a\in\cA/\big\{\astar(\theta)\big\}  \right\}$ \\
		\bottomrule
	\end{tabular}
	\caption{Some examples of different transductive sets.}
	\label{tab:transductive_sets}
\end{table}

\paragraph{Frank-Wolfe.} We can use a Frank-Wolfe heuristic to compute the optimizer of~\eqref{eq:complexity_general} shown in Algorithm~\ref{alg:fw_aa}. This heuristic is used for example by~\citet{fiez2019transductive}. Note that this heuristic has been proved to have a linear convergence guarantee when $\cB = \cA$~\citep{ahipasaoglu2008fw}. It is not clear, however, that the same guarantee holds for other transductive sets.

A simple sanity check to test whether a solver works smoothly is  to solve $\cA\cBstar(\theta)$ for classical multi-armed bandits (i.e. when $\cA= \{e_1,e_2,\ldots,e_d\}$), for which a solver with guarantee exists (see \citealt{garivier2018explore}). In particular we found instances where Algorithm~\ref{alg:fw_aa} does not converge toward the optimal weights, for example: $\cA= \{e_1,e_2,e_3\}, \theta = (0.9, 0.5, 0.5)$.

% \begin{algorithm}[ht]
%    \caption{Frank-Wolfe heuristic for computing $\gopt$-design}
%    \label{alg:fw_aa}
% \begin{algorithmic}
%    \State {\bfseries Input:} arm set $\cA\subset\R^d$, transductive set $\cB\subset\R^d$, maximum iterations $n$
%    \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), V \leftarrow{} I_d, t \leftarrow{} 0$
%    \While{$t<n$}
%         \State $i_{t}\in\argmin_{i\in\{1,\ldots,A\}}\max_{b\in\cB}\normm{b}^2_{(V+a_i a_i^\top)^{-1}}$
%         \State $V \leftarrow{} V + a_{i_{t}}a_{i_{t}}^\top$
%         \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{i_{t}}$
%         \State $t \leftarrow{} t+1$
%    \EndWhile
%    \RETURN $w$
% \end{algorithmic}
% \end{algorithm}


\begin{algorithm}[ht]
\centering
\caption{Frank-Wolfe heuristic for computing $\gopt$-design}
\label{alg:fw_aa}
\begin{algorithmic}
   \State {\bfseries Input:} arm set $\cA\subset\R^d$, transductive set $\cB\subset\R^d$, maximum iterations $n$
   \State  {\bfseries Initialize:} $w \leftarrow{} (1, 1, \ldots, 1)\in\R^A, V \leftarrow{} I_d, t \leftarrow{} 0$
   \While{$t<n$}
        \State $\ta\in\argmin_{a\in\cA}\max_{b\in\cB}\langle a,b\rangle_{V^{-1}}^2$%\normm{b}^2_{(V+a_i a_i^\top)^{-1}}$
        \State $V \leftarrow{} V + \ta \ta^\top$
        \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{\ta}$
        \State $t \leftarrow{} t+1$
   \EndWhile
   \State {\bfseries Return} $w$
\end{algorithmic}
\end{algorithm}

We propose a variant of the previous heuristic that takes into account a count for each element in the transductive set $\cB$. The pseudo-code of our method is displayed in Algorithm~\ref{alg:fw_ab}. $N\in\NN^{|\cB|}$ denotes the vector of counts for all $b\in\cB$. Sanity check on various MAB instances shows the correctness of our heuristic, its convergence guarantee remains for the future work.

% \begin{algorithm}[ht]
%    \caption{Saddle Frank-Wolfe heuristic for computing generic $\cA\cB$-design}
%    \label{alg:fw_ab}
% \begin{algorithmic}
%    \State {\bfseries Input:} arm set $\cA\subset\R^d$, transductive set $\cB\subset\R^d$, maximum iterations $n$
%    \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), N \leftarrow{} (1, 1, \ldots, 1), V \leftarrow{} I_d, t \leftarrow{} 0$
%    \While{$t<n$}
%         \State $i_{t}\in\argmin_{i\in\{1,\ldots,A\}}\sum_{j=1}^B(-2N[j]\langle a_i,V b_j\rangle^2)$
%         \State $j_{t}\in\argmax_{j\in\{1,\ldots,B\}}\normm{b_{j}}^2_{V^{-1}}$
%         \State $V \leftarrow{} V + a_{i_{t}}a_{i_{t}}^\top$
%         \State $N[j_t] \leftarrow{} N[j_t] + 1$
%         \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{i_{t}}$
%         \State $t \leftarrow{} t+1$
%    \EndWhile
%    \RETURN $w$
% \end{algorithmic}
% \end{algorithm}



\begin{algorithm}[ht]
\centering
\caption{Saddle Frank-Wolfe heuristic for computing generic $\cA\cB$-design}
\label{alg:fw_ab}
\begin{algorithmic}
   \State {\bfseries Input:} arm set $\cA\subset\R^d$, transductive set $\cB\subset\R^d$, maximum iterations $n$
   \State  {\bfseries Initialize:} $w \leftarrow{} (1, 1, \ldots, 1)\in\R^A, \tV \leftarrow{} I_d, V \leftarrow{} I_d, t \leftarrow{} 0$
   \While{$t<n$}
        \State $\ta\in\argmax_{a\in\cA}  \norm{a}_{V^{-1} \tV V^{-1} }^2$
        \State $\tb\in\argmax_{b\in\cB}\normm{b}^2_{V^{-1}}$
        \State $V \leftarrow{} V + \ta\ta^\top$
		\State $\tV \leftarrow{} \tV + \tb\tb^\top$
        \State $w \leftarrow{} \frac{t}{t+1}w+\frac{1}{t+1}e_{\ta}$
        \State $t \leftarrow{} t+1$
   \EndWhile
   \State {\bfseries Return} $w$
\end{algorithmic}
\end{algorithm}

\paragraph{Entropic mirror descent.}
An entropic mirror descent alternative is used by~\citet{tao2018alba} to compute $\gopt$. The entropic mirror descent approach requires the knowldge of the Lipschitz constant of $\log\det V_w$. Unfortunately, that Lipschitzness property does not seem to hold. \citet{lu2018convex} propose a solution to overcome the Lipschitz issue, but only for $\gopt$-design. Whether it still works for general $\cA\cB$-design remains an open question.

%\begin{algorithm}[ht]
%   \caption{Entropic mirror descent heuristic for computing $\gopt$-design}
%   \label{alg:algoMD}
%\begin{algorithmic}
%   \State {\bfseries Input:} arm set $\cA\subset\R^d$, transductive set $\cB\subset\R^d$, tolerance constant $\epsilon$, Lipschitz constant $L$
%   \State  {\bfseries Initialize:} $w \leftarrow{} (1/A, 1/A, \ldots, 1/A), t \leftarrow{} 1$
%   \While{$|\max_{a\in\cA}a^\top V_w^{-1} a - d| \geq \epsilon$}
%        \State $\gamma \leftarrow{} \frac{2\sqrt{A}}{L} \frac{1}{\sqrt{t}}$
%        \State Compute gradient $\nabla^i \leftarrow{} \Tr(V_w^{-1}a_ia_i^\top)$
%        \State $w^{a_i} \leftarrow{} \frac{w^{a_i}\exp(\gamma \nabla^i)}{\sum_{i=1}^A w^{a_i}\exp(\gamma \nabla^i)}$
%        \State $t \leftarrow{} t+1$
%   \EndWhile
%   \RETURN $w$
%\end{algorithmic}
%\end{algorithm}
