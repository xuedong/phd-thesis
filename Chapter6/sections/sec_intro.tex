\section{Introduction}\label{sec:dttts.intro}

Training a machine learning algorithm often requires to specify several parameters. For instance, for neural networks, it is the architecture of the network and also the parameters of the gradient algorithm used or the choice of regularization. These \emph{hyper-parameters} are difficult to learn through the standard training process and are often manually specified.
%\todo{some ref so this is not just blabla}

When it is not feasible to design algorithms with a few hyper-parameters, we opt for hyper-parameter optimization. \gls{hpo} is a crucial component of modern machine learning and \gls{automl}. \gls{hpo} can be viewed as a \gls{bbo} problem where the evaluation of the objective function is expensive as it is the accuracy of a learning algorithm for a given configuration of hyper-parameters. Indeed, a typical function evaluation involves training the primary machine learning algorithm to completion on a dataset, which often takes a considerable amount of time or resources, in particular for large \gls{dl} models. For example, the training of language representation model \texttt{BERT-Large}~\citep{devlin2019bert} was performed on 16 Cloud TPUs (64 TPU chips in total), and each pre-training took 4 days to complete. This vastly limits the number of evaluations that can be carried out, which calls for a design of efficient high-level algorithms that automate the tuning procedure.

In this chapter, we are interested in exploring how \gls{mab}, or more precisely \gls{bai}, can guide the design of efficient \gls{hpo}. Indeed, some bandit tools have already been employed for \gls{go} (see Chapter~\ref{CHAP:GPO}) and \gls{hpo}: First, in the field of \gls{bo}, the \GPUCB algorithm \citep{srinivas2010gpucb} is a Gaussian process extension of the classical \UCB bandit algorithm~\citep{auer2002ucb}. Later, \citet{hoffman2014bayesgap} proposed to use \gls{bai} tools -- still with a Bayesian flavor -- for automated machine learning, where the goal is to smartly try hyper-parameters from a pre-specified \emph{finite} grid. 

However, in most cases, the number of hyper-parameter configurations to explore is infinite. In this chapter, we investigate the use of bandit tools suited for an \emph{infinite} number of arms. There are two lines of work for tackling a very large or infinite number of configurations (arms). The first is the continuum-armed bandits elaborated in Chapter~\ref{CHAP:GPO} (see also~\citealt{bubeck2010x,grill2015poo,shang2019adaptive,bartlett2019simple}). It makes use of hierarchical bandit tools as  and aims at exploiting the (possibly unknown) smoothness of the black-box function to optimize. To the best of our knowledge, these methods have never been investigated for \gls{hpo}. 

The second line of work does not assume any smoothness: At each round, the learner may ask for a new arm from a \gls{reservoir} distribution $\nu_0$ (pick randomly a new hyper-parameter configuration) and add it to the current arm pool $\cX$, or re-sample one of the previous arms (evaluate configuration already included in $\cX$), in order to find an arm with a good mean reward (i.e., a hyper-parameter configuration with a good validation accuracy). It is the \gls{infinitely-armed bandits} setting. In particular, we study the stochastic case where observations are assumed to be independent. The \gls{siab} is studied by \citet{berry1997infinite,wang2008ucbv} for the rewards maximization problem while \citet{carpentier2015siri,aziz2018confidence} study the simple regret problem, which is related to \gls{bai}. While most proposed algorithms consist of querying an \emph{adequate} number of arms from the reservoir before running a standard BAI algorithm, \cite{li2017hyperband} propose a more robust approach called \Hyperband{} that uses several such phases.

\paragraph{Contributions.}
\textbf{1)}
In this chapter, we go even further and propose the first \emph{dynamic} algorithm for BAI in SIAB, that at each round, may either query a new arm from the reservoir or re-sample arms previously queried. Our algorithm leverages a Bayesian model and builds on \TTTS{}. 
\textbf{2)}
We also introduce a variant of \Hyperband{} where the \SHA subroutine~\citep{karnin2013sha} is replaced by \TTTS{}. 
\textbf{3)}
Numerical studies are presented to show the competitiveness of our algorithm with respect to state-of-the-art \gls{hpo} methods.
%\todo{cut this in space needed}
%\paragraph{Outline} We first introduce a general framework for hyper-parameter optimization in Section~\ref{sec:framework}. In Section~\ref{sec:bai}, we explain how infinitely-many armed bandit algorithms can be used for \gls{hpo}, before presenting our two new algorithms in Section~\ref{sec:algo}. Finally, we provide experimental results in Section~\ref{sec:result}. % before concluding.
