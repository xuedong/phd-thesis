\section{Active Thompson Sampling for HPO}\label{sec:dttts.algorithm}

In this section, we introduce a new algorithm for BAI in an infinite bandit model, that is an adaptation of Thompson sampling (\citealp{thompson1933}). Thompson sampling can be seen as the very first bandit algorithm ever proposed, but has been used for the \emph{rewards maximization} objective, which is quite different from BAI, as explained by~\cite{bubeck2009pure}. Instead of using vanilla Thompson sampling, we build on \TTTS that is an adaptation of Thompson sampling for BAI in finite-arm bandits. Unlike the state-of-the-art algorithm \SHA that requires the knowledge of the total budget to operate, \TTTS is particularly appealing as it does not need to have it. Such algorithms are referred to as \emph{anytime}. Besides, it is known to be optimal in a Bayesian (asymptotic) sense, as it attains the best possible rate of decay of the posterior probability of the set of wrong models. 

As a Bayesian algorithm, \TTTS uses a prior distribution $\Pi_0$ over the vector of means of the $K$ arms, $\bm\mu \triangleq (\mu_1,\dots,\mu_K)$, which can be updated to a posterior distribution $\Pi_t$ after $t$ observations. Under the Bernoulli bandit model, arm $i$ produces a reward $Y_{t,i}=1$ with probability $\mu_i$, and $Y_{t,i}=0$ with probability $1-\mu_i$ when sampled at round $t$. Given independent uniform prior for the mean of each arm, the posterior distribution on $\bm\mu$ is a product of $K$ Beta distributions: $\Pi_t = \bigotimes_{i=1}^{K} \texttt{Beta}(1+S_{t,i},N_{t,i}-S_{t,i}+1)$, where $N_{t,i}$ is the number of selections of arm $i$ until round $t$ and $S_{t,i}$ is the sum of rewards obtained from that arm. At each round $t$, \TTTS chooses one arm from the following two candidates to evaluate: (1) it first samples a parameter ${\bm\theta}$ from $\Pi_{t-1}$, and the first candidate is defined as $I_t^{\,(1)} \eqdef \argmax_{i\in\cA} {\theta}_i$, (2) it repeatedly samples new ${\bm\theta}'$ until $I_t^{\,(2)} \eqdef \argmax_{i\in\cA} \theta_i'$ is different from $I_t^{\,(1)}$. \TTTS depends on a parameter $\beta \in (0,1)$. In particular, the algorithm selects $I_t = I_t^{\,(1)}$ with probability $\beta$ and $I_t = I_t^{\,(2)}$ with probability $1-\beta$.

\TTTS can also be used for bandit settings in which the rewards are bounded in $[0,1]$ by using a binarization trick first proposed by \cite{agrawal2012analysis}: When a reward $Y_{t,i} \in [0,1]$ is observed, the algorithm is updated with a fake reward  $Y'_{t,i} \sim \texttt{Ber}(Y_{t,i}) \in \{0,1\}$. \TTTS can thus be used for BAI for a \emph{finite} number of arms that with rewards in $[0,1]$. We now present a simple way of extending \TTTS{} to deal with an infinite number of arms, namely \gls{dttts}.

\paragraph{Dynamic \TTTS{}.}

%The rationale behind \DTTTS is the following. 

In an infinite bandit algorithm, at each round, we either query a new arm from the reservoir \emph{and sample it}, or re-sample a previous arm. In a Bayesian setting, we can also imagine that at each round, an arm is queried from the reservoir and added with a \textit{uniform prior} to the list of queried arms, \emph{regardless of whether it is sampled or not}. Then, at round $t$, \DTTTS consists in running \TTTS on these $t$ arms, out of which several are endowed with a uniform prior and have never been sampled. 

Leveraging the fact the the maximum of $k$ uniform distribution has a \texttt{Beta}($k,1$) distribution and that \TTTS only depends on the maxima of posterior samples, we give the following equivalent implementation for \DTTTS (Algorithm~\ref{alg:dttts}). Letting $\cL_t$ be the list of arms that have been queried  from the reservoir and sampled \textit{at least once} before round $t$, at round $t$ we run \TTTS on the set $\cA_t \triangleq \cL_t \cup \{\mu_0\}$ where $\mu_0$ is a pseudo-arm with posterior distribution \texttt{Beta}($t-k_t, 1$), where $k_t\triangleq|\cL_t|$.  

\begin{algorithm}[ht]
\centering
\caption{Sampling rule of Dynamic \DTTTS{}}
\label{alg:dttts}
\begin{algorithmic}[1] %[1] enables line numbers
    \State {\bfseries Input: } $\beta$; $B$ (total budget); $\nu_0$
    \State {\bfseries Initialization: } $\mu_1 \sim \nu_0$; $t \gets 0$; $\cA \gets \{\mu_0,\mu_1\}$; $m\gets1$; $S_0, N_0 \gets 0$; $S_1 \sim \texttt{Ber}(\mu_1)$, $N_1 \gets 1$\
    
    \While{$t < B$}
	    \State $\forall i=0,\dots,m$, $\theta_i \sim \texttt{Beta}(S_i+1,N_i-S_i+1)$; $U \sim \cU([0,1])$
	    \State $I^{\,(1)} \gets \argmax_{i=0,\dots,m} \theta_i$
	    \If{$U > \beta$}
	        \While{$I^{\,(2)}\neq I^{\,(1)}$} 
	            \State $\forall i=0,\dots,m, \theta_i' \sim \texttt{Beta}(S_i+1,N_i-S_i+1)$
	            \State $I^{\,(2)} \leftarrow \argmax_{i=0,\dots,m}\theta_i'$ 
	        \EndWhile
	        \State $I^{\,(1)} \gets I^{\,(2)}$
	    \EndIf
	    \If{$I^{\,(1)} \neq 0$}
	        \State $Y \leftarrow$ \text{Evaluate arm} $I^{\,(1)}$; $X \sim \texttt{Ber}(Y)$ 
	        \State $S_{I^{\,(1)}} \gets S_{I^{\,(1)}} + X$; $N_{I^{\,(1)}} \gets N_{I^{\,(1)}} + 1$; $S_0 \gets S_0 + 1$
	    \Else
            \State $\mu_{m+1}\sim \nu_0$; $\cA \gets \cA \cup \{\mu_{m+1}\}$; 
            \State $Y \leftarrow$ \text{Evaluate arm} $m+1$; $X \sim \texttt{Ber}(Y)$
           	\State $S_{m+1} \gets X$; $N_{m+1}
           	\gets 1$; $m \gets m + 1$
 	   \EndIf
  	   \State $t\gets t+1$
    \EndWhile
\end{algorithmic}
\end{algorithm}

It remains to decide how to recommend the arm as our best guess. %It is obviously not a good idea to output the arm with the best empirical means since some lately sampled arms may have very high empirical mean with no confidence. 
In this paper, we choose the most natural recommendation strategy for Bayesian algorithms that outputs the arm with the largest \emph{posterior probability of being optimal}. Letting $\Theta_i$ be the subset of the set $\Theta$ of possible mean vectors such that arm $i$ is optimal, $\Theta_i \eqdef \big\{ \btheta\in\Theta \,|\, \theta_i > \max_{j\neq i}\theta_j \big\}$, the posterior probability that arm $i$ is optimal after round $t$ is defined as $\Pi_{t}(\Theta_i)$. At any time $t$, we therefore recommend  arm $\hat{I}_t \eqdef \argmax_{i\in\cA} \Pi_{t}(\Theta_i)$.
