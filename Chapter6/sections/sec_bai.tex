\section{Best-Arm Identification for Hyper-Parameter Tuning}\label{sec:dttts.bai}

Hyper-parameter optimization can be modeled as a BAI game. Given a finite set of arms $\cX \eqdef \{1,\ldots,K\}$, when we select arm $i$, we get an independent observation from some unknown distribution $\nu_i$ with mean $\mu_i$. A BAI algorithm sequentially selects arms in order to identify the arm with the largest mean\footnote{Here we present BAI problems in a standard way for which we search for an arm with the largest mean. For \gls{hpo}, however, it is important to mention that we are searching for a hyper-parameter configuration that minimizes the validation error. One can easily see that it does not change the problem in principle.}, $I^\star \eqdef \argmax_{i\in\cA}\mu_i$. 

In the context of \gls{hpo}, each arm models the quality of a given hyper-parameter configuration $\bm\lambda$. When the arm is sampled, a noisy evaluation of $f(\bm\lambda)$ is received, which is the mean reward of that arm.

%Recall that a fixed-budget BAI (or pure-exploration) algorithm consists of a sequential arm selection strategy, indicating which arm $I_n$ is selected at round $n$, coupled with a decision that selects a candidate best arm ${I}^\star_n$ at round $n$. The goal is either to minimize the error probability $\bP(\mu_{{I}^\star_n}\neq\mu_{I^\star})$ \citep{audibert2010budget,karnin2013sha} or the \emph{simple regret}~\citep{bubeck2009pure,gabillon2012ugape}, defined as $r_t = \mu_{I^\star} - \mu_{{I}^\star_t}$, possibly after a total budget $B$, whose knowledge may be used by the algorithm. Note that the BAI problem can also be studied from a fixed-confidence point of view \citep{even-dar2003confidence}. 

As stated in Section~\ref{sec:dttts.intro}, standard \gls{bai} algorithms are not straightforwardly applicable to \gls{hpo} when the search space can be infinite and is often continuous. To handle such cases, we rather turn our attention to \gls{siab}. In this context, there is an infinite pool of arms, whose means are assumed to be drawn from some \gls{reservoir} distribution $\nu_0$. In such a model, a BAI algorithm maintains a list of arms that have been tried before. At each round it can either query a new arm from the reservoir, add it to the list and sample it, or sample an arm already in the list.

A natural way to perform BAI in an infinite-many armed bandit model consists of first querying a \emph{well-chosen} number of arms from the reservoir and then running a standard BAI algorithm on those arms~\citep{carpentier2015siri}. However this ideal number may rely on the difficulty of the learning task, which is hardly known in practice. The \Hyperband{} algorithm \citep{li2017hyperband} takes a step further and successively queries several batches of arms from the reservoir, including a decreasing number of arms in each batch, while increasing the budget dedicated to each of them. \SHA \citep{karnin2013sha}, a state-of-the-art fixed-budget \gls{bai} algorithm, is then run on each of these batches of arms. This approach seems more robust in that it trades off between \emph{the number of arms that is needed to capture a good arm} and \emph{how much measurement effort we should allocate to each of them}. However, a numerical study performed by \cite{aziz2018infinite} seems to reveal that an infinite bandit algorithm based on \SHA should always query the maximal number of arms from the reservoir\footnote{This reference is a preliminary draft that has been withdrawn due to technical issues in the proofs. Yet we believe the experimental section to be sound.}. 

In Table~\ref{table:hpo}, we summarize how to cast \gls{hpo} as a \gls{bai} problem with infinitely-many arms.

\begin{table}[ht]
\centering
\def\arraystretch{1.5}
\begin{tabular}[r]{|c|c|} \hline
\textbf{\texttt{BAI}} & \textbf{\texttt{HPO}}\\
\hline
query $\nu_0$ & \ pick a new configuration $\blambda$\.\ \\
\hline
\ sample an arm\. & train the classifier $g_{\blambda}$ \\
\hline
reward & cross-validation loss \\
\hline
\end{tabular}
\caption{Casting HPO as a BAI problem.}
\label{table:hpo}
\end{table}


All existing algorithms are still subject to a pre-defined scheduling of how many arms should be queried from the reservoir. The algorithm (\DTTTS) we propose in next section does not need to decide in advance how many arms will be queried, and is therefore fully \emph{dynamic}. %The algorithm is inspired by a variant of Thompson sampling~\citep{thompson1933} called Top-Two Thompson sampling (\TTTS,~\citealt{russo2016ttts}). 

\begin{remark}
\begin{leftbar}[remarkbar]
\Hyperband is proposed specifically for hyper-parameter tuning. Its original philosophy is to adaptively allocate \emph{resources} to more promising configurations. Resources here can be time, dataset sub-sampling, feature sub-sampling, etc. In such a setting, the classifier is not always trained into completion given a parameter configuration, but is rather stopped early if it is shown to be bad so that we can allocate more resources to other configurations. In this case, different evaluations of a single configuration cannot be considered as i.i.d. anymore. Thus, \gls{hpo} is stated as a non-stochastic infinitely-armed bandit problem. This idea of early stopping is also further investigated by combining Bayesian optimization with it~\citep{falkner2018bohb}. However, this is about the model evaluation of Section~\ref{sec:dttts.survey.hpo} and is out of the scope of this thesis.
\end{leftbar}
\end{remark}
